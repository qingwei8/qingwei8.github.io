<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>kubeadm</title>
    <link href="/2024/01/05/kubeadm-2024-01-05-kubeadm/"/>
    <url>/2024/01/05/kubeadm-2024-01-05-kubeadm/</url>
    
    <content type="html"><![CDATA[<h1 id="使用-kubeadm-安装-kubernetes-集群"><a href="#使用-kubeadm-安装-kubernetes-集群" class="headerlink" title="使用 kubeadm 安装 kubernetes 集群"></a>使用 kubeadm 安装 kubernetes 集群</h1><p>对于需要学习 kubernetes 的同学来说，必须需要先学会如何搭建一个学习环境（即 kubernetes 集群），然后才能学习更多相关 kubernetes 及云原生的知识。目前随着社区的不断发展，kubernetes 的搭建已经非常方便，有基于 minikube/kind 等工具搭建的单节点集群，其搭建速度非常快（除去拉去镜像的时间，集群本身的搭建时间在分钟级别以内就能完成），但是单节点的 kubernetes 集群只能在本地学习的时候使用，没法部署到线上环境且也不支持多个节点的部署。而 kubeadm 正好为多个节点的 kubernetes 集群的搭建提供了工具，且能使用在大规模的生产环境中进行集群的搭建和部署。kubeadm 提供了简单便捷的方式来安装 kubernetes 集群，用户只需要使用 kubeadm 提供的简单命令来管理集群，如：</p><ul><li>kubeadm init 命令用来初始化一个集群</li><li>kubeadm join 命令用来给已有的一个集群添加新的节点</li><li>kubeadm reset 命令用来给已有的集群做重置操作，相当于删出一个已有集群，注意这个操作会将已有集群中的相关配置全部删除，请谨慎操作！！<br>下面的实操将以 kubeadm 来安装一个多个节点的 kubernetes 集群。kubeadm 的工作流程可参考：<a href="https://www.jianshu.com/p/029465088933">https://www.jianshu.com/p/029465088933</a></li></ul><h2 id="部署环境"><a href="#部署环境" class="headerlink" title="部署环境"></a>部署环境</h2><p>本次部署环境在 ubuntu 系统上进行操作，系统版本为 22.04 版本。<br>必要条件：</p><ul><li>master 节点：必须提供至少 2C CPU 和 8GiB 的内存资源。</li><li>node 节点：必须提供至少 1C CPU 和 1GiB 的内存资源。</li></ul><h2 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h2><h3 id="初始化操作（所有节点都必须要执行）"><a href="#初始化操作（所有节点都必须要执行）" class="headerlink" title="初始化操作（所有节点都必须要执行）"></a>初始化操作（所有节点都必须要执行）</h3><p>所有的节点都必须执行下面脚本，才能安装成功。注意如下脚本都需要在 root 账户下运行。</p><ul><li><p>关闭系统服务<br>在节点上创建文件 init.sh 脚本，将如下代码拷贝到 init.sh 里面执行初始化操作。</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><br><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;修改DNS配置如有需要&quot;</span><br>chattr -i /etc/resolv.conf<br>cat &lt;&lt;<span class="hljs-string">EOF &gt;&gt;  /etc/resolv.conf </span><br><span class="hljs-string">nameserver 10.10.10.10  # 改成自己需要的</span><br><span class="hljs-string">nameserver 10.10.10.11  # 改成自己需要的</span><br><span class="hljs-string">EOF</span><br><br><br><span class="hljs-comment">### 关闭swap分区</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;swapoff ## 临时生效&quot;</span><br>swapoff -a<br><br><span class="hljs-comment">## 永久生效可以编辑/etc/fstab文件 注释掉swap那行 https://www.idceval.com/111.html</span><br>sed -ri <span class="hljs-string">&#x27;s/.*swap.*/#&amp;/&#x27;</span> /etc/fstab<br><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;关闭防火墙&quot;</span><br>ufw <span class="hljs-built_in">disable</span><br><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;关闭selinux&quot;</span><br>setenforce 0<br>sed -i <span class="hljs-string">&#x27;s/enforcing/disabled/&#x27;</span> /etc/selinux/config<br></code></pre></td></tr></table></figure></li><li><p>安装必要的软件<br>在节点上创建文件 run.sh 脚本，将如下代码拷贝到 run.sh 里面执行安装必须的软件，如 docker、kubelet、kubeadm、kubectl 等工具。</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-comment">#!/bin/bash</span><br><br>sudo apt update -y &amp;&amp; sudo apt upgrade -y<br>sudo apt-get install -y ca-certificates curl gnupg lsb-release<br>sudo install -m <span class="hljs-number">0755</span> -d <span class="hljs-regexp">/etc/</span>apt/keyrings<br><br>curl -fsSL https:<span class="hljs-regexp">//</span>download.docker.com<span class="hljs-regexp">/linux/u</span>buntu<span class="hljs-regexp">/gpg | sudo gpg --dearmor -o /</span>etc<span class="hljs-regexp">/apt/</span>keyrings/docker.gpg<br>sudo chmod a+r <span class="hljs-regexp">/etc/</span>apt<span class="hljs-regexp">/keyrings/</span>docker.gpg<br><br><span class="hljs-comment"># Add the repository to Apt sources:</span><br>echo \<br><span class="hljs-string">&quot;deb [arch=&quot;</span>$(dpkg --print-architecture)<span class="hljs-string">&quot; signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \</span><br><span class="hljs-string">&quot;</span>$(. <span class="hljs-regexp">/etc/</span>os-release &amp;&amp; echo <span class="hljs-string">&quot;$VERSION_CODENAME&quot;</span>)<span class="hljs-string">&quot; stable&quot;</span> | \<br><br>sudo tee <span class="hljs-regexp">/etc/</span>apt<span class="hljs-regexp">/sources.list.d/</span>docker.list &gt; <span class="hljs-regexp">/dev/</span>null<br>sudo apt-get update<br><br>sudo apt-get -y install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin<br><br>systemctl enable docker.service<br>systemctl start docker.service<br><br>cat &lt;&lt;EOF | sudo tee <span class="hljs-regexp">/etc/m</span>odules-load.d/k8s.conf<br>overlay<br>br_netfilter<br>EOF<br><br>sudo modprobe overlay<br>sudo modprobe br_netfilter<br><br><span class="hljs-comment"># sysctl params required by setup, params persist across reboots</span><br>cat &lt;&lt;EOF | sudo tee <span class="hljs-regexp">/etc/</span>sysctl.d/k8s.conf<br>net.bridge.bridge-nf-call-iptables  = <span class="hljs-number">1</span><br>net.bridge.bridge-nf-call-ip6tables = <span class="hljs-number">1</span><br>net.ipv4.ip_forward                 = <span class="hljs-number">1</span><br>EOF<br><br><span class="hljs-comment"># Apply sysctl params without reboot</span><br>sudo sysctl --system<br><br>lsmod | grep br_netfilter<br>lsmod | grep overlay<br><br>sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward<br><br>cat &lt;&lt;EOF | sudo tee <span class="hljs-regexp">/etc/</span>docker/daemon.json<br>&#123;<br><span class="hljs-string">&quot;exec-opts&quot;</span>: [<span class="hljs-string">&quot;native.cgroupdriver=systemd&quot;</span>],<br><span class="hljs-string">&quot;log-driver&quot;</span>: <span class="hljs-string">&quot;json-file&quot;</span>,<br><span class="hljs-string">&quot;log-opts&quot;</span>: &#123;<br><span class="hljs-string">&quot;max-size&quot;</span>: <span class="hljs-string">&quot;100m&quot;</span><br>&#125;,<br><span class="hljs-string">&quot;storage-driver&quot;</span>: <span class="hljs-string">&quot;overlay2&quot;</span><br>&#125;<br>EOF<br><br>sudo systemctl enable docker<br>sudo systemctl daemon-reload<br>sudo systemctl restart docker<br><br>sudo apt-get update<br>sudo apt-get install -y apt-transport-https ca-certificates curl gpg<br><br>curl -fsSL https:<span class="hljs-regexp">//</span>pkgs.k8s.io<span class="hljs-regexp">/core:/</span>stable:<span class="hljs-regexp">/v1.28/</span>deb<span class="hljs-regexp">/Release.key | sudo gpg --dearmor -o /</span>etc<span class="hljs-regexp">/apt/</span>keyrings/kubernetes-apt-keyring.gpg<br><br>echo <span class="hljs-string">&#x27;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /&#x27;</span> | sudo tee <span class="hljs-regexp">/etc/</span>apt<span class="hljs-regexp">/sources.list.d/</span>kubernetes.list<br><br>sudo apt-get update<br>sudo apt-get install -y kubelet kubeadm kubectl<br>sudo apt-mark hold kubelet kubeadm kubectl<br><br><span class="hljs-comment"># containerd config</span><br>rm -rf <span class="hljs-regexp">/etc/</span>containerd/config.toml<br>containerd config default &gt; <span class="hljs-regexp">/etc/</span>containerd/config.toml<br>systemctl daemon-reload &amp;&amp; systemctl restart containerd<br></code></pre></td></tr></table></figure></li></ul><h3 id="master-节点上搭建集群"><a href="#master-节点上搭建集群" class="headerlink" title="master 节点上搭建集群"></a>master 节点上搭建集群</h3><h4 id="部署服务"><a href="#部署服务" class="headerlink" title="部署服务"></a>部署服务</h4><p>在一个 master 节点上创建新的 kubernetes 集群，部署服务之前请确保 <strong>初始化操作</strong> 都已完成。</p><ul><li><p>修改节点hostname，方便统一管理。</p>  <figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dsconfig"><span class="hljs-string">hostnamectl</span> <span class="hljs-built_in">set-hostname</span> <span class="hljs-string">k8s-master-01</span><br></code></pre></td></tr></table></figure></li><li><p>在 master 节点上通过 kube init 命令创建集群。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">kubeadm init <span class="hljs-attribute">--image-repository</span>=registry.aliyuncs.com/google_containers <span class="hljs-attribute">--pod-network-cidr</span>=10.244.0.0/16 --kubernetes-version stable<br></code></pre></td></tr></table></figure></li></ul><ul><li><p>拷贝kube/config文件 这样在当前节点可以执行 kubectl指令。</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk">mkdir -p <span class="hljs-variable">$HOME</span>/.kube<br>sudo cp -i <span class="hljs-regexp">/etc/</span>kubernetes<span class="hljs-regexp">/admin.conf $HOME/</span>.kube/config<br>sudo chown $(id -u):$(id -g) <span class="hljs-variable">$HOME</span><span class="hljs-regexp">/.kube/</span>config<br></code></pre></td></tr></table></figure></li><li><p>在master 节点上安装网络组件。</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">kubectl apply -f https:<span class="hljs-regexp">//</span>raw.githubusercontent.com<span class="hljs-regexp">/coreos/</span>flannel<span class="hljs-regexp">/master/</span>Documentation/kube-flannel.yml<br></code></pre></td></tr></table></figure></li><li><p>查看服务是否正常</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs applescript">kubectl <span class="hljs-keyword">get</span> pods <span class="hljs-comment">--all-namespaces</span><br></code></pre></td></tr></table></figure></li></ul><h4 id="安装过程中遇到的问题"><a href="#安装过程中遇到的问题" class="headerlink" title="安装过程中遇到的问题"></a>安装过程中遇到的问题</h4><ul><li>由于 kubeadm 要在多个几节点创建集群，集群内 pod 之间的通信需要通过 kubernetes 的cni plugin 来提供，因此需要选择合适的 cni 插件来初始化集群，通过 –pod-network-cidr 参数来指定 pod 的网段。此处选择使用 flannel 网络插件，其他的可<a href="https://medium.com/@arbnair97/how-to-create-a-kubernetes-cluster-using-kubeadm-in-ubuntu-3e6e475e1252">参考</a>。</li><li>由于 kubeadm 初始化时，docker 默认选择 registry.k8s.io 镜像仓库拉取镜像，这个仓库对国内环境不可访问，因此可以指定默认仓库为阿里云的镜像仓库。</li><li>经过上述两步操作后，集群搭建可能会成功，如果仍报 “registry.k8s.io/pause:3.6” 错误，是因为 kubelet 在创建容器时会去默认仓库（sandbox_image = “registry.k8s.io）拉取镜像，需要将 containerd 中的默认仓库地址修改为阿里云的地址（sandbox_image = “registry.aliyuncs.com/google_containers/pause:3.9”），可<a href="https://www.cnblogs.com/-ori/p/16971368.html">参考</a>。</li><li>解决网络问题及默认仓库后，服务不一定正常启动，碰到的问题可<a href="https://blog.csdn.net/ldjjbzh626/article/details/128400797">参考</a>，其主要原因是 kubelet 使用 etcd 的静态文件（/etc/kubernetes/manifests）创建静态 pod 的时候会失败，导致集群的 kube-apiserver、kube-controller-manager 等组件一直重启，可以使用 <code>crictl ps -a</code> 和 <code>crictl logs</code> 查看相关容器的日志。最终的原因是 ectd 服务不断重启是由于 containerd 中的 <code>/etc/containerd/config.toml</code> 文件配置不正确，需要将文件中的 <code>SystemdCgroup = false</code> 改为 <code>SystemdCgroup = true</code>，再执行 <code>systemctl restart containerd.service</code> 重启 containerd 服务。</li></ul><h3 id="添加新的-master-到集群"><a href="#添加新的-master-到集群" class="headerlink" title="添加新的 master 到集群"></a>添加新的 master 到集群</h3><p>添加新 master 节点到 kubernetes 集群之前请确保 <strong>初始化操作</strong> 都已完成，<a href="https://cloud.tencent.com/developer/article/2170756">可参考</a>。</p><ul><li><p>修改节点hostname，方便统一管理。</p><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dsconfig"><span class="hljs-string">hostnamectl</span> <span class="hljs-built_in">set-hostname</span> <span class="hljs-string">k8s-master-01</span><br></code></pre></td></tr></table></figure></li><li><p>在集群的 master 节点中执行如下命令，获取添加 node 的命令。</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gauss">kubeadm <span class="hljs-built_in">token</span> <span class="hljs-keyword">create</span> --<span class="hljs-keyword">print</span>-join-command<br></code></pre></td></tr></table></figure><p>然后，再在 node 节点上执行上述获取的命令，即可将新 node 添加到集群中</p></li><li><p>在 master上生成用于新 master 加入的证书</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">kubeadm init phase upload-certs <span class="hljs-comment">--experimental-upload-certs</span><br></code></pre></td></tr></table></figure></li><li><p>添加新 master，把新生成的 certificate key 加到 –experimental-control-plane –certificate-key 参数后，执行如下命令：</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gauss">kubeadm join &#123;apiserver-IP:Port&#125; --<span class="hljs-built_in">token</span> &#123;<span class="hljs-built_in">token</span>-<span class="hljs-built_in">key</span>&#125; --discovery-<span class="hljs-built_in">token</span>-ca-cert-hash &#123;ca-cert-<span class="hljs-built_in">key</span>&#125; --experimental-control-plane --certificate-<span class="hljs-built_in">key</span> &#123;certificate-<span class="hljs-built_in">key</span>&#125;<br></code></pre></td></tr></table></figure></li></ul><h3 id="添加新-node-节点到集群"><a href="#添加新-node-节点到集群" class="headerlink" title="添加新 node 节点到集群"></a>添加新 node 节点到集群</h3><p>添加新节点到 kubernetes 集群之前请确保 <strong>初始化操作</strong> 都已完成。</p><ul><li><p>修改节点hostname，方便统一管理。</p><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dsconfig"><span class="hljs-string">hostnamectl</span> <span class="hljs-built_in">set-hostname</span> <span class="hljs-string">k8s-node-01</span><br></code></pre></td></tr></table></figure></li><li><p>在集群的 master 节点中执行如下命令，获取添加 node 的命令。</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gauss">kubeadm <span class="hljs-built_in">token</span> <span class="hljs-keyword">create</span> --<span class="hljs-keyword">print</span>-join-command<br></code></pre></td></tr></table></figure></li><li><p>再在 node 节点上执行上述获取的命令，即可将新 node 添加到集群中：</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gauss">kubeadm join &#123;apiserver-IP:Port&#125; --<span class="hljs-built_in">token</span> &#123;<span class="hljs-built_in">token</span>-<span class="hljs-built_in">key</span>&#125; --discovery-<span class="hljs-built_in">token</span>-ca-cert-hash &#123;ca-cert-<span class="hljs-built_in">key</span>&#125;<br></code></pre></td></tr></table></figure></li></ul><h3 id="卸载集群"><a href="#卸载集群" class="headerlink" title="卸载集群"></a>卸载集群</h3><ul><li><p>驱逐 master/node 上的所有 pod 以及服务。</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">kubectl drain &lt;node_name&gt; <span class="hljs-comment">--ignore-daemonsets</span><br></code></pre></td></tr></table></figure></li><li><p>将 node 节点从集群中剔除出去</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">kubectl delete <span class="hljs-keyword">node</span> <span class="hljs-title">&lt;worker</span> <span class="hljs-keyword">node</span> <span class="hljs-title">name</span>&gt;<br></code></pre></td></tr></table></figure></li><li><p>执行清理工作，会删除 /etc/kubernetes 中的所有文件</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">kubeadm reset -f</span><br></code></pre></td></tr></table></figure></li><li><p>删除网络插件中的数据</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">rm <span class="hljs-regexp">/etc/</span>cni<span class="hljs-regexp">/net.d/</span>* -rf<br></code></pre></td></tr></table></figure></li></ul><h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><ul><li><a href="https://medium.com/@arbnair97/how-to-create-a-kubernetes-cluster-using-kubeadm-in-ubuntu-3e6e475e1252">https://medium.com/@arbnair97/how-to-create-a-kubernetes-cluster-using-kubeadm-in-ubuntu-3e6e475e1252</a></li><li><a href="https://developer.aliyun.com/article/791138">https://developer.aliyun.com/article/791138</a></li><li><a href="https://blog.csdn.net/weixin_43144516/article/details/118445728">https://blog.csdn.net/weixin_43144516/article/details/118445728</a></li><li><a href="https://www.cnblogs.com/helong-123/p/15637122.html">https://www.cnblogs.com/helong-123/p/15637122.html</a></li><li><a href="https://blog.csdn.net/ldjjbzh626/article/details/128400797">https://blog.csdn.net/ldjjbzh626/article/details/128400797</a></li><li><a href="https://www.cnblogs.com/-ori/p/16971368.html">https://www.cnblogs.com/-ori/p/16971368.html</a></li><li><a href="https://cloud.tencent.com/developer/article/2170756">https://cloud.tencent.com/developer/article/2170756</a></li><li><a href="https://www.idceval.com/111.html">https://www.idceval.com/111.html</a></li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>使用 kubeadm 安装 kubernetes 集群</title>
    <link href="/2024/01/04/cloud-native-k8s-install-kubeadm-2024-01-04-kubeadm/"/>
    <url>/2024/01/04/cloud-native-k8s-install-kubeadm-2024-01-04-kubeadm/</url>
    
    <content type="html"><![CDATA[<h1 id="使用-kubeadm-安装-kubernetes-集群"><a href="#使用-kubeadm-安装-kubernetes-集群" class="headerlink" title="使用 kubeadm 安装 kubernetes 集群"></a>使用 kubeadm 安装 kubernetes 集群</h1><p>对于需要学习 kubernetes 的同学来说，必须需要先学会如何搭建一个学习环境（即 kubernetes 集群），然后才能学习更多相关 kubernetes 及云原生的知识。目前随着社区的不断发展，kubernetes 的搭建已经非常方便，有基于 minikube/kind 等工具搭建的单节点集群，其搭建速度非常快（除去拉去镜像的时间，集群本身的搭建时间在分钟级别以内就能完成），但是单节点的 kubernetes 集群只能在本地学习的时候使用，没法部署到线上环境且也不支持多个节点的部署。而 kubeadm 正好为多个节点的 kubernetes 集群的搭建提供了工具，且能使用在大规模的生产环境中进行集群的搭建和部署。kubeadm 提供了简单便捷的方式来安装 kubernetes 集群，用户只需要使用 kubeadm 提供的简单命令来管理集群，如：</p><ul><li>kubeadm init 命令用来初始化一个集群</li><li>kubeadm join 命令用来给已有的一个集群添加新的节点</li><li>kubeadm reset 命令用来给已有的集群做重置操作，相当于删出一个已有集群，注意这个操作会将已有集群中的相关配置全部删除，请谨慎操作！！<br>下面的实操将以 kubeadm 来安装一个多个节点的 kubernetes 集群。kubeadm 的工作流程可参考：<a href="https://www.jianshu.com/p/029465088933">https://www.jianshu.com/p/029465088933</a></li></ul><h2 id="部署环境"><a href="#部署环境" class="headerlink" title="部署环境"></a>部署环境</h2><p>本次部署环境在 ubuntu 系统上进行操作，系统版本为 22.04 版本。<br>必要条件：</p><ul><li>master 节点：必须提供至少 2C CPU 和 8GiB 的内存资源。</li><li>node 节点：必须提供至少 1C CPU 和 1GiB 的内存资源。</li></ul><h2 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h2><h3 id="初始化操作（所有节点都必须要执行）"><a href="#初始化操作（所有节点都必须要执行）" class="headerlink" title="初始化操作（所有节点都必须要执行）"></a>初始化操作（所有节点都必须要执行）</h3><p>所有的节点都必须执行下面脚本，才能安装成功。注意如下脚本都需要在 root 账户下运行。</p><ul><li><p>关闭系统服务<br>在节点上创建文件 init.sh 脚本，将如下代码拷贝到 init.sh 里面执行初始化操作。</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><br><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;修改DNS配置如有需要&quot;</span><br>chattr -i /etc/resolv.conf<br>cat &lt;&lt;<span class="hljs-string">EOF &gt;&gt;  /etc/resolv.conf </span><br><span class="hljs-string">nameserver 10.10.10.10  # 改成自己需要的</span><br><span class="hljs-string">nameserver 10.10.10.11  # 改成自己需要的</span><br><span class="hljs-string">EOF</span><br><br><br><span class="hljs-comment">### 关闭swap分区</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;swapoff ## 临时生效&quot;</span><br>swapoff -a<br><br><span class="hljs-comment">## 永久生效可以编辑/etc/fstab文件 注释掉swap那行 https://www.idceval.com/111.html</span><br>sed -ri <span class="hljs-string">&#x27;s/.*swap.*/#&amp;/&#x27;</span> /etc/fstab<br><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;关闭防火墙&quot;</span><br>ufw <span class="hljs-built_in">disable</span><br><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;关闭selinux&quot;</span><br>setenforce 0<br>sed -i <span class="hljs-string">&#x27;s/enforcing/disabled/&#x27;</span> /etc/selinux/config<br></code></pre></td></tr></table></figure></li><li><p>安装必要的软件<br>在节点上创建文件 run.sh 脚本，将如下代码拷贝到 run.sh 里面执行安装必须的软件，如 docker、kubelet、kubeadm、kubectl 等工具。</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-comment">#!/bin/bash</span><br><br>sudo apt update -y &amp;&amp; sudo apt upgrade -y<br>sudo apt-get install -y ca-certificates curl gnupg lsb-release<br>sudo install -m <span class="hljs-number">0755</span> -d <span class="hljs-regexp">/etc/</span>apt/keyrings<br><br>curl -fsSL https:<span class="hljs-regexp">//</span>download.docker.com<span class="hljs-regexp">/linux/u</span>buntu<span class="hljs-regexp">/gpg | sudo gpg --dearmor -o /</span>etc<span class="hljs-regexp">/apt/</span>keyrings/docker.gpg<br>sudo chmod a+r <span class="hljs-regexp">/etc/</span>apt<span class="hljs-regexp">/keyrings/</span>docker.gpg<br><br><span class="hljs-comment"># Add the repository to Apt sources:</span><br>echo \<br><span class="hljs-string">&quot;deb [arch=&quot;</span>$(dpkg --print-architecture)<span class="hljs-string">&quot; signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \</span><br><span class="hljs-string">&quot;</span>$(. <span class="hljs-regexp">/etc/</span>os-release &amp;&amp; echo <span class="hljs-string">&quot;$VERSION_CODENAME&quot;</span>)<span class="hljs-string">&quot; stable&quot;</span> | \<br><br>sudo tee <span class="hljs-regexp">/etc/</span>apt<span class="hljs-regexp">/sources.list.d/</span>docker.list &gt; <span class="hljs-regexp">/dev/</span>null<br>sudo apt-get update<br><br>sudo apt-get -y install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin<br><br>systemctl enable docker.service<br>systemctl start docker.service<br><br>cat &lt;&lt;EOF | sudo tee <span class="hljs-regexp">/etc/m</span>odules-load.d/k8s.conf<br>overlay<br>br_netfilter<br>EOF<br><br>sudo modprobe overlay<br>sudo modprobe br_netfilter<br><br><span class="hljs-comment"># sysctl params required by setup, params persist across reboots</span><br>cat &lt;&lt;EOF | sudo tee <span class="hljs-regexp">/etc/</span>sysctl.d/k8s.conf<br>net.bridge.bridge-nf-call-iptables  = <span class="hljs-number">1</span><br>net.bridge.bridge-nf-call-ip6tables = <span class="hljs-number">1</span><br>net.ipv4.ip_forward                 = <span class="hljs-number">1</span><br>EOF<br><br><span class="hljs-comment"># Apply sysctl params without reboot</span><br>sudo sysctl --system<br><br>lsmod | grep br_netfilter<br>lsmod | grep overlay<br><br>sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward<br><br>cat &lt;&lt;EOF | sudo tee <span class="hljs-regexp">/etc/</span>docker/daemon.json<br>&#123;<br><span class="hljs-string">&quot;exec-opts&quot;</span>: [<span class="hljs-string">&quot;native.cgroupdriver=systemd&quot;</span>],<br><span class="hljs-string">&quot;log-driver&quot;</span>: <span class="hljs-string">&quot;json-file&quot;</span>,<br><span class="hljs-string">&quot;log-opts&quot;</span>: &#123;<br><span class="hljs-string">&quot;max-size&quot;</span>: <span class="hljs-string">&quot;100m&quot;</span><br>&#125;,<br><span class="hljs-string">&quot;storage-driver&quot;</span>: <span class="hljs-string">&quot;overlay2&quot;</span><br>&#125;<br>EOF<br><br>sudo systemctl enable docker<br>sudo systemctl daemon-reload<br>sudo systemctl restart docker<br><br>sudo apt-get update<br>sudo apt-get install -y apt-transport-https ca-certificates curl gpg<br><br>curl -fsSL https:<span class="hljs-regexp">//</span>pkgs.k8s.io<span class="hljs-regexp">/core:/</span>stable:<span class="hljs-regexp">/v1.28/</span>deb<span class="hljs-regexp">/Release.key | sudo gpg --dearmor -o /</span>etc<span class="hljs-regexp">/apt/</span>keyrings/kubernetes-apt-keyring.gpg<br><br>echo <span class="hljs-string">&#x27;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /&#x27;</span> | sudo tee <span class="hljs-regexp">/etc/</span>apt<span class="hljs-regexp">/sources.list.d/</span>kubernetes.list<br><br>sudo apt-get update<br>sudo apt-get install -y kubelet kubeadm kubectl<br>sudo apt-mark hold kubelet kubeadm kubectl<br></code></pre></td></tr></table></figure></li></ul><h3 id="master-节点上搭建集群"><a href="#master-节点上搭建集群" class="headerlink" title="master 节点上搭建集群"></a>master 节点上搭建集群</h3><h4 id="部署服务"><a href="#部署服务" class="headerlink" title="部署服务"></a>部署服务</h4><p>在一个 master 节点上创建新的 kubernetes 集群，部署服务之前请确保 <strong>初始化操作</strong> 都已完成。</p><ul><li><p>修改节点hostname，方便统一管理。</p>  <figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dsconfig"><span class="hljs-string">hostnamectl</span> <span class="hljs-built_in">set-hostname</span> <span class="hljs-string">k8s-master-01</span><br></code></pre></td></tr></table></figure></li><li><p>在 master 节点上通过 kube init 命令创建集群。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">kubeadm init <span class="hljs-attribute">--image-repository</span>=registry.aliyuncs.com/google_containers <span class="hljs-attribute">--pod-network-cidr</span>=10.244.0.0/16 --kubernetes-version stable<br></code></pre></td></tr></table></figure></li></ul><ul><li><p>拷贝kube/config文件 这样在当前节点可以执行 kubectl指令。</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk">mkdir -p <span class="hljs-variable">$HOME</span>/.kube<br>sudo cp -i <span class="hljs-regexp">/etc/</span>kubernetes<span class="hljs-regexp">/admin.conf $HOME/</span>.kube/config<br>sudo chown $(id -u):$(id -g) <span class="hljs-variable">$HOME</span><span class="hljs-regexp">/.kube/</span>config<br></code></pre></td></tr></table></figure></li><li><p>在master 节点上安装网络组件。</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">kubectl apply -f https:<span class="hljs-regexp">//</span>raw.githubusercontent.com<span class="hljs-regexp">/coreos/</span>flannel<span class="hljs-regexp">/master/</span>Documentation/kube-flannel.yml<br></code></pre></td></tr></table></figure></li><li><p>查看服务是否正常</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs applescript">kubectl <span class="hljs-keyword">get</span> pods <span class="hljs-comment">--all-namespaces</span><br></code></pre></td></tr></table></figure></li></ul><h4 id="安装过程中遇到的问题"><a href="#安装过程中遇到的问题" class="headerlink" title="安装过程中遇到的问题"></a>安装过程中遇到的问题</h4><ul><li>由于 kubeadm 要在多个几节点创建集群，集群内 pod 之间的通信需要通过 kubernetes 的cni plugin 来提供，因此需要选择合适的 cni 插件来初始化集群，通过 –pod-network-cidr 参数来指定 pod 的网段。此处选择使用 flannel 网络插件，其他的可<a href="https://medium.com/@arbnair97/how-to-create-a-kubernetes-cluster-using-kubeadm-in-ubuntu-3e6e475e1252">参考</a>。</li><li>由于 kubeadm 初始化时，docker 默认选择 registry.k8s.io 镜像仓库拉取镜像，这个仓库对国内环境不可访问，因此可以指定默认仓库为阿里云的镜像仓库。</li><li>经过上述两步操作后，集群搭建可能会成功，如果仍报 “registry.k8s.io/pause:3.6” 错误，是因为 kubelet 在创建容器时会去默认仓库（sandbox_image = “registry.k8s.io）拉取镜像，需要将 containerd 中的默认仓库地址修改为阿里云的地址（sandbox_image = “registry.aliyuncs.com/google_containers/pause:3.9”），可<a href="https://www.cnblogs.com/-ori/p/16971368.html">参考</a>。</li><li>解决网络问题及默认仓库后，服务不一定正常启动，碰到的问题可<a href="https://blog.csdn.net/ldjjbzh626/article/details/128400797">参考</a>，其主要原因是 kubelet 使用 etcd 的静态文件（/etc/kubernetes/manifests）创建静态 pod 的时候会失败，导致集群的 kube-apiserver、kube-controller-manager 等组件一直重启，可以使用 <code>crictl ps -a</code> 和 <code>crictl logs</code> 查看相关容器的日志。最终的原因是 ectd 服务不断重启是由于 containerd 中的 <code>/etc/containerd/config.toml</code> 文件配置不正确，需要将文件中的 <code>SystemdCgroup = false</code> 改为 <code>SystemdCgroup = true</code>，再执行 <code>systemctl restart containerd.service</code> 重启 containerd 服务。</li></ul><h3 id="添加新的-master-到集群"><a href="#添加新的-master-到集群" class="headerlink" title="添加新的 master 到集群"></a>添加新的 master 到集群</h3><p>添加新 master 节点到 kubernetes 集群之前请确保 <strong>初始化操作</strong> 都已完成，<a href="https://cloud.tencent.com/developer/article/2170756">可参考</a>。</p><ul><li><p>修改节点hostname，方便统一管理。</p><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dsconfig"><span class="hljs-string">hostnamectl</span> <span class="hljs-built_in">set-hostname</span> <span class="hljs-string">k8s-master-01</span><br></code></pre></td></tr></table></figure></li><li><p>在集群的 master 节点中执行如下命令，获取添加 node 的命令。</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gauss">kubeadm <span class="hljs-built_in">token</span> <span class="hljs-keyword">create</span> --<span class="hljs-keyword">print</span>-join-command<br></code></pre></td></tr></table></figure><p>然后，再在 node 节点上执行上述获取的命令，即可将新 node 添加到集群中</p></li><li><p>在 master上生成用于新 master 加入的证书</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">kubeadm init phase upload-certs <span class="hljs-comment">--experimental-upload-certs</span><br></code></pre></td></tr></table></figure></li><li><p>添加新 master，把新生成的 certificate key 加到 –experimental-control-plane –certificate-key 参数后，执行如下命令：</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gauss">kubeadm join &#123;apiserver-IP:Port&#125; --<span class="hljs-built_in">token</span> &#123;<span class="hljs-built_in">token</span>-<span class="hljs-built_in">key</span>&#125; --discovery-<span class="hljs-built_in">token</span>-ca-cert-hash &#123;ca-cert-<span class="hljs-built_in">key</span>&#125; --experimental-control-plane --certificate-<span class="hljs-built_in">key</span> &#123;certificate-<span class="hljs-built_in">key</span>&#125;<br></code></pre></td></tr></table></figure></li></ul><h3 id="添加新-node-节点到集群"><a href="#添加新-node-节点到集群" class="headerlink" title="添加新 node 节点到集群"></a>添加新 node 节点到集群</h3><p>添加新节点到 kubernetes 集群之前请确保 <strong>初始化操作</strong> 都已完成。</p><ul><li><p>修改节点hostname，方便统一管理。</p><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dsconfig"><span class="hljs-string">hostnamectl</span> <span class="hljs-built_in">set-hostname</span> <span class="hljs-string">k8s-node-01</span><br></code></pre></td></tr></table></figure></li><li><p>在集群的 master 节点中执行如下命令，获取添加 node 的命令。</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gauss">kubeadm <span class="hljs-built_in">token</span> <span class="hljs-keyword">create</span> --<span class="hljs-keyword">print</span>-join-command<br></code></pre></td></tr></table></figure></li><li><p>再在 node 节点上执行上述获取的命令，即可将新 node 添加到集群中：</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gauss">kubeadm join &#123;apiserver-IP:Port&#125; --<span class="hljs-built_in">token</span> &#123;<span class="hljs-built_in">token</span>-<span class="hljs-built_in">key</span>&#125; --discovery-<span class="hljs-built_in">token</span>-ca-cert-hash &#123;ca-cert-<span class="hljs-built_in">key</span>&#125;<br></code></pre></td></tr></table></figure></li></ul><h3 id="卸载集群"><a href="#卸载集群" class="headerlink" title="卸载集群"></a>卸载集群</h3><ul><li><p>驱逐 master/node 上的所有 pod 以及服务。</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">kubectl drain &lt;node_name&gt; <span class="hljs-comment">--ignore-daemonsets</span><br></code></pre></td></tr></table></figure></li><li><p>将 node 节点从集群中剔除出去</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">kubectl delete <span class="hljs-keyword">node</span> <span class="hljs-title">&lt;worker</span> <span class="hljs-keyword">node</span> <span class="hljs-title">name</span>&gt;<br></code></pre></td></tr></table></figure></li><li><p>执行清理工作，会删除 /etc/kubernetes 中的所有文件</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">kubeadm reset -f</span><br></code></pre></td></tr></table></figure></li><li><p>删除网络插件中的数据</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">rm <span class="hljs-regexp">/etc/</span>cni<span class="hljs-regexp">/net.d/</span>* -rf<br></code></pre></td></tr></table></figure></li></ul><h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><ul><li><a href="https://medium.com/@arbnair97/how-to-create-a-kubernetes-cluster-using-kubeadm-in-ubuntu-3e6e475e1252">https://medium.com/@arbnair97/how-to-create-a-kubernetes-cluster-using-kubeadm-in-ubuntu-3e6e475e1252</a></li><li><a href="https://developer.aliyun.com/article/791138">https://developer.aliyun.com/article/791138</a></li><li><a href="https://blog.csdn.net/weixin_43144516/article/details/118445728">https://blog.csdn.net/weixin_43144516/article/details/118445728</a></li><li><a href="https://www.cnblogs.com/helong-123/p/15637122.html">https://www.cnblogs.com/helong-123/p/15637122.html</a></li><li><a href="https://blog.csdn.net/ldjjbzh626/article/details/128400797">https://blog.csdn.net/ldjjbzh626/article/details/128400797</a></li><li><a href="https://www.cnblogs.com/-ori/p/16971368.html">https://www.cnblogs.com/-ori/p/16971368.html</a></li><li><a href="https://cloud.tencent.com/developer/article/2170756">https://cloud.tencent.com/developer/article/2170756</a></li><li><a href="https://www.idceval.com/111.html">https://www.idceval.com/111.html</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>cloud-native</category>
      
      <category>kubeadm</category>
      
    </categories>
    
    
    <tags>
      
      <tag>kubeadm</tag>
      
      <tag>kubernetes</tag>
      
      <tag>cloud-native</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes 弹性伸缩之 HPA 原理详解</title>
    <link href="/2022/01/18/kubernetes-controller-hpa-2022-01-18-hpa/"/>
    <url>/2022/01/18/kubernetes-controller-hpa-2022-01-18-hpa/</url>
    
    <content type="html"><![CDATA[<h1 id="Kubernetes-弹性伸缩之-HPA-原理详解"><a href="#Kubernetes-弹性伸缩之-HPA-原理详解" class="headerlink" title="Kubernetes 弹性伸缩之 HPA 原理详解"></a>Kubernetes 弹性伸缩之 HPA 原理详解</h1><p>本文基于 <a href="https://github.com/kubernetes/kubernetes/tree/release-1.23">kubernetes 1.23 版本</a>源码进行分析。</p><h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h2><p>Kubernetes 弹性伸缩机制的主要目的是解决资源与业务负载之间供需平衡的问题。当随着业务的实际使用逐渐增大时，其对资源的需求也将逐渐变得更多，这时就需要对集群和业务增加资源以满足需求，即扩容；而当随着业务结束后，其所需要的资源将变得很少，此时为了不浪费资源就需要将集群或业务的资源进行缩小，同时可将缩下来的资源提供给其他需要的业务进行使用以提高资源整体的利用率，即缩容。在 Kubernetes 的 autoscaler 体系中存在基于两种对象（pod 和node）的自动扩缩容机制，即基于集群角度的 cluster-autoscale 和基于业务 pod 角度的 hpa 和 vpa 扩缩容机制。Kubernetes 中主要的弹性伸缩机制：</p><table><thead><tr><th>类型</th><th>操作对象</th><th>说明</th></tr></thead><tbody><tr><td>Cluster-Autoscale</td><td>node</td><td>集群容量（node数量）自动伸缩，跟自动化部署相关的，依赖iaas的弹性伸缩，主要用于虚拟机容器集群</td></tr><tr><td>Horizontal-Pod-Autoscaler (HPA)</td><td>pod</td><td>Pod水平自动伸缩，如自动scale deployment的replicas，依赖业务实时负载指标</td></tr><tr><td>Vertical Pod Autoscaler （VPA）</td><td>pod</td><td>Pod垂直（资源配置）自动伸缩，如自动计算或调整deployment的Pod模板limit/request，依赖业务历史负载指标</td></tr></tbody></table><p>其中 HPA和 VPA都是从业务负载角度对 pod 数量/资源进行优化的。HPA需要解决的是当业务负载波动变化很大时，HPA controller 会根据业务忙闲情况自动调整副本数量，确保业务的整体的负载达到期望状态以维持负载的平衡稳定，保证服务的可靠性。VPA 主要解决资源配额（Pod的CPU、内存的limit/request）评估不准的问题，业务在运行初期无法确定资源的使用情况，用户往往会申请过多的资源而导致资源的极大浪费，而 VPA 会根据业务的历史资源使用情况来推荐合理的资源请求值，从而使业务的资源申请使用更合理。本文主要详细剖析 hpa 的工作原理。</p><h2 id="2-HPA-架构设计详解"><a href="#2-HPA-架构设计详解" class="headerlink" title="2. HPA 架构设计详解"></a>2. HPA 架构设计详解</h2><p>hpa 的全称是 Horizontal Pod Autoscaler，其主要是通过监控业务的资源利用情况来实现对 pod副本数进行自动水平扩缩容（scale）的机制，也是k8s里使用需求最广泛的一种自动扩缩容机制。hpa 的实现原理可以简单的理解为两个部分：</p><ul><li><strong>数据采集：</strong> 即监控指标的采集，HPA 通过 metrcis-server（支持 custom/external metrics server 用户自定义方式采集）从 kubelet 中获取 node/pod 的监控指标，并通过 k8s apiserver 向外部暴露 api 接口。 </li><li><strong>自动扩缩容：</strong> hpa 控制器通过 k8s apiserver 暴露出来的 api 接口获取到监控指标数据并计算出期望副本数，同时 hpa 控制器还会通过一系列算法和约束条件调整期望副本数，保证 hpa 扩缩容的稳定性。hpa 在得到期望副本数后会修改关联对象（如 deployment）子资源的 scale 副本数，然后通过关联对象（如 deployment）来实现对 pod 的自动扩缩容。</li></ul><p>其主要架构如下图所示:</p><p><img src="/2022/01/18/kubernetes-controller-hpa-2022-01-18-hpa/hpa.png" alt="hpa"></p><p>如上图可知，其主要组件包括 hpa controller（核心组件）, scaleTargetRef, apiserver， custom metrics server（核心组件）, prometheus-server, prometheus-adapter 等。</p><h3 id="2-1-hpa-controller"><a href="#2-1-hpa-controller" class="headerlink" title="2.1. hpa controller"></a>2.1. hpa controller</h3><p><a href="https://github.com/kubernetes/kubernetes/blob/release-1.23/pkg/controller/podautoscaler/horizontal.go#L573">hpa controller</a> 主要处理 hpa 对象的业务逻辑，hpa 控制器会定期（可在 kube-controller-manager 通过 –horizontal-pod-autoscaler-sync-period 配置，默认 15s）reconcile 每个 HPA对像，通过定期检查业务的监控指标来触发扩缩容操作。reconcile 逻辑主要是：</p><ul><li>首先通过 metrics 的 API 获取该 HPA 的 metrics 实时最新值（在当前副本数服务情况下），并将其与目标期望值进行比较。</li><li>然后根据比较的结果确定后期 hpa 的操作：扩容、缩容、不变。若比较结果为不变则直接返回，不需要进行扩缩容操作。</li><li>否则会使用相应的算法来计算出目标副本数，最后调用操作对象（如 deployment）的scale接口来调整当前副本数，然后通过操作对象的控制器（如 deployment）来实现 pod 的扩缩容，使业务的每个pod的最终metrics指标（平均值）基本维持到用户期望的水平。</li></ul><h3 id="2-2-scaleTargetRef"><a href="#2-2-scaleTargetRef" class="headerlink" title="2.2. scaleTargetRef"></a>2.2. scaleTargetRef</h3><p>可理解为 hpa 的扩缩容操作对象，hpa 会通过 scaleTargetRef 所指定的资源对象（必须支持 scale 接口）来实现对 pod 的扩缩容操作。scaleTargetRef 目前支持的对象： Deployment，StatefulSet, ReplicatSet。</p><h3 id="2-3-apiserver"><a href="#2-3-apiserver" class="headerlink" title="2.3. apiserver"></a>2.3. apiserver</h3><p>是 k8s 的 apiserver，这里主要是指 kube aggregator server，即通过 k8s apiserver 的扩展机制 hpa 的 http 请求将通过 aggregator server 转发到真正的 metrics server 后端进行处理。同时 hpa controller 也会通过 list-watch 机制与 apiserver 建立联系，不断从 apiserver 中获取 metrics 监控指标来 reconcile hpa 对象。如下图所示：</p><p><img src="/2022/01/18/kubernetes-controller-hpa-2022-01-18-hpa/hpa-k8sapiserver.png" alt="apiserver"></p><h3 id="2-4-custom-metrics-server"><a href="#2-4-custom-metrics-server" class="headerlink" title="2.4. custom metrics server"></a>2.4. custom metrics server</h3><p>server 是真正采集/聚合 metrics 指标的服务器，通常会在 k8s 中启动一个 pod 来提供服务，也可以自定义 metrics 服务器（只需实现 <a href="https://github.com/kubernetes/metrics">标准的 metrics API 接口</a> 即可）。目前 k8s 支持三种类型的 server(括号中为 k8s中对应的 api接口)： metrcis-server(metrics.k8s.io), custom metrcis server(custom.metrics.k8s.io), external metrics server(external.metrics.k8s.io)。</p><ul><li><strong>metrcis server:</strong><br><a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/#metrics-server">metrics server</a> 是实现 metrics API 接口的服务端，它通过 k8s 的 kubelet API 接口从 pod/node 中获取数据； 同时 metrics server 需要注册到 k8s apiserver 中，并通过 API Aggregation 聚合层来向外部提供各种数据的查询服务（kubectl top 以及 HPA/VPA 都是通过），其实现示意图如下所示。使用 metrics server 时需要注意：<ul><li>metrcis-server 只支持查询node/pod <strong>CPU 和内存</strong> 的使用指标。注意在 autoscaling/v1 版本时 HPA <strong>只支持CPU指标</strong> ，后期版本 hpa 的spec 值中 metrics 字段改成了列表形式才支持 cpu 和 内存两中形式。</li><li>metrics-serve 通过 node 节点上的kubelet 提供的接口将采集到的数据汇总到本地，由于 metrics-server 没有持久化模块，数据的采集全保留在内存中，因此是没有保留历史数据，只提供当前最新采集的数据。因此如果要通过 metrics server 获取历史值一般会借助 prometheus 进行数据持久化。</li></ul></li></ul><p><img src="/2022/01/18/kubernetes-controller-hpa-2022-01-18-hpa/hpa-metrics-server.png" alt="metrics-server"></p><ul><li><p><strong>custom/external metrics server:</strong><br>为了适应更灵活的需求，metrics API开始扩展支持用户自定义metrics指标（custom metrics），自定义数据采集/聚合则需要用户自行开发custom metrics server，社区有提供专门的 custom adpater 框架 <a href="https://github.com/kubernetes-sigs/custom-metrics-apiserver">custom-metrics-apiserver</a> ，该框架定义了 Custom 和 External 的 MetricsProvider 接口（如下所示），用户只需要实现对应的接口，即可实现用户自定义的 metrics server。</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-keyword">type</span> MetricsProvider interface &#123;<br>    CustomMetricsProvider<br>    ExternalMetricsProvider<br>&#125;<br><br><span class="hljs-keyword">type</span> CustomMetricsProvider interface &#123;<br>    <span class="hljs-comment">// GetMetricByName fetches a particular metric for a particular object.</span><br>    <span class="hljs-comment">// The namespace will be empty if the metric is root-scoped.</span><br>    <span class="hljs-constructor">GetMetricByName(<span class="hljs-params">name</span> <span class="hljs-params">types</span>.NamespacedName, <span class="hljs-params">info</span> CustomMetricInfo, <span class="hljs-params">metricSelector</span> <span class="hljs-params">labels</span>.Selector)</span> (*custom_metrics.MetricValue, error)<br><br>    <span class="hljs-comment">// GetMetricBySelector fetches a particular metric for a set of objects matching</span><br>    <span class="hljs-comment">// the given label selector.  The namespace will be empty if the metric is root-scoped.</span><br>    <span class="hljs-constructor">GetMetricBySelector(<span class="hljs-params">namespace</span> <span class="hljs-params">string</span>, <span class="hljs-params">selector</span> <span class="hljs-params">labels</span>.Selector, <span class="hljs-params">info</span> CustomMetricInfo, <span class="hljs-params">metricSelector</span> <span class="hljs-params">labels</span>.Selector)</span> (*custom_metrics.MetricValueList, error)<br><br>    <span class="hljs-comment">// ListAllMetrics provides a list of all available metrics at</span><br>    <span class="hljs-comment">// the current time.  Note that this is not allowed to return</span><br>    <span class="hljs-comment">// an error, so it is reccomended that implementors cache and</span><br>    <span class="hljs-comment">// periodically update this list, instead of querying every time.</span><br>    <span class="hljs-constructor">ListAllMetrics()</span> <span class="hljs-literal">[]</span>CustomMetricInfo<br>&#125;<br><br><span class="hljs-keyword">type</span> ExternalMetricsProvider interface &#123;<br>    <span class="hljs-constructor">GetExternalMetric(<span class="hljs-params">namespace</span> <span class="hljs-params">string</span>, <span class="hljs-params">metricSelector</span> <span class="hljs-params">labels</span>.Selector, <span class="hljs-params">info</span> ExternalMetricInfo)</span> (*external_metrics.ExternalMetricValueList, error)<br><br>    <span class="hljs-constructor">ListAllExternalMetrics()</span> <span class="hljs-literal">[]</span>ExternalMetricInfo<br>&#125;<br></code></pre></td></tr></table></figure><p>目前社区已经有人基于 prometheus server 的监控数据源，实现了一个 <a href="https://github.com/kubernetes-sigs/prometheus-adapter">prometheus adapter</a> 来提供 custom metrics server 服务，如果需要的自定义指标数据已经在 prometheus 里了，则可以直接对接使用，否则要先把自定义的指标数据注入到 prometheus server里才行。因为 HPA的负载一般来源于监控数据，而 prometheus 又是 CNCF 标准的监控服务，所以 prometheus adapter基本也可以满足我们所有自定义metrics的HPA的扩展需求。</p></li></ul><h3 id="2-5-prometheus-server"><a href="#2-5-prometheus-server" class="headerlink" title="2.5. prometheus-server"></a>2.5. prometheus-server</h3><p>prometheus 是一个知名开源监控系统，具有数据维度多，存储高效，使用便捷等特点。用户可以通过丰富的表达式和内置函数，定制自己所需要的监控数据。</p><h3 id="2-6-prometheus-adapter"><a href="#2-6-prometheus-adapter" class="headerlink" title="2.6. prometheus-adapter"></a>2.6. prometheus-adapter</h3><p>许多监控系统通过 adapter 实现了接口转换操作，同时给HPA提供指标数据。prometheus-adapter 在 prometheus 和 api-server 中起到了适配器的作用。prometheus-adapter 接受从 HPA 中发来的请求（通过apiserver aggregator中转的指标查询请求），然后根据内容发送相应的请求给prometheus 拿到指标数据，经过处理后返回给 HPA使用。prometheus 可以同时实现 metrics.k8s.io、custom.metrics.k8s.io、 external.metrics.k8s.io 三种 api接口，代替 k8s 自己的 metrics-server 提供指标数据服务。</p><h2 id="3-HPA-对象设计详解"><a href="#3-HPA-对象设计详解" class="headerlink" title="3. HPA 对象设计详解"></a>3. HPA 对象设计详解</h2><h3 id="3-1-HPA-版本介绍"><a href="#3-1-HPA-版本介绍" class="headerlink" title="3.1. HPA 版本介绍"></a>3.1. HPA 版本介绍</h3><p>目前 <a href="https://github.com/kubernetes/kubernetes/tree/release-1.23/staging/src/k8s.io/api/autoscaling">kubernetes 1.23 版本 autoscaling</a> 支持 4个版本 hpa 对象资源，分别是 <a href="https://github.com/kubernetes/kubernetes/blob/release-1.23/staging/src/k8s.io/api/autoscaling/v1/types.go#L38">autoscaling/v1</a>、<a href="https://github.com/kubernetes/kubernetes/blob/release-1.23/staging/src/k8s.io/api/autoscaling/v2beta1/types.go#L37">autoscaling/v2beta1</a>、<a href="https://github.com/kubernetes/kubernetes/blob/release-1.23/staging/src/k8s.io/api/autoscaling/v2beta2/types.go#L54">autoscaling/v2beta2</a>、<a href="https://github.com/kubernetes/kubernetes/blob/release-1.23/staging/src/k8s.io/api/autoscaling/v2/types.go#L51">autoscaling/v2</a>、。其主要区别如下表所示：</p><table><thead><tr><th>hpa 版本</th><th>spec 字段</th><th>说明</th></tr></thead><tbody><tr><td>autoscaling/v1</td><td>TargetCPUUtilizationPercentage</td><td>只支持 cpu 利用率指标，且将 metrics字段（指标类型如 object, resource 等metrics 类型）放在了annotation中进行处理</td></tr><tr><td>autoscaling/v2beta1</td><td>Metrics []MetricSpec</td><td>支持 Resource 类型的指标（如pod的CPU 和 内存）和 Custom Metrics 自定义指标</td></tr><tr><td>autoscaling/v2bet2</td><td>Metrics []MetricSpec，Behavior</td><td>支持 Resource 类型的指标（如pod的CPU 和 内存）、Custom Metrics 自定义指标 和 ExternalMetrics（k8s 外部指标）；同时支持自定义扩缩容行为 behavior</td></tr><tr><td>autoscaling/v2</td><td>Metrics []MetricSpec，Behavior</td><td>支持 Resource 类型的指标（如pod的CPU 和 内存）、Custom Metrics 自定义指标 和 ExternalMetrics（k8s 外部指标）；同时支持自定义扩缩容行为 behavior</td></tr></tbody></table><h3 id="3-2-hpa-结构字段详解"><a href="#3-2-hpa-结构字段详解" class="headerlink" title="3.2. hpa 结构字段详解"></a>3.2. hpa 结构字段详解</h3><p>下面将主要分析 autoscaling/v2 最新版本的 hpa 对象字段:</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"># https://github.com/kubernetes/kubernetes/blob/<span class="hljs-keyword">release</span><span class="hljs-number">-1.23</span>/staging/src/k8s.io/api/autoscaling/v2/<span class="hljs-keyword">types</span>.go<br>// HorizontalPodAutoscalerSpec describes the desired functionality <span class="hljs-keyword">of</span> the HorizontalPodAutoscaler.<br><span class="hljs-keyword">type</span> HorizontalPodAutoscalerSpec struct &#123;<br>  // scaleTargetRef points <span class="hljs-keyword">to</span> the target resource <span class="hljs-keyword">to</span> scale, <span class="hljs-keyword">and</span> <span class="hljs-keyword">is</span> used <span class="hljs-keyword">to</span> the pods <span class="hljs-keyword">for</span> which metrics<br>  // should be collected, <span class="hljs-keyword">as</span> well <span class="hljs-keyword">as</span> <span class="hljs-keyword">to</span> actually change the <span class="hljs-keyword">replica</span> count.<br>  ScaleTargetRef CrossVersionObjectReference `<span class="hljs-type">json</span>:&quot;scaleTargetRef&quot; protobuf:&quot;bytes,1,opt,name=scaleTargetRef&quot;`<br>  // minReplicas <span class="hljs-keyword">is</span> the lower <span class="hljs-keyword">limit</span> <span class="hljs-keyword">for</span> the number <span class="hljs-keyword">of</span> replicas <span class="hljs-keyword">to</span> which the autoscaler<br>  // can scale down.  It defaults <span class="hljs-keyword">to</span> <span class="hljs-number">1</span> pod.  minReplicas <span class="hljs-keyword">is</span> allowed <span class="hljs-keyword">to</span> be <span class="hljs-number">0</span> <span class="hljs-keyword">if</span> the<br>  // alpha feature gate HPAScaleToZero <span class="hljs-keyword">is</span> enabled <span class="hljs-keyword">and</span> at least one <span class="hljs-keyword">Object</span> <span class="hljs-keyword">or</span> <span class="hljs-keyword">External</span><br>  // metric <span class="hljs-keyword">is</span> configured.  Scaling <span class="hljs-keyword">is</span> active <span class="hljs-keyword">as</span> long <span class="hljs-keyword">as</span> at least one metric <span class="hljs-keyword">value</span> <span class="hljs-keyword">is</span><br>  // available.<br>  // +optional<br>  MinReplicas *int32 `<span class="hljs-type">json</span>:&quot;minReplicas,omitempty&quot; protobuf:&quot;varint,2,opt,name=minReplicas&quot;`<br>  // maxReplicas <span class="hljs-keyword">is</span> the upper <span class="hljs-keyword">limit</span> <span class="hljs-keyword">for</span> the number <span class="hljs-keyword">of</span> replicas <span class="hljs-keyword">to</span> which the autoscaler can scale up.<br>  // It cannot be less that minReplicas.<br>  MaxReplicas int32 `<span class="hljs-type">json</span>:&quot;maxReplicas&quot; protobuf:&quot;varint,3,opt,name=maxReplicas&quot;`<br>  // metrics contains the specifications <span class="hljs-keyword">for</span> which <span class="hljs-keyword">to</span> use <span class="hljs-keyword">to</span> calculate the<br>  // desired <span class="hljs-keyword">replica</span> count (the maximum <span class="hljs-keyword">replica</span> count across <span class="hljs-keyword">all</span> metrics will<br>  // be used).  The desired <span class="hljs-keyword">replica</span> count <span class="hljs-keyword">is</span> calculated multiplying the<br>  // ratio <span class="hljs-keyword">between</span> the target <span class="hljs-keyword">value</span> <span class="hljs-keyword">and</span> the <span class="hljs-keyword">current</span> <span class="hljs-keyword">value</span> <span class="hljs-keyword">by</span> the <span class="hljs-keyword">current</span><br>  // number <span class="hljs-keyword">of</span> pods.  Ergo, metrics used must decrease <span class="hljs-keyword">as</span> the pod count <span class="hljs-keyword">is</span><br>  // increased, <span class="hljs-keyword">and</span> vice-versa.  See the individual metric source <span class="hljs-keyword">types</span> <span class="hljs-keyword">for</span><br>  // more information about how <span class="hljs-keyword">each</span> <span class="hljs-keyword">type</span> <span class="hljs-keyword">of</span> metric must respond.<br>  // <span class="hljs-keyword">If</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">set</span>, the <span class="hljs-keyword">default</span> metric will be <span class="hljs-keyword">set</span> <span class="hljs-keyword">to</span> <span class="hljs-number">80</span>% average CPU utilization.<br>  // +listType=atomic<br>  // +optional<br>  Metrics []MetricSpec `<span class="hljs-type">json</span>:&quot;metrics,omitempty&quot; protobuf:&quot;bytes,4,rep,name=metrics&quot;`<br><br>  // behavior configures the scaling behavior <span class="hljs-keyword">of</span> the target<br>  // <span class="hljs-keyword">in</span> <span class="hljs-keyword">both</span> Up <span class="hljs-keyword">and</span> Down directions (scaleUp <span class="hljs-keyword">and</span> scaleDown fields respectively).<br>  // <span class="hljs-keyword">If</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">set</span>, the <span class="hljs-keyword">default</span> HPAScalingRules <span class="hljs-keyword">for</span> scale up <span class="hljs-keyword">and</span> scale down are used.<br>  // +optional<br>  Behavior *HorizontalPodAutoscalerBehavior `<span class="hljs-type">json</span>:&quot;behavior,omitempty&quot; protobuf:&quot;bytes,5,opt,name=behavior&quot;`<br>&#125;<br></code></pre></td></tr></table></figure><p>对应的 yaml 部署文件如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">autoscaling/v2</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">HorizontalPodAutoscaler</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">php-apache</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-comment"># HPA的伸缩对象描述，HPA会动态修改该对象的pod数量</span><br>  <span class="hljs-attr">scaleTargetRef:</span><br>    <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><br>    <span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">php-apache</span><br>  <span class="hljs-comment"># HPA的最小pod数量和最大pod数量，控制 hpa 扩缩容的上下界</span><br>  <span class="hljs-attr">minReplicas:</span> <span class="hljs-number">1</span><br>  <span class="hljs-attr">maxReplicas:</span> <span class="hljs-number">10</span><br>  <span class="hljs-comment"># 监控的指标数组，支持多种类型的指标共存</span><br>  <span class="hljs-attr">metrics:</span><br>  <span class="hljs-comment"># Object类型的指标</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">Object</span><br>    <span class="hljs-attr">object:</span><br>      <span class="hljs-attr">metric:</span><br>        <span class="hljs-comment"># 指标名称</span><br>        <span class="hljs-attr">name:</span> <span class="hljs-string">requests-per-second</span><br>      <span class="hljs-comment"># 监控指标的对象描述，指标数据来源于该对象</span><br>      <span class="hljs-attr">describedObject:</span><br>        <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1beta1</span><br>        <span class="hljs-attr">kind:</span> <span class="hljs-string">Ingress</span><br>        <span class="hljs-attr">name:</span> <span class="hljs-string">main-route</span><br>      <span class="hljs-comment"># Value类型的目标值，Object类型的指标只支持Value和AverageValue类型的目标值</span><br>      <span class="hljs-attr">target:</span><br>        <span class="hljs-attr">type:</span> <span class="hljs-string">Value</span><br>        <span class="hljs-attr">value:</span> <span class="hljs-string">10k</span><br>  <span class="hljs-comment"># Resource类型的指标</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">Resource</span><br>    <span class="hljs-attr">resource:</span><br>      <span class="hljs-attr">name:</span> <span class="hljs-string">cpu</span><br>      <span class="hljs-comment"># Utilization类型的目标值，Resource类型的指标只支持Utilization和AverageValue类型的目标值</span><br>      <span class="hljs-attr">target:</span><br>        <span class="hljs-attr">type:</span> <span class="hljs-string">Utilization</span><br>        <span class="hljs-attr">averageUtilization:</span> <span class="hljs-number">50</span><br>  <span class="hljs-comment"># ContainerResource类型的指标</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">ContainerResource</span><br>    <span class="hljs-attr">containerResource:</span><br>      <span class="hljs-attr">container:</span> <span class="hljs-string">C1</span>  <span class="hljs-comment"># 只收集容器 container name 为 C1 的 memory 指标</span><br>      <span class="hljs-attr">name:</span> <span class="hljs-string">memory</span><br>      <span class="hljs-attr">target:</span><br>        <span class="hljs-attr">type:</span> <span class="hljs-string">AverageValue</span><br>        <span class="hljs-attr">averageValue:</span> <span class="hljs-string">300Mi</span><br>  <span class="hljs-comment"># Pods类型的指标</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">Pods</span><br>    <span class="hljs-attr">pods:</span><br>      <span class="hljs-attr">metric:</span><br>        <span class="hljs-attr">name:</span> <span class="hljs-string">packets-per-second</span><br>      <span class="hljs-comment"># AverageValue类型的目标值，Pods指标类型下只支持AverageValue类型的目标值</span><br>      <span class="hljs-attr">target:</span><br>        <span class="hljs-attr">type:</span> <span class="hljs-string">AverageValue</span><br>        <span class="hljs-attr">averageValue:</span> <span class="hljs-string">1k</span><br>  <span class="hljs-comment"># External类型的指标</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">External</span><br>    <span class="hljs-attr">external:</span><br>      <span class="hljs-attr">metric:</span><br>        <span class="hljs-attr">name:</span> <span class="hljs-string">queue_messages_ready</span><br>        <span class="hljs-comment"># 该字段与第三方的指标标签相关联，（此处官方文档有问题，正确的写法如下）</span><br>        <span class="hljs-attr">selector:</span><br>          <span class="hljs-attr">matchLabels:</span><br>            <span class="hljs-attr">env:</span> <span class="hljs-string">&quot;stage&quot;</span><br>            <span class="hljs-attr">app:</span> <span class="hljs-string">&quot;myapp&quot;</span><br>      <span class="hljs-comment"># External指标类型下只支持Value和AverageValue类型的目标值</span><br>      <span class="hljs-attr">target:</span><br>        <span class="hljs-attr">type:</span> <span class="hljs-string">AverageValue</span><br>        <span class="hljs-attr">averageValue:</span> <span class="hljs-number">30</span><br>  <span class="hljs-comment"># 扩缩容策略，控制扩缩容的速率</span><br>  <span class="hljs-attr">behavior:</span><br>    <span class="hljs-attr">scaleDown:</span><br>      <span class="hljs-attr">stabilizationWindowSeconds:</span> <span class="hljs-number">300</span><br>      <span class="hljs-attr">policies:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">Percent</span><br>        <span class="hljs-attr">value:</span> <span class="hljs-number">100</span><br>        <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">15</span><br>    <span class="hljs-attr">scaleUp:</span><br>      <span class="hljs-attr">stabilizationWindowSeconds:</span> <span class="hljs-number">0</span><br>      <span class="hljs-attr">policies:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">Percent</span><br>        <span class="hljs-attr">value:</span> <span class="hljs-number">100</span><br>        <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">15</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">Pods</span><br>        <span class="hljs-attr">value:</span> <span class="hljs-number">4</span><br>        <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">15</span><br>      <span class="hljs-attr">selectPolicy:</span> <span class="hljs-string">Max</span><br></code></pre></td></tr></table></figure><p>如上所示，hpa spec 字段中的含义：</p><ul><li><p><strong>ScaleTargetRef：</strong> 定义 hpa 的操作对象，该对象（需要实现 scale 接口，支持对 scale 子资源（sub-resource）进行操作）也是自动扩缩容的管理对象，如 Deployment, ReplicatSet, StatefulSet。</p></li><li><p><strong>MinReplicas：</strong> hpa 所能缩容副本数的最小值（下限），可在一定范围内对计算的值起到纠正作用，也起到兜底的作用；默认值是1。k8s v1.16版本之后 minReplicas 可设为 0，但必须满足两个条件：1. metrics 必须配置至少有一个 Object 或者 External 类型；2. 需要对 apiserver 开启 <code>--feature-gates HPAScaleToZero=true</code> 特性。</p></li><li><p><strong>MaxReplicas：</strong> hpa 所能扩容副本数的最大值（上限），可确保集群中的资源不被某一个 hpa 对象耗尽。</p></li><li><p><strong>Metrics：</strong> hpa 扩容时需要根据用户指定的 metrics 类型（type）和 目标期望值（target）进行扩缩容。目前 metrics 中的 type字段有五种类型的值：Object、Pods、Resource、ContainerResource、External，其中 Pods、External、Object 支持使用筛选器进行筛选，允许进行条件选择。target 共有3种类型：Utilization、Value、AverageValue；其中 Utilization 表示平均使用率，Value表示裸值，AverageValue表示平均值，当业务的整体指标值大于 target 中指定的值时 hpa 会执行扩容逻辑。其详细描述如下表所示：</p><table><thead><tr><th>metrics type 类型</th><th>支持的 target 类型</th><th>说明</th></tr></thead><tbody><tr><td>Resource</td><td>Utilization, AverageValue</td><td>支持k8s里Pod的所有系统资源（包括cpu、memory等），但是一般只会用cpu，memory因为不太敏感而且跟语言相关</td></tr><tr><td>ContainerResource</td><td>Utilization, AverageValue</td><td>支持 k8s 中某个 pod 下特定 container的cpu和memory指标， 如一个 pod 中包含业务 container 和 sidecar, ContainerResource 可只根据指定的业务container（不考虑 sidecar）指标进行扩缩容</td></tr><tr><td>Pods</td><td>AverageValue</td><td>表示除了cpu，memory 等系统资源之外且是由 Pod 自身提供的自定义 metrics 数据。如 web服务中对 pod 的 QPS 实时监控指标， 该指标数据可通过 prometheus 采集获取并提供给 hpa。需要第三方adapter提供数据。</td></tr><tr><td>Object</td><td>Value, AverageValue</td><td>metrics 指标数据不是由 Pod本身的服务提供，但可以由 k8s 内部的其他资源提供，比如ingress等。object 类型一般需要聚合 Deployment下所有关联 pods的总指标。需要第三方adapter提供数据。</td></tr><tr><td>External</td><td>Value, AverageValue</td><td>metrics 指标数据的来源跟 k8s本身无关，指标数据完全取自外部系统。需要第三方adapter提供数据。</td></tr></tbody></table><p><strong>注意：</strong> 如果HPA spec里不配置任何metrics，k8s会默认设置 Resource类型的 CPU指标，目标类型是 AverageUtilization，value为80%。</p></li><li><p><strong>Behavior：</strong> behavior 在 autoscaling/v2beta2 和 autoscaling/v2 中定义，用来精确控制 hpa 扩缩容的速率，遵循 “<strong>快速扩容，谨慎缩容</strong>“ 的思想，尽量保证线上服务的可靠性。其数据结构定义可参考<a href="https://github.com/kubernetes/kubernetes/blob/release-1.23/staging/src/k8s.io/api/autoscaling/v2/types.go#L139">github</a>，其结构关系可用如下思维导航图表述：</p><p><img src="/2022/01/18/kubernetes-controller-hpa-2022-01-18-hpa/hpa-behavior.png" alt="behavior"></p><p>从 behavior 结构定义中可知，主要包括：scaleDown:(缩容速度策略) 和 scaleUp(扩容速度策略) 两个策略，每个扩缩容策略的规则主要包含 StabilizationWindowSeconds（稳定窗口）、SelectPolicy（策略选择）、Policies（扩缩容策略定义）三个字段。</p><table><thead><tr><th>字段</th><th>说明</th></tr></thead><tbody><tr><td>StabilizationWindowSeconds（稳定窗口）</td><td>当用于扩缩的指标不断波动时，稳定窗口用于限制副本计数的波动。自动扩缩算法使用此窗口来推断先前的期望状态并避免对工作负载规模进行不必要的更改，注意：稳定窗口的推荐值求解是<strong>扩容取最小值，缩容取最大值</strong>，参考<a href="https://github.com/kubernetes/kubernetes/blob/release-1.23/pkg/controller/podautoscaler/horizontal.go#L870">github 源码</a> ，这样可使变化最小保证稳定。如当指标显示目标应该缩容时，自动扩缩算法查看之前计算的期望状态，并使用指定时间间隔内的<strong>最大值</strong>作为缩容的期望值。这样的好处是避免了扩缩算法频繁增删 Pod。</td></tr><tr><td>SelectPolicy（扩缩容选择方向）</td><td>可以指定扩缩方向的 selectPolicy 字段来更改策略选择。每一个扩缩容策略规则（scaleDown）中可以定义多个 Policies 规则，这样同一个 hpa 对象会计算得到多个 metrics 数据或者副本数，这时可以通过该字段（SelectPolicy：Max/Min/Disabled）对 metrics 数据进行聚合。其中 Max：表示选择其中最大值作为<strong>变化量</strong>，Min：表示选择其中最小值作为<strong>变化量</strong>，Disabled: 表示完全禁用该方向的扩缩容操作。注意：如果该字段没有设置或为空，默认按照 Max 进行聚合，表示扩缩容时副本数的变化量选择最大的值。变化量：是指每次扩容（缩容）最多扩（缩）多少个副本数，变化量不是最终的期望值。</td></tr><tr><td>Policies（扩缩容策略）</td><td>定义真正的扩缩容速率规则，其值是个切片（同一个hpa可以通过 polices 定义多个规则）。Policies 包含三个字段：Type（指定规则的计算方式）, Vaule（指定规则的计算基数）, PeriodSeconds（指定在该规则下扩缩容的执行周期）。Policies 规则（Type字段）有两种计算方式（注意：这两种方法计算出的值都是变化值，不是目标值，即每次扩缩容多少个），一种是直接基于 Pods 进行计算，即每次扩缩容时直接指定需要扩缩容多少个 pod；另一种是基于 Percent 百分比计算后得到每次需要扩缩容的 pod 数。注意：Policies 的 PeriodSeconds 必须大于 0 且小等于 1800s (30min)。</td></tr></tbody></table><p>autoscaling/v2bet2 版本的 HPA的 Behavior如果不设置，k8s会自动设置扩缩容的默认配置, 具体内容如下：</p>  <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">behavior:</span><br><span class="hljs-attr">scaleDown:</span><br>  <span class="hljs-attr">stabilizationWindowSeconds:</span> <span class="hljs-number">300</span><br>  <span class="hljs-attr">policies:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">Percent</span><br>    <span class="hljs-attr">value:</span> <span class="hljs-number">100</span><br>    <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">15</span><br><span class="hljs-attr">scaleUp:</span><br>  <span class="hljs-attr">stabilizationWindowSeconds:</span> <span class="hljs-number">0</span><br>  <span class="hljs-attr">policies:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">Percent</span><br>    <span class="hljs-attr">value:</span> <span class="hljs-number">100</span><br>    <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">15</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">Pods</span><br>    <span class="hljs-attr">value:</span> <span class="hljs-number">4</span><br>    <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">15</span><br>  <span class="hljs-attr">selectPolicy:</span> <span class="hljs-string">Max</span><br></code></pre></td></tr></table></figure><p>  默认配置里分别定义了扩容和缩容的速率策略，缩容按照百分比，每15秒最多减少currentReplicas*100%个副本（但最终不可小于minReplicas），且缩容后的最终副本不得低于过去300s内计算的历史副本数的最大值；扩容则采用快速扩容，不考虑历史计算值（窗口时间为0），每15秒副本数翻倍或者每15秒新增4个副本数（两者取最大值），即：max(2*currentReplicas,4)。</p></li></ul><h2 id="4-HPA-算法详解"><a href="#4-HPA-算法详解" class="headerlink" title="4. HPA 算法详解"></a>4. HPA 算法详解</h2><p>HPA 扩缩容算法主要在 <a href="https://github.com/kubernetes/kubernetes/blob/release-1.23/pkg/controller/podautoscaler/horizontal.go#L573">HPA 控制器</a>里面实现，其中比较重要的算法主要有：1. 如何从 metrics 获取指标并计算得到期望副本数。2. 如何计算出的副本值进行约束（规范化），避免 metrics 中的数据异常导致扩缩容异常。3. 如何通过 behavior 方式控制扩缩容的速率，保证扩缩容的稳定性。下面先看看 hpa controller 的主要逻辑是如何实现的。</p><h3 id="4-1-HPA-扩缩容主流程"><a href="#4-1-HPA-扩缩容主流程" class="headerlink" title="4.1. HPA 扩缩容主流程"></a>4.1. HPA 扩缩容主流程</h3><p>HPA controller 会定期（可在 kube-controller-manager 通过 –horizontal-pod-autoscaler-sync-period 配置，默认 15s）通过list-watch 机制从 apiserver 中 watch 到 hpa 对象并不断调协每个 HPA 对像使其达到期望值，hpa controller 的主要逻辑是如何通过一定的算法计算出期望值，然后再调用 ScaleTargetRef 的 scale 子资源接口间接对 pod 进行扩缩容操作。从源码中可知 hpa controller 的主要逻辑如下图所：</p><p><img src="/2022/01/18/kubernetes-controller-hpa-2022-01-18-hpa/hpa-autoscale01.png" alt="autoscale01"></p><p>从上图可知，hpa controller 的主要流程如下：</p><ul><li>首先通过 scaleTargetRef 从 scheme 中获取到关联对象的 GVK/GVR 等资源，并通过 resources 获取到子资源 scale。并将子资源的replicas 计为当前 hpa 的 currentReplicas。</li><li>随后对 currentReplicas 的值进行一系列的判断，根据 currentReplicas 的值做出不同的操作，并计算出 desiredReplicas。<ul><li>标注 [1] 表示该资源不支持扩所容操作，会直接结束 reconcile。</li><li>标注 [2], [3] 表示 hpa scale 的当期值不满足 hpa 对象的定义，直接将 scale 的值规范到 hpa定义的约束中，再执行扩缩容操作。</li><li>标注 [4] 表示 hpa scale 的值满足 hpa 定义的约束，接下来就是根据 metrics 指标数据进行自动扩缩容，后面会主要讲这部分的逻辑。</li></ul></li><li>根据计算得出的 desiredReplicas 调用 scale 接口进行扩缩容操作。</li></ul><p>上图中最重要的逻辑是标注[4]，该部分逻辑主要包括：通过 metrics 指标计算 desiredReplicas；对计算出来 desiredReplicas 进行规范化处理；如果定义了 behavior 字段，会通过 behavior 字段中的值对扩缩容速率进行限制。</p><h3 id="4-2-通过-metrics-指标计算副本数"><a href="#4-2-通过-metrics-指标计算副本数" class="headerlink" title="4.2. 通过 metrics 指标计算副本数"></a>4.2. 通过 metrics 指标计算副本数</h3><p>hpa 的主要思想是通过用户指定 metrics 指标从监控中获取数据并计算出扩缩容的副本数，这部分的主要逻辑主要在 <a href="https://github.com/kubernetes/kubernetes/blob/release-1.23/pkg/controller/podautoscaler/horizontal.go#L247">computeReplicasForMetrics</a>中。对副本的计算方法，不同类型的 metrics 其计算 desiredReplicas 实现细节不一样，且同一种 metric 不同的 target 类型其算法也是不一样的，因此每个 metrics 都有其单独的算法来计算副本数，如 <a href="https://github.com/kubernetes/kubernetes/blob/release-1.23/pkg/controller/podautoscaler/horizontal.go#L303">computeReplicasForMetric</a>。但是总的来说，计算公式可以抽象为：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">desiredReplicas</span> = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )]<br></code></pre></td></tr></table></figure><p>从上述公式中，我们可以通过metrics指标的系数比 (currentMetricValue / desiredMetricValue) 来判断 hpa 是扩容还是缩容逻辑，如果: 系数比&gt;1.0，则 hpa 会进行扩容操作； 系数比=1.0，则不变；系数比&lt;1.0，则 hpa 会进行缩容操作。同时公式中的 ceil 操作会对计算出的浮点数向上取整操作，如计算值=5.2 时 desiredReplicas 会取值 6。</p><p>此外为避免获取的 metrics 指标异常或者 metrics 指标数据变化频繁而导致 hpa 频繁扩缩最终服务抖动的现象出现，可设置 hpa 的容忍度配置（默认为 0.1，即如果期望变化的副本倍数在[0.9, 1.1] 之间就直接停止计算并返回），容忍度配置可在 kube-controller-manager里的参数 horizontal-pod-autoscaler-tolerance 指定。</p><p>注意：如果 hpa 对象中指定了多个 metrics 指标，hpa controller 会从多个 metrics 中取<strong>最大的值</strong>作为期望的副本数。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">说明：这里对各种 metrics 的具体实现暂不做分析，后期有时间会专门研究另作一篇文章。<br></code></pre></td></tr></table></figure><h3 id="4-3-扩缩容副本数规范化（desiredReplicas-约束）"><a href="#4-3-扩缩容副本数规范化（desiredReplicas-约束）" class="headerlink" title="4.3. 扩缩容副本数规范化（desiredReplicas 约束）"></a>4.3. 扩缩容副本数规范化（desiredReplicas 约束）</h3><p>扩缩容副本数规范化是指对从 metrics 指标中计算出的 desiredReplicas 进行规范约束，即将 desiredReplicas 的值设置在 hpa 定义的合理范围内，避免 metrics 指标异常导致计算的副本数异常，可在一定范围内保证服务的稳定性。对 desiredReplicas 值的规范约束可用如下图表示：</p><p><img src="/2022/01/18/kubernetes-controller-hpa-2022-01-18-hpa/hpa-desiredresplicas.png" alt="desiredReplicas"></p><p>其中 hapMaxReplicas、hapMinReplicas(没有指定时默认为1) 是指 hpa 对象能扩缩容的最上、下限，分别对应 hpa spec 中的 MaxReplicas,、MinReplicas。maximumAllowedReplicas、minimumAllowedReplicas 分别表示 hpa 对象扩缩容经过一系列约束（包含稳定窗口约束、behavior 扩缩容速率约束等）后被允许达到的最大值、最小值， maximumAllowedReplicas 的取值必须在 [minimumAllowedReplicas, hapMaxReplicas] 范围内; minimumAllowedReplicas 的取值必须在 [hapMinReplicas, maximumAllowedReplicas] 范围内。desiredReplicas 是扩缩容的期望副本数，其取值范围在[minimumAllowedReplicas, maximumAllowedReplicas] 内。</p><p>在 hpa controller 逻辑里主要有两个函数实现了 desiredReplicas 规范化：<a href="https://github.com/kubernetes/kubernetes/blob/release-1.23/pkg/controller/podautoscaler/horizontal.go#L728">normalizeDesiredReplicas</a> 和 <a href="https://github.com/kubernetes/kubernetes/blob/release-1.23/pkg/controller/podautoscaler/horizontal.go#L763">normalizeDesiredReplicasWithBehaviors</a> 函数。</p><ul><li><p><strong>normalizeDesiredReplicas:</strong> <code>behavior 字段为空时</code>对副本数进行规范化处理（可等同于behavior 不为空时的默认扩缩容机制），主要包括下面三个处理流程。如下图所示：</p><ul><li>设置默认的缩容稳定窗口 downscaleStabilisationWindow，即<code>缩容冷却机制</code>。</li><li>计算扩容副本约束值，max(2<em>currentReplicas,4)，即单次扩容的副本数不能超过当前的2倍（而如果原副本数小于2，则可以一次性*<em>扩容到</em></em> 4个副本）。其目的是避免 metrics 中计算的副本数变化倍数太大时 hpa 一次性扩容太多而将集群中的资源消耗殆尽。</li><li>通过约束值对 desiredReplicas 进行合理求值。</li></ul><p><img src="/2022/01/18/kubernetes-controller-hpa-2022-01-18-hpa/hpa-autoscale02.png" alt="autoscale02"></p></li><li><p><strong>normalizeDesiredReplicasWithBehaviors:</strong> <code>behavior 字段非空时</code> 通过 behavior 字段中设置的值来约束副本数，即对副本数进行规范化处理，主要也包含下面三个处理流程。如下图所示：</p><ul><li>通过 behavior 中的 StabilizationWindowSeconds 字段（如果为空，缩容会设置默认值为 downscaleStabilisationWindow，即设置默认的缩容稳定窗口冷却机制；扩容不会设置默认值）来求出稳定窗口中的推荐值，扩容时：推荐值等于所有记录中的<strong>最小值</strong>，缩容时：推荐值等于所有记录中的<strong>最大值</strong>。推荐值会作为候选 desiredReplicas 继续后面的约束判断。</li><li>根据上面的 desiredReplicas 和 currentReplicas 来判断扩/缩容逻辑。<ul><li>desiredReplicas &gt; currentReplicas 为扩容，通过 <a href="https://github.com/kubernetes/kubernetes/blob/release-1.23/pkg/controller/podautoscaler/horizontal.go#L1035">calculateScaleUpLimitWithScalingRules</a> 计算扩容副本的约束值，然后通过约束值求得 desiredReplicas。</li><li>desiredReplicas &lt; currentReplicas 为缩容，通过 <a href="https://github.com/kubernetes/kubernetes/blob/release-1.23/pkg/controller/podautoscaler/horizontal.go#L1063">calculateScaleDownLimitWithBehaviors</a> 计算缩容副本的约束值，然后通过约束值求得 desiredReplicas。</li></ul></li></ul><p><img src="/2022/01/18/kubernetes-controller-hpa-2022-01-18-hpa/hpa-autoscale03.png" alt="autoscale03"></p></li></ul><p>综上可知：hpa controller 的 desiredReplicas 规范化约束包含两种模式：一种适应于旧版本（如 autoscaling/v1），一种是添加 behavior 字段的 autoscaling/v2bet2 以上版本中使用。从源代码的角度可知，behavior 字段包含了 StabilizationWindowSeconds、SelectPolicy、Policies 字段，实现了更为精细的扩缩容控制。</p><h3 id="4-4-扩缩容的几个约束"><a href="#4-4-扩缩容的几个约束" class="headerlink" title="4.4. 扩缩容的几个约束"></a>4.4. 扩缩容的几个约束</h3><p>在上一节讲述 desiredReplicas 值的取值约束中，maximumAllowedReplicas、minimumAllowedReplicas 的设计思想目的是尽量保证 hpa 扩缩容的稳定性，确保尽量做的”快速扩容，谨慎缩容”，而这两个值的计算也是通过一些系列约束得出的。在 hpa controller 逻辑里主要包含下面几个约束：<code>缩容冷却机制</code>、<code>稳定窗口</code>、<code>behavior 扩缩容速率约束</code>。</p><h4 id="4-4-1-缩容冷却机制"><a href="#4-4-1-缩容冷却机制" class="headerlink" title="4.4.1. 缩容冷却机制"></a>4.4.1. 缩容冷却机制</h4><p>缩容冷却机制总的来说是避免因流量异常而导致服务缩容异常，最终导致服务抖动不稳定。k8s HPA算法的默认扩缩容原则是：<strong>快速扩容，谨慎缩容</strong>。</p><ul><li><p>对 hpa 对象的单次缩容机制：<br>虽然 HPA 同时支持扩容和缩容，但在生产环境上扩容一般来说重要性更高，特别是在大促流量突增的时候，能否快速扩容决定了系统的稳定性，所以HPA的算法里对扩容的时机是没有额外限制的，只要达到扩容条件就执行扩容逻辑（当前一次至少扩容到原来的1.1倍）。但对于缩容，为了避免过早缩导致服务来回波动（thrashing 抖动）而影响服务的稳定性，HPA的算法对缩容的要求比较严格，通常会设置一个滑动窗口（默认为 5分钟，可通过 kube-controller-manager 的参数 horizontal-pod-autoscaler-downscale-stabilization 来修改）来记录过去最近一段时间内的期望副本数，只有连续窗口时间内（如 5分钟内）计算出的期望副本数都比当前副本数小，才执行scale缩容操作，缩容的目标副本数取窗口期内所有记录的最大值。从源码中可以看到在 hpa controller 对 desiredReplicas 进行规范化约束前会执行<a href="https://github.com/kubernetes/kubernetes/blob/release-1.23/pkg/controller/podautoscaler/horizontal.go#L705">缩容冷却机制</a>。</p></li><li><p>对同一个 hpa 对象的多次扩缩容之间的冷却机制：<br>在弹性伸缩中，冷却周期是不能逃避的一个话题，很多时候我们期望快速弹出与快速回收，而另一方面，我们又不希望集群震荡导致服务不稳定，所以一个弹性伸缩活动冷却周期的具体数值是多少，一直被开发者所挑战。在 HPA 中，默认的扩容冷却周期是 3 分钟，即对于同一个 hpa 对象的两次扩容时间间隔是 3分钟；缩容冷却周期是 5分钟，即对于同一个 hpa 对象的两次缩容时间间隔是 5分钟。</p></li></ul><h4 id="4-4-2-稳定窗口机制"><a href="#4-4-2-稳定窗口机制" class="headerlink" title="4.4.2. 稳定窗口机制"></a>4.4.2. 稳定窗口机制</h4><p>稳定窗口机制在<a href="https://github.com/kubernetes/kubernetes/blob/release-1.23/pkg/controller/podautoscaler/horizontal.go#L705">早期</a>是只有缩容时才有的机制（缩容冷却机制 downscaleStabilisationWindow），即在缩容时会通过一个滑动窗口来记录最近计算的副本数，并选择<strong>最大值</strong>作为目标期望副本数，且该滑动窗口可以通过参数在 horizontal-pod-autoscaler-downscale-stabilization 来修改，扩容使用固定速率：max(2*currentReplicas, 4)。但随着 <a href="https://github.com/kubernetes/kubernetes/blob/release-1.23/pkg/controller/podautoscaler/horizontal.go#L870">autoscaling/v2bet2 版本</a>添加 behavior 字段后，使扩缩容做到更精确更细致的控制，通过 StabilizationWindowSeconds 字段对扩容和缩容实现了稳定窗口机制，对于<strong>扩容</strong>会选择窗口内的所有记录中的<strong>最小值</strong>作为候选 desiredReplicas，对于<strong>缩容</strong>会选择窗口内的所有记录中的<strong>最大值</strong>作为候选 desiredReplicas。扩容选择最小值缩容选择最大值都是为保证在一次扩缩容时使得变化量最少，可最大程度保证服务的稳定性。如下图所示：</p><p><img src="/2022/01/18/kubernetes-controller-hpa-2022-01-18-hpa/hpa-behavior01.png" alt="behavior01"></p><h4 id="4-4-3-扩缩容速率控制"><a href="#4-4-3-扩缩容速率控制" class="headerlink" title="4.4.3. 扩缩容速率控制"></a>4.4.3. 扩缩容速率控制</h4><p>在 HPA controller里默认扩缩容总原则是：<strong>快速扩容，谨慎缩容</strong>。早期版本，扩容是使用固定速率：max(2*currentReplicas, 4)来进行扩容；缩容仅仅只是通过设置一个集群全局的窗口时间（downscaleStabilisationWindow）来约束，窗口期过后也就失去控制能力。但 autoscaling/v2bet2 版本后对扩缩容速率做了更多细致化的优化，即会通过 behavior 字段进行约束（StabilizationWindowSeconds + SelectPolicy + Policies）。下面主要讲述 autoscaling/v2bet2 下基于 bahavior 方式来控制扩缩容的速率。具体看如下图所示：</p><ul><li><p>behavior 中的 StabilizationWindowSeconds 用来实现滑动窗口机制，在扩容/缩容时都可以配置滑动窗口的时间来更细粒度的控制扩/缩容的速率，在计算滑动窗口的推荐值时，扩容会选则滑动窗口期内的所有记录的最小值作为推荐值，缩容会选择滑动窗口期内的所有记录的最大值作为推荐值。<strong>注意：</strong> 缩容时，如果 ScaleDown 策略中的 <a href="https://github.com/kubernetes/kubernetes/blob/release-1.23/pkg/controller/podautoscaler/horizontal.go#L791">behavior 非空并且 StabilizationWindowSeconds 没有设置</a>时，hpa 会使用默认的 downscaleStabilisationWindow 窗口来限制缩容的速率。</p></li><li><p>behavior 中的 Policies 是真正控制扩缩容速率的部分：</p><ul><li>policies 中对副本的计算方式有两种类型 type：Pods 和 Percent</li><li>指定周期 PeriodSeconds，每次扩缩容周期内只能扩/缩 podsValue（currentReplicas*PercentValue） 个副本数</li><li>结合 behavior 中的选择策略方向 SelectPolicy，当有多个策略同时计算副本时，副本数的最后结果需要通过SelectPolicy：Max/Min/Disabled做聚合，注意：Max表示选择<strong>变化量</strong>最大的值，Min表示选择<strong>变化量</strong>最小的值，Disabled表示禁止这个方向的扩缩容。</li></ul></li></ul><p><img src="/2022/01/18/kubernetes-controller-hpa-2022-01-18-hpa/hpa-behavior02.png" alt="behavior02"></p><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><p>本文首先从架构设计的角度进行分析，对 hpa 的各个组件的功能和设计上进行了简单描述，从整体上了解到 hpa 工作原理，简述如下：hpa controller 通过 apiserver 从 metrics-server 中获取监控指标，并通过一定的算法来计算出期望的副本数，随后 hpa controller 会调用 hpa 对象所关联对象的 scale 接口实现对 pod 的扩缩容操作；而监控指标的获取是 metrics-server 调用 kubelet 提供的 api 接口从容器中获取的。由此可简单的根据 pod cpu 指标的变化情况实现 hpa 对pod 的扩缩容操作；但为了满足复杂的业务场景，hpa 也支持用户通过自定义指标来实现自动扩缩容操作，目前用户可通过 custom.metrics.k8s.io 和 external.metrics.k8s.io 两个接口实现自定义 api 接口的扩展，但是相应的需要自己实现类似 metrics-server 的 server 功能，实现对指标的采集和聚合操作并向外提供api查询服务，由此社区也提供了 prometheus + prometheus-adapter 的方案。</p><p>随后本文从源码角度剖析了 hpa 的实现原理，先是简单介绍了 hpa 多个版本的区别，之后通过分析 hpa 对象的字段结构、yaml文件配置等了解到如何使用 hpa 的。hpa 对象的 spec 字段主要包括 ScaleTargetRef、MaxReplicas、MinReplicas、Metrics、Behavior 五个字段，其中 ScaleTargetRef 和 MaxReplicas 是必选字段，其他三个是可选字段。</p><ul><li>ScaleTargetRef 用来指定扩缩容的关联对象；hpa 会通过它的 scale 子资源接口实现扩缩容操作。</li><li>MinReplicas 和 MaxReplicas 用来指定 hpa 扩缩容的上下界，MinReplicas 未指定时默认值为1；</li><li>Metrics 用来指定扩缩容指标，是一个切片类型，可以指定多个metric，目前每个 metric 支持的类型主要要五种：Resource、ContainerResource、Pods、Object、External 类型，每种类型的使用场景和值类型也不一样，其计算 metrics指标的聚合数据方法也是不一样的，前面 4种类型使用于 k8s 内部资源指标，External 使用于 k8s 外部资源指标。</li><li>Behavior 字段用来指定 hpa 扩缩容速率，其包含三个字段：StabilizationWindowSeconds、SelectPolicy、Policies。StabilizationWindowSeconds 字段用来实现稳定窗口，目的是确保在 metrics 数据异常或者流量激增的情况下可保证服务的稳定性，特别是缩容场景中。SelectPolicy 和 Policies 结合使用，用来确定扩缩容的速率以及方向或者禁用扩缩容。</li></ul><p>最后详细介绍了 hpa 逻辑中涉及到的一些算法和约束，如缩容冷却机制，desiredReplicas 规范化，稳定窗口机制，扩缩容速率约束等算法。通过这些算法，从总体上实现了 k8s hpa 扩缩容的基本原则：<strong>快速扩容，谨慎缩容</strong>。</p><h2 id="6-参考"><a href="#6-参考" class="headerlink" title="6. 参考"></a>6. 参考</h2><ul><li><a href="https://github.com/kubernetes/kubernetes/blob/release-1.23/pkg/controller/podautoscaler/horizontal.go">https://github.com/kubernetes/kubernetes/blob/release-1.23/pkg/controller/podautoscaler/horizontal.go</a></li><li><a href="https://granulate.io/kubernetes-autoscaling-the-hpa/">https://granulate.io/kubernetes-autoscaling-the-hpa/</a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/</a></li><li><a href="https://zhuanlan.zhihu.com/p/245208287">https://zhuanlan.zhihu.com/p/245208287</a></li><li><a href="https://zhuanlan.zhihu.com/p/89453704">https://zhuanlan.zhihu.com/p/89453704</a></li><li><a href="https://zhuanlan.zhihu.com/p/74936498">https://zhuanlan.zhihu.com/p/74936498</a></li><li><a href="https://zhuanlan.zhihu.com/p/368865741">https://zhuanlan.zhihu.com/p/368865741</a></li><li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>kubernetes</category>
      
      <category>controller</category>
      
    </categories>
    
    
    <tags>
      
      <tag>kubernetes</tag>
      
      <tag>hpa</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>knative serving 概述</title>
    <link href="/2021/11/20/serverless-knative-summary-2021-11-20-knative-summary/"/>
    <url>/2021/11/20/serverless-knative-summary-2021-11-20-knative-summary/</url>
    
    <content type="html"><![CDATA[<h1 id="knative-serving-概述"><a href="#knative-serving-概述" class="headerlink" title="knative serving 概述"></a>knative serving 概述</h1><h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h2><p>KNative是谷歌开源的 serverless 架构方案，旨在提供一套简单易用的 serverless 方案，把 serverless 标准化。目前参与的公司主要是 Google、Pivotal、IBM、Red Hat，2018年7月24日对外发布，目前还处于快速发展的阶段。knative 是为了解决以容器为核心的serverless 应用的构建、部署和运行的问题。</p><p>用户只需要编写代码（或者函数），以及配置文件（如何build、运行以及访问等声明式信息），然后运行build和deploy就能把应用自动部署到集群（可以是公有云，也可以是私有云）。其他事情交由serverless平台（比如这里的KNative）自动处理的，这些事情包括：</p><ul><li>自动完成代码到容器的构建。</li><li>把应用（或者函数）和特定的事件进行绑定：当事件发生时，自动触发应用（或者函数）。</li><li>网络的路由和流量控制以及灰度发布</li><li>函数的自动伸缩</li></ul><p>和标准化的 FaaS 不同（只运行特定标准的 Function 代码），KNative 期望能够运行所有的 workload: traditional application、function、container。</p><p>knative 建立在 kubernetes 和 istio 平台之上，使用 kubernetes 提供对容器的编排管理能力（deployment、replicaset、和 pods等），以及 istio 提供的网络管理功能（ingress、LB、dynamic route等）。</p><p><img src="/2021/11/20/serverless-knative-summary-2021-11-20-knative-summary/knative-summary01.png" alt="s01"></p><p>为了实现 serverless 应用的管理，knative 把整个系统分成了三个部分：</p><ul><li>Build：构建系统，把用户定义的函数和应用 build 成容器镜像。</li><li>Serving：服务系统，用来配置应用的路由、升级策略、自动扩缩容等功能。</li><li>Eventing：事件系统，用来自动完成事件的绑定和触发。</li></ul><p>本文主要介绍下 knative serving 部分。</p><h2 id="2-knative-serving-工作原理"><a href="#2-knative-serving-工作原理" class="headerlink" title="2. knative serving 工作原理"></a>2. knative serving 工作原理</h2><p>knative serving 的核心功能是使用户配置的应用在 kubernetes 中运行起来，并向外提供服务。主要需要实现的功能包括如下：</p><ul><li>自动化启动和销毁容器</li><li>根据名字生成网络访问相关的 service、ingress 等对象</li><li>监控应用的请求，并自动扩缩容</li><li>支持蓝绿发布、回滚功能，方便应用发布流程</li></ul><p>knative serving 功能是基于 kubernetes 和 istio 开发的，它使用 kubernetes 来管理容器（deployment、pod），使用 istio 来管理网络路由（VirtualService、DestinationRule）。而 kubernetes 和 istio 本身的组件和概念非常多，理解和管理起来相对困难，使用起来对用户也不够友好。所以 knative 在此之上基于 k8s CRD 的方式提供了更高一层的抽象，可以如下所示的资源关系图了解 knatvie 工作原理：</p><p><img src="/2021/11/20/serverless-knative-summary-2021-11-20-knative-summary/knative-sumaary02.png" alt="s02"></p><ul><li>Service：注意这里 service 不是 kubernetes 中提供服务发现的那个 service，而是 knative 自定义的 CRD，它的全称是 services.serving.knative.dev。单独控制 route 和 configuration 就能实现 serving 的所有功能，但 knative 更推荐使用 Service 来管理，因为它会自动帮你管理 route 和 configuration 中的所有资源。Service 负责管理 workload 的整个生命周期，负责创建相应的 route 和 configuration 资源对象，并通过监听 route、configuration、revision 等资源的变化情况来更新 service 本身。</li><li>Route：knative 定义的 CRD资源，全称是 route.serving.knative.dev。应用的路由规则，也就是进来的流量如何访问应用，对应了 istio的流量管理（VirtualService），route 通过配置多种不同的规则可以实现多版本应用（多个revision）的灰度发布，指定流量分配，动态回滚等功能。</li><li>configuration：knative 定义的 CRD资源，全称是 configuration.serving.knative.dev。应用的最新配置，也就是应用目前期望的状态，维护 kubernetes 的容器的期望数（deployment replicas）。每次应用升级都会更新 configuration，而 knative 也会保留历史版本的记录（图中的 revision），结合 route的流量管理，knative 可以让多个不同的版本共同提供服务，方便蓝绿发布和滚动更新。注意：修改 configuration 将创建新的 revision，而不删除原有的 revision（revision 的删除有 knative gc 来处理）。</li><li>Revision: knative 定义的 CRD资源，全称是 revision.serving.knative.dev。可理解为版本快照，是对工作负荷所做的每个修改的时间点快照。revision 一旦被创建就不会再重新修改（除非删除），每次 configuration 的修改都会对应一个新的revision，每个 revision 都会根据流入的流量自定进行扩缩容操作。</li></ul><h2 id="3-knative-serving-CRD-资源对象"><a href="#3-knative-serving-CRD-资源对象" class="headerlink" title="3. knative serving CRD 资源对象"></a>3. knative serving CRD 资源对象</h2><p>knative 是通过 k8s CRD 的方式建立在 k8s 之上的服务，我们可以根据资源之间的关联关系来做个梳理，如下图所示：</p><p><img src="/2021/11/20/serverless-knative-summary-2021-11-20-knative-summary/knative-resources01.png" alt="rs01"></p><ol><li><p>Service 会创建 route、configuration 资源对象。管理 workload 的整个生命周期，在 k8s serving 中其中入口的作用。</p></li><li><p>Route 会创建 ingress，service 资源对象。其中 ingress 是提供域名供外部访问pa，service 可以理解为 VirtualService 也是一个 k8s service 资源，有两点作用：第一是起着占位作用，防止后续与后续service 冲突，第二是关联 ingress，控制流量和管理流量的作用。</p></li><li><p>Configuration 会创建 revision 资源，且会管理多个不同的 revision 版本。</p></li><li><p>Revision 会创建 imageCache、deployment、podAutoscaler 资源对象。</p><ul><li>imageCache 主要是解决冷启动下载镜像慢的问题；</li><li>deployment 是k8s 资源对象，可实现 pod 的滚动升级，revision 通过管理 deployment 来实现扩缩容，刚创建 deployment 时会设定一个初始副本数（可设为0），但是不同版本的revison 对于不同的 deployment；</li><li>podAutoscaler 是 knative中真正控制扩缩容计算的资源对象，如 kpa 会根据并发数进行伸缩对应的 deployment。</li></ul></li><li><p>podAutoscaler 是真正控制扩缩容的资源对象，它会创建 serverlessService(sks) 资源对象。podAutoscaler 也是通过一个控制器来实现的，其代码主要在 Autoscaler 组件中实现，扩缩容时 Autoscaler 会先根据 metrics 从 queue-proxy 中获取监控指标数据，再通过 decider 计算出扩缩容的期望副本数，然后控制器会将期望的副本数通过 <code>patch 的方式</code> 传给 <code>pa.Spec.ScaleTargetRef</code>，最后扩缩容的具体流程是通过 ScaleTargetRef（如 deployment）来实现的。podAutoscaler 在knative 中支持两种方式：kpa 和 hpa:</p><ul><li>kpa 是基于流量的并发请求数来实现的，优点是：可以实现从 0–&gt;1 或 1–&gt;0 的扩缩容方式，缺点是：只支持 rps 模式的扩缩容方式，knative 默认是使用 kpa 方式。</li><li>hpa 是基于 kubernetes 原生 hpa 方式进行扩缩容，优点是：可以支持 cpu/mem 以及用户自定义的方式进行扩缩容，缺点是：不支持从 0–&gt;1 的扩容，因为当 deployment副本数置为0时流量永远进不来，metrics数据永远为0，这时 hpa 是无法通过 metrics 数据来进行扩缩容的。</li></ul></li><li><p>ServerlessService(sks) 是对 k8s service 之上的一个抽象，主要是用来控制数据流是直接流向服务 Revision（实例数不为零），还是经过 Activator（实例数为0）。sks 会创建 privateService、publicService、publicEndpoints 资源对象。<a href="https://developer.aliyun.com/article/702969">参考</a></p><ul><li>privateService 是标准的 k8s service，通过label selector 来筛选对应的deployment产生的Pod，即 svc 对应的 endpoints 由 k8s 自动管控。</li><li>publicService 是不受 k8s 管控的，没有 label selector，不会像 Private service 一样自动生成 endpoints。publicService 对应的 pubilicEndpoints 由 Knative SKS reconciler 来控制的。</li></ul><p> 注意：sks 工作时有两种模式：proxy和 serve。</p><ul><li>proxy 模式： PublicService 后端 endpoints 指向 Activator 对应的pod地址，所有流量都会流经 Activators 处理后再转发到Revision 后端对应的 pod。</li><li>serve 模式： PublicService 后端 endpoints 跟 Private service一样， 所有流量都会直接指向 Revision 后端对应的 pod。</li></ul></li></ol><h2 id="4-knative-serving-组件"><a href="#4-knative-serving-组件" class="headerlink" title="4. knative serving 组件"></a>4. knative serving 组件</h2><p>knative 部署完成后可以在 knative-serving namespace 下看到创建出的组件：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs sql">$ kubectl <span class="hljs-keyword">get</span> pod <span class="hljs-operator">-</span>n knative<span class="hljs-operator">-</span>serving<br>NAME                                READY   STATUS    RESTARTS   AGE<br>activator<span class="hljs-number">-8</span>bdsjwle879<span class="hljs-operator">-</span>z233d          <span class="hljs-number">2</span><span class="hljs-operator">/</span><span class="hljs-number">2</span>     <span class="hljs-keyword">Running</span>   <span class="hljs-number">0</span>          <span class="hljs-number">2</span>d<br>autoscaler<span class="hljs-operator">-</span>dcff9585622<span class="hljs-operator">-</span>pr998         <span class="hljs-number">2</span><span class="hljs-operator">/</span><span class="hljs-number">2</span>     <span class="hljs-keyword">Running</span>   <span class="hljs-number">1</span>          <span class="hljs-number">2</span>d<br>autoscaler<span class="hljs-operator">-</span>hpa<span class="hljs-number">-584</span>s08ssdd8<span class="hljs-number">-89</span>hjk     <span class="hljs-number">2</span><span class="hljs-operator">/</span><span class="hljs-number">2</span>     <span class="hljs-keyword">Running</span>   <span class="hljs-number">2</span>          <span class="hljs-number">2</span>d<br>controller<span class="hljs-operator">-</span>k98812mf<span class="hljs-number">-89</span>hjhsjds        <span class="hljs-number">2</span><span class="hljs-operator">/</span><span class="hljs-number">2</span>     <span class="hljs-keyword">Running</span>   <span class="hljs-number">0</span>          <span class="hljs-number">2</span>d<br>webhook<span class="hljs-number">-68923</span>ji001<span class="hljs-number">-90</span>xdfs            <span class="hljs-number">2</span><span class="hljs-operator">/</span><span class="hljs-number">2</span>     <span class="hljs-keyword">Running</span>   <span class="hljs-number">5</span>          <span class="hljs-number">2</span>d<br></code></pre></td></tr></table></figure><p>serving 共有 6个主要的组件，其中 5个在 knative-serving 这个 namespace 下面，分别为 activator、autoscaler、autoscaler-hpa、controller、webhook 这五个组件；还有一个 queue，运行在每个应用的 pod里，作为 pod 的 sidecar 存在。</p><p><strong>1. Activator：</strong><br> Activator 主要目的是缓存请求和上报请求指标给Autoscaler，主要功能是在冷启动（从零启动）中缓存请求、突发流量过程中对请求进行负载均衡。</p><ul><li><p>knative 冷启动（从零启动）过程：当对某个 Revision 实例从 0-&gt;1 进行冷启动时，流量请求会先经过 Activator 而不是直接到 Revision 的 Pod实例，即当流量从 Ingress 到达 Activator 后会先缓存这些请求，同时 activator 会携带请求指标（请求并发数）去触发 Autoscaler 扩容实例，当 Revision 中的实例 ready 后，Activator 才会将它缓存的请求转发到新的实例中。同时 sks 的模式也从 proxy 模式变为 server 模式。</p></li><li><p>缓存突发流量过程：为了避免已存在 Pod 的流量过载，Activator 还会充当一个负载均衡器的作用。系统会根据不同的情况来决定是否让 Activator 缓冲并转发请求，当一个应用下中有足够多的 Pod实例时，Activator 将不再转发请求，sks 会被置为 serve状态，请求会直接路由到 Revision 中的 pod 中，从而降低转发带来的网络性能开销。但如果应用下的 pod 数量不够时，activator 会根据请求量来决定将流量转发到哪个实例中，且在不超过设置的负载并发量的前提下，activator 会尽量将所有请求平均分发到后端所有Pod上。</p></li></ul><p>注意：上报指标时，Activator 通过 websocket长连接方式实时上报指标给 Autoscaler，这样能在一定程度上缩短冷启动的时间。而在正常的扩缩容场景中 queue-proxy 与 Autoscaler 之间的数据流通方式是，Autoscaler的 metrics 会自动发现应用的Pod，然后到会主动通过 queue-proxy 指定的端口拉取指标。</p><p><strong>2. Autoscaler：</strong></p><p>Autoscaler 主要负责收集来自 Activator/queue-proxy 中的指标数据，然后通过其中的 Kpa 模块进行实时的扩缩容操作。其逻辑上由 metrics Collector、decider、pa 三部分组成：</p><ul><li>metrics Collector: 主要是从 queue-proxy 中收集每个实例的指标，然后对指标数据进行聚合。为了实现扩缩容，Collector会搜集所有应用实例的样本，并将收d到的样本反映到整个集群。</li><li>decider：从 metrics Collector 中获取聚合后的指标数据，并通过一定的约束算法计算出扩缩容期望的副本数，之后将该副本数推荐给 pa。简单的计算公式如下：期望的实例个数 = 系统所需的并发数/每个实例的并发数</li><li>kpa（PodAutoscaler）： kpa 本身是通过 controller 的方式来实现/维护扩缩容操作的，kpa 会从 decider 中拿到期望的副本数，然后通过 patch 的方式对 deployment 进行更新，实现扩缩容操作。注意 kpa 只支持 rps 方式扩缩容，不支持 cpu/mem 方式扩缩容。</li></ul><p>另外，扩缩的实例个数也会受到 Revision 中最大最小实例数的限制。同时 Autoscaler 还会计算当前系统中剩余多少突发请求容量（可扩缩容多少实例）从而决定Activator 是否需要代理转发请求。</p><p><strong>3. Autoscaler-hpa：</strong><br>通过 k8s 原生的 hpa 方式进行扩缩容，支持 cpu/mem 以及用户自定义指标的方式进行扩缩容，但是只支持从 1–&gt;N 的扩容，不支持从 0–&gt;1 的方式进行扩容。</p><p><strong>4. Controller：</strong> 负责对 Service 整个生命周期的管理，通过 informer 机制和控制器模式对 Service、Route、Configuration、Revision、SKS等资源对象进行 CURD 操作，并不断调协使其达到期望状态。</p><p><strong>5. Webhook：</strong><br>主要负责对创建和更新的参数进行校验。</p><p><strong>6. Queue：</strong><br>负载拦截转发给 Pod 的请求，统计 Pod 的请求并发量，并将请求并发量等指标上报给 autoscaler 对应用进行扩缩容。queue-proxy 是一个伴随着用户容器运行的 Sidecar 容器，跟用户容器部署在同一个 Pod 中。发送到应用程序实例的每个请求都首先通queue-proxy。<br>queue-proxy 的主要作用是统计和控制到达业务容器的请求并发量，当对一个 Revision 设置了并发量之后（比如设置了5），queue-proxy 会确保同一时间不会有超过5个请求打到业务容器。当有超过5个请求到来时，queue-proxy会先把请求暂存在自己的队列 queue 里，（这也是为什么名字里有个queue的由来）。queue-proxy 会统计进来的请求量，同时会通过指定端口提供平均并发量和 rps（每秒请求量）给 autoscaler。</p><p>此外 queue-proxy 还有其他的一些功能：</p><ul><li>应用请求数等指标统计</li><li>Pod健康检查（k8s探针）</li><li>代理转发流量</li><li>判断Ingress是否ready需要通过访问queue-proxy实现</li></ul><p>参考：</p><ul><li><a href="https://blog.csdn.net/zhangoic/article/details/104053421">https://blog.csdn.net/zhangoic/article/details/104053421</a></li><li><a href="https://developer.aliyun.com/article/722412">https://developer.aliyun.com/article/722412</a></li><li><a href="https://developer.aliyun.com/article/722193">https://developer.aliyun.com/article/722193</a></li></ul><h2 id="5-knative-serving-扩缩容简介"><a href="#5-knative-serving-扩缩容简介" class="headerlink" title="5. knative serving 扩缩容简介"></a>5. knative serving 扩缩容简介</h2><p>根据上面的介绍，我们知道 knative 中自定义的各种资源，并简单介绍了knative 中主要组件的一些功能，下面将主要介绍 knative 的扩缩容逻辑，knative Serving 模块的核心原理如下图所示，图中的 Route 可以理解成是 Istio Gateway 的角色。</p><p><img src="/2021/11/20/serverless-knative-summary-2021-11-20-knative-summary/knative-scaling01.png" alt="scaling01"></p><ul><li>当 pod 数为零进来的流量就会指到 Activator 上面；然后 activator 会触发 autoscaler 扩容实例，最后流量才会从 activator 转发到实例上。</li><li>当 Pod 数不为零时流量就会指到对应的 Pod 上面，此时流量不经过 Activator。knative Serving 会为每个 POD 注入 QUEUE 代理容器 (queue-proxy)，该容器负责向 Autoscaler 报告用户容器的并发指标。Autoscaler 接收到这些指标之后，会根据并发请求数及相应的算法，调整 Deployment 的 POD 数量，从而实现自动扩缩容。</li></ul><p>更详细的扩缩容逻辑如下所示。</p><h3 id="5-1-冷启动（从零开始扩）"><a href="#5-1-冷启动（从零开始扩）" class="headerlink" title="5.1. 冷启动（从零开始扩）"></a>5.1. 冷启动（从零开始扩）</h3><p>冷启动是指当 revision 下的 pod 数量为 0 时 knative 如何实现从 0–&gt;1 的扩容逻辑，其流程可以用如下图所示：</p><p><img src="/2021/11/20/serverless-knative-summary-2021-11-20-knative-summary/scale-from-0.png" alt="0-1"></p><p>根据上图可知,如果 Revision的实例数为零时，此时系统中有一个请求试图达到该 Revision，为了正常服务系统需要将扩容。流量的处理流程如下所示：</p><ul><li><p>一方面，流量从 ingress 流入，ingress 会将流量转发到 public Service，但由于此时 sks 为 proxy 模式，public Service 中的 endpoints 保存的是 activator 的 ip 地址，所以流量会流入到 activator(1) 中，activator 会缓存流量并计算并发请求数，随后 activator 会触发 autoscaler(2.1) 进行扩容操作，同时不断 watch(2.2) private Service (k8s 原生 service) 中的 endpoints 数据，一但 endpoints 中有 pod 数据(7)（只有健康检查通过的 pod， 能接入流量的 pod 才能加入到 endpoints 中）activator 会将缓存的流量转发给的 pod 实例(8.2)。</p></li><li><p>另一方面，Autoscaler 收到 Activator 发送的指标后，会立即启动扩容的逻辑，即 metrics 会将抓取到的指标数据交给 decider（3），随后 decider 计算出期望副本数推荐给 kpa（4），kpa 会通过 deployment 来进行扩容（5.1）(6)，同时会将 sks 的模式改成 serve 模式（5.2）。这个过程的得出的结论是：至少一个Pod要被创造出来。Deployment创建 Pod 后会更新 Private service 的 endpoint（7）， sks 控制器监听到 Private service 的变化后会更新 Public service 并使其与 Private service 的 endpoints 保持一致（8.1）。</p></li></ul><p>综上所述，流量先经过 activator 缓存并触发扩缩容等一系列操作后才将流量转发到健康实例上，最终完成 revision 的冷启动扩容。</p><h3 id="5-2-稳定状态正常扩缩容"><a href="#5-2-稳定状态正常扩缩容" class="headerlink" title="5.2. 稳定状态正常扩缩容"></a>5.2. 稳定状态正常扩缩容</h3><p>稳定扩缩容状态是指 knative 如何实现 m–&gt;n (m 和 n 都不能为 0) 扩缩容，如上图可知其（稳定状态）流程如下：</p><p><img src="/2021/11/20/serverless-knative-summary-2021-11-20-knative-summary/scale-up-down.png" alt="m-n"></p><p><strong>简单过程可理解为：流量从 ingress 流入，经过 public Service 后直接流向了后端实例上。</strong><br>详细过程如下：<br>流量通过 ingress 路由到 Public service 后 ，此时 Public service 的 endpoints 对应 Revision 后端中的 pods，即流量可直接到达 endpoints 中。在 knative autoscaler 扩缩容逻辑中，Autoscaler 通过 metrics 不断的从 queue-proxy 暴露的端口中抓取指标，通过 decider 模块计算出副本数的推荐值，最后再通过 pa 来不断的修改 revision 中 deployment 并通过 deployment 来不断调协 Revision 中实例。此外 sks 分模式一直是 serve 模式，并会不断监控 Private service 的状态，保持 Public service 的 endpoints 与 Private service 一致。</p><h3 id="5-3-缩容到零"><a href="#5-3-缩容到零" class="headerlink" title="5.3. 缩容到零"></a>5.3. 缩容到零</h3><p>缩容到零是指 knative 如何实现从 1–&gt;0 的缩容逻辑，其流程如下：</p><p><img src="/2021/11/20/serverless-knative-summary-2021-11-20-knative-summary/scale-to-0.png" alt="1-0"></p><p>在正常的处理逻辑中，Autoscaler 会不断通过 queue-proxy 获取 Revision 实例的请求指标（1），然后根据这些指标数据来决定扩缩容逻辑。而当系统中某个 Revision 不再接收到请求（此时 Activator 和 queue-proxy 收到的请求数都为 0）（2），autoscaler 通过 metrics 从 queue-proxy 中抓取到的指标数据也为 0，此时 Decider 会将推荐值0 交给 pa (3)，随后 pa 会修改 deployment 的副本数为 0（4.2），之后的所容逻辑有 deployment 控制器来控制。而在 knative 需要删掉 Revision 的最后一个Pod之前，会有些特殊处理：</p><ul><li>pa 控制器会将 SKS 变为 proxy 模式（4.1），此时 SKS 的 Public service 后端的 endpoints 变为 Activator 的IP，所有的流量都直接导到 Activator（6.2）中。</li><li>在删除最后一个 pod 之前会设置一个宽限期（可通过_scale-to-zero-grace-period_进行配置），如果在宽限期内依然没有流量到来，则Revision 的最后一个 pod将被删除（5）。</li></ul><h2 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h2><p>本文主要讲述了 knative serving 是如何实现的，knative serving 是建立在 k8s 之上的服务，通过 crd 的方式来实现其业务逻辑。本文先是介绍了实现 knative 的技术背景，随后介绍了knative 的工作原理以及 crd 相关资源对象，之后简单介绍了 knative 中几个重要组件，knative 的crd 资源通过这些组件的实现来完成 knative 的扩缩容逻辑。最后详细介绍了 knative 的三个特殊的扩缩容逻辑，冷启动扩容、正常扩缩容、缩容到零，其中零启动扩容方式是 knative 技术的一大亮点，但其也存在诸多缺点，其中最大的确实就是冷启动的时间太长（号称冷启动时间为 6s），这在绝大多数应用场景中是不可忍受的，所以对 knative 的性能优化将是社区和行业亟待解决的一个大问题，其中<a href="https://cloud.tencent.com/developer/news/652064">网易</a>对此有个优化实践.</p><h2 id="7-参考"><a href="#7-参考" class="headerlink" title="7. 参考"></a>7. 参考</h2><ul><li><a href="https://github.com/knative/serving/tree/release-1.5/">https://github.com/knative/serving/tree/release-1.5/</a></li><li><a href="https://github.com/knative/serving/tree/release-1.5/docs/scaling">https://github.com/knative/serving/tree/release-1.5/docs/scaling</a></li><li><a href="https://zhuanlan.zhihu.com/p/353216880">https://zhuanlan.zhihu.com/p/353216880</a></li><li><a href="https://cloud.tencent.com/developer/news/617053">https://cloud.tencent.com/developer/news/617053</a></li><li><a href="https://cloud.tencent.com/developer/news/652064">https://cloud.tencent.com/developer/news/652064</a></li><li><a href="https://blog.tianfeiyu.com/source-code-reading-notes/knative/knative_serving.html">https://blog.tianfeiyu.com/source-code-reading-notes/knative/knative_serving.html</a></li><li><a href="https://blog.csdn.net/zhangoic/article/details/104053421">https://blog.csdn.net/zhangoic/article/details/104053421</a></li><li><a href="https://developer.aliyun.com/article/722412">https://developer.aliyun.com/article/722412</a></li><li><a href="https://developer.aliyun.com/article/722193">https://developer.aliyun.com/article/722193</a></li><li><a href="https://zhuanlan.zhihu.com/p/172431080">https://zhuanlan.zhihu.com/p/172431080</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>serverless</category>
      
      <category>knative</category>
      
    </categories>
    
    
    <tags>
      
      <tag>knative</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes 默认调度器调度策略概述</title>
    <link href="/2021/10/28/kubernetes-kube-scheduler-algorithm-2021-10-28-scheduler-algorithm/"/>
    <url>/2021/10/28/kubernetes-kube-scheduler-algorithm-2021-10-28-scheduler-algorithm/</url>
    
    <content type="html"><![CDATA[<h1 id="Kubernetes-默认调度器调度策略概述"><a href="#Kubernetes-默认调度器调度策略概述" class="headerlink" title="Kubernetes 默认调度器调度策略概述"></a>Kubernetes 默认调度器调度策略概述</h1><p>在上一篇文章<a href="../summary/scheduler-summary.md">Kubernetes Scheduler 概述</a> 中有讲述 Kubernetes scheduler 的工作原理和架构，其中有讲述到 kube-scheduler 的核心就是通过调度算法（预选算法和优选算法）来选出合适的节点供 pod 节点使用。本篇文章主要介绍汇总下 Kubernetes 默认调度器中使用到的 Predicates(预选算法) 和 Priorities（优选算法）。</p><h2 id="1-Predicates-预选算法"><a href="#1-Predicates-预选算法" class="headerlink" title="1. Predicates 预选算法"></a>1. Predicates 预选算法</h2><p>Predicates 预选算法在调度过程中可以理解为 Filter 的功能，即：它按照指定的调度策略，从当前集群的所有节点中，“过滤”出一系列符合条件的节点。这些节点，都是可以运行待调度 Pod 的宿主机。在 Kubernetes 中，默认的调度策略按照类型分类可以有如下四种：</p><ul><li>第一种类型，叫作 GeneralPredicates 类型的过滤策略：这种类型的策略主要负责最基础的调度策略。</li><li>第二种是与 Volume 相关的过滤规则：主要负责的是跟容器持久化 Volume 相关的调度策略。</li><li>第三种是与宿主机相关的过滤规则：主要考察待调度 Pod 是否满足 Node 本身的某些条件。</li><li>第四种是与 Pod相关的过滤规则。这一组规则，跟 GeneralPredicates 大多数是重合的。另有比较特殊的是 PodAffinityPredicate（pod亲和性相关的过滤规则）。</li></ul><h3 id="1-1-GeneralPredicates-过滤规则"><a href="#1-1-GeneralPredicates-过滤规则" class="headerlink" title="1.1. GeneralPredicates 过滤规则"></a>1.1. GeneralPredicates 过滤规则</h3><p>这一组过滤规则，负责的是最基础的调度策略。部分规则统计如下表所示：<br>|规则|说明|<br>|—|—|<br>|PodFitsResources| 节点上剩余的资源（如 CPU 和内存）是否满足 Pod 请求的资源，计算时会根据 Pod 的 requests 字段进行判断求值|<br>|PodFitsHost| 如果 Pod 指定了 NodeName，检查节点名称是否和 NodeName 匹配。|<br>|PodFitsHostPorts | 如果 Pod 中定义了 spec.hostPort 属性，那么需要先检查这个指定端口是否已经被节点上其他服务占用了|<br>|PodMatchNodeSelector |检查 Pod 的 nodeSelector 或者 nodeAffinity 指定的节点，是否与node 的标签值匹配。|</p><p>像上面这样一组 GeneralPredicates，正是 Kubernetes 考察一个 Pod 能不能运行在一个 Node 上最基本的过滤条件。所以，GeneralPredicates 也会被其他组件（比如kubelet）直接调用。在文章<a href="../summary/scheduler-summary.md">Kubernetes Scheduler 概述</a> 有提到过，kubelet 在启动 Pod 前，会执行一个 Admit 操作来进行二次确认。这里二次确认的规则，就是执行一遍 GeneralPredicates 过滤规则。</p><h3 id="1-2-与-Volume-相关的过滤规则"><a href="#1-2-与-Volume-相关的过滤规则" class="headerlink" title="1.2. 与 Volume 相关的过滤规则"></a>1.2. 与 Volume 相关的过滤规则</h3><p>这一组过滤规则，负责的是跟容器持久化 Volume 相关的调度策略。部分规则统计如下：</p><table><thead><tr><th>规则</th><th>说明</th></tr></thead><tbody><tr><td>NoDiskConflict</td><td>检查是否多个 Pod 之间声明挂载的持久化 Volume 是否有冲突。比如，AWS EBS 类型的 Volume，是不允许被两个 Pod 同时使用的。</td></tr><tr><td>NoVolumeZoneConflict</td><td>检测 Pod 请求的 Volumes 在节点上是否可用，因为某些存储卷存在区域调度约束</td></tr><tr><td>MaxPDVolumeCountPredicate</td><td>检查一个节点上某种类型的持久化 Volume 是不是已经超过了一定数目，如果是的话，那么声明使用该类型持久化 Volume 的 Pod 就不能再调度到这个节点了。</td></tr><tr><td>VolumeZonePredicate</td><td>检查持久化 Volume 的 Zone（高可用域）标签，是否与待考察节点的 Zone 标签相匹配。</td></tr><tr><td>VolumeBindingPredicate</td><td>检查 Pod 对应的 PV 的 nodeAffinity 字段，是否跟某个节点的标签相匹配。这个过滤规则在 local PV 的延迟绑定中非常有用，Local Persistent Volume（本地持久化卷），必须使用 nodeAffinity 来跟某个具体的节点绑定。此外，如果该 Pod 的 PVC 还没有跟具体的 PV 绑定的话，调度器还要负责检查所有待绑定 PV，当有可用的 PV 存在并且该 PV 的 nodeAffinity 与待考察节点一致时，这条规则才会返回“成功”。</td></tr></tbody></table><h3 id="1-3-与宿主机-node-节点相关的过滤规则"><a href="#1-3-与宿主机-node-节点相关的过滤规则" class="headerlink" title="1.3. 与宿主机 node 节点相关的过滤规则"></a>1.3. 与宿主机 node 节点相关的过滤规则</h3><p>这一组规则，主要考察待调度 Pod 是否满足 Node 本身的某些条件。部分规则统计如下：<br>|规则|说明|<br>|—|—|<br>|CheckNodeDiskPressure| 检查节点磁盘空间是否符合要求|<br>|PodToleratesNodeTaints| 负责检查我们经常用到的 Node 的“污点”机制。只有当 Pod 的 Toleration 字段与 Node 的 Taint 字段能够匹配的时候，这个 Pod 才能被调度到该节点上。|<br>|NodeMemoryPressurePredicate | 检查当前节点的内存是不是已经不够充足，如果是的话，那么待调度 Pod 就不能被调度到该节点上。|<br>|CheckNodeCondition | Node 可以上报其自身的状态，如磁盘、网络不可用，表明 kubelet 未准备好运行 Pod，如果节点被设置成这种状态，那么 Pod 不会被调度到这个节点上|</p><h3 id="1-4-与-pod-相关的过滤规则（主要是-pod-亲和性过滤）"><a href="#1-4-与-pod-相关的过滤规则（主要是-pod-亲和性过滤）" class="headerlink" title="1.4. 与 pod 相关的过滤规则（主要是 pod 亲和性过滤）"></a>1.4. 与 pod 相关的过滤规则（主要是 pod 亲和性过滤）</h3><p>这一组规则，跟 GeneralPredicates 大多数是重合的。另外一个比较特殊的是 <code>PodAffinityPredicate</code> 这个规则，检查待调度 Pod 与 Node 上的已有 Pod 之间的亲密（affinity）和反亲密（anti-affinity）关系，即检查 pod 之间的亲和性和反亲和性关系。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><br><span class="hljs-attr">metadata:</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">with-pod-antiaffinity</span><br><span class="hljs-attr">spec:</span><br>    <span class="hljs-attr">containers:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">with-pod-affinity</span><br>      <span class="hljs-attr">image:</span> <span class="hljs-string">docker.io/ocpqe/hello-pod</span><br>    <span class="hljs-attr">affinity:</span><br>        <span class="hljs-attr">podAntiAffinity:</span><br>            <span class="hljs-attr">requiredDuringSchedulingIgnoredDuringExecution:</span><br>            <span class="hljs-bullet">-</span> <span class="hljs-attr">weight:</span> <span class="hljs-number">100</span><br>              <span class="hljs-attr">podAffinityTerm:</span><br>                <span class="hljs-attr">labelSelector:</span><br>                    <span class="hljs-attr">matchExpressions:</span><br>                    <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">security</span><br>                      <span class="hljs-attr">operator:</span> <span class="hljs-string">In</span><br>                      <span class="hljs-attr">values:</span><br>                      <span class="hljs-bullet">-</span> <span class="hljs-string">S2</span><br>                <span class="hljs-attr">topologyKey:</span> <span class="hljs-string">kubernetes.io/hostname</span><br>        <span class="hljs-attr">podAffinity:</span><br>            <span class="hljs-attr">requiredDuringSchedulingIgnoredDuringExecution:</span><br>            <span class="hljs-bullet">-</span> <span class="hljs-attr">labelSelector:</span><br>                <span class="hljs-attr">matchExpressions:</span><br>                <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">security</span><br>                  <span class="hljs-attr">operator:</span> <span class="hljs-string">In</span><br>                  <span class="hljs-attr">values:</span><br>                  <span class="hljs-bullet">-</span> <span class="hljs-string">S1</span><br>            <span class="hljs-attr">topologyKey:</span> <span class="hljs-string">failure-domain.beta.kubernetes.io/zone</span><br></code></pre></td></tr></table></figure><p>如上这个例子中所示定义了 pod 之间否反亲和性，表示这个 Pod 不希望跟任何携带了 security=S2 标签的 Pod 存在于同一个 Node 上。<strong>注意：</strong> 对于 PodAffinityPredicate 可以通过 <code>topologyKey</code> 关键字指定规则生效的作用域，比如上面这条规则，只会对携带了 Key 是kubernetes.io/hostname 标签的 Node 上的所有 pods 执行pod 反亲和性过滤判断。而后面的 podAffinity（pod 亲和性判断）表示该 pod只会被调度到已经有携带了 security=S1 标签的 Pod 运行的 Node上，同时这条规则的作用域是所有携带 Key 是failure-domain.beta.kubernetes.io/zone 标签的 Node 上操作。<br>同时 <code>requiredDuringSchedulingIgnoredDuringExecution</code> 字段表示的含义是：调度器在调度 pod 时必须考虑这个过滤规则，即对 pod 进行过滤检查（requiredDuringScheduling）；但如果是已经运行的 Pod 发生变化（如 Label 被修改），导致该 Pod 不再适合运行在这个 Node 上了，Kubernetes 不会对 pod进行主动修正（IgnoredDuringExecution），即 kubelet 不会 pod 执行驱逐操作。</p><p><strong>注意:</strong> 调度器在执行上面的过滤规则时，会异步启动 <code>16 个 goroutine</code> 来执行过滤算法，最终会得到满足条件的 node 列表。同时在为每个 Node 执行 Predicates 时，调度器会按照固定的顺序来进行检查。这个顺序是按照 Predicates 本身的含义来确定的。比如，宿主机相关的 Predicates 会被放在相对靠前的位置进行检查。要不然的话，在一台资源已经严重不足的宿主机上，上来就开始计算 PodAffinityPredicate，是没有实际意义的。</p><p>除了这些过滤算法之外，还有一些其他的算法，更多更详细的我们可以查看源码文件：k8s.io/kubernetes/pkg/scheduler/algorithm/predicates/predicates.go。</p><h2 id="2-Priorities-优选算法"><a href="#2-Priorities-优选算法" class="headerlink" title="2. Priorities 优选算法"></a>2. Priorities 优选算法</h2><p>在预选阶段完成了节点的“过滤”之后，优选阶段的工作就是为这些节点打分，这里打分的范围是 0-10 分，得分最高的节点就是最后被 Pod 绑定的最佳节点。部分规则统计如下：<br>|规则|说明|<br>|—|—|<br>|LeastRequestedPriority|通过计算node cpu 和内存的剩余资源数来打分，剩余资源多的分数越高，即这个算法实际上就是在选择空闲资源（CPU 和 Memory）最多的宿主机。|<br>|BalancedResourceAllocation | 通过计算cpu、memory、磁盘等资源分配之间的距离，资源分配差距越小的节点分数越高。即调度时会从所有节点中优先选择各种资源分配最均衡的那个节点，从而避免一个节点上 CPU 被大量分配、而 Memory 大量剩余的情况出现。常与 LeastRequestedPriority 一起配合使用。|<br>|ImageLocalityPriority|如果待调度 Pod 需要使用的镜像很大，并且已经存在于某些 Node上，那么这些 Node 的得分就会比较高。这个算法的缺点是会造成调度pod的堆叠现象，即所有使用该镜像的pod 都会调度到该 node 上。为了避免引发调度堆叠，调度器在计算得分的时候必须根据镜像的分布进行优化，即：如果大镜像分布的节点数目很少，那么这些节点的权重就会被调低，从而降低引起调度堆叠的风险。|<br>|SelectorSpreadPriority | 为了更好的高可用，对同属于一个 Deployment 或者 RC 下面的多个 Pod 副本，尽量调度到多个不同的节点上，当一个 Pod 被调度的时候，会先去查找该 Pod 对应的 controller，然后查看该 controller 下面的已存在的 Pod，运行 Pod 越少的节点权重越高|<br>|InterPodAffinityPriority|遍历 Pod 的亲和性条目，并将那些能够匹配到给定节点的条目的权重相加，结果值越大的节点得分越高|<br>|MostRequestedPriority | 空闲资源比例越低的 Node 得分越高，这个调度策略会把你的所有的工作负载（Pod）调度到尽量少的节点上，类似于资源集中使用|<br>|RequestedToCapacityRatioPriority| 为 Node 上每个资源占用比例设定得分值，给资源打分函数在打分时使用|<br>|NodePreferAvoidPodsPriority| 这个策略将根据 Node 的注解信息中是否含有 scheduler.alpha.kubernetes.io/preferAvoidPods 来计算其优先级，使用这个策略可以将两个不同 Pod 运行在不同的 Node 上|<br>|NodeAffinityPriority | 基于 Pod 属性中 PreferredDuringSchedulingIgnoredDuringExecution 来进行 Node 亲和性调度|<br>|TaintTolerationPriority| 基于 Pod 中对每个 Node 上污点容忍程度进行优先级评估，这个策略能够调整待选 Node 的排名|<br>| ServiceSpreadingPriority | 这个调度策略的主要目的是确保将归属于同一个 Service 的 Pod 调度到不同的 Node 上，这个策略的最终目的是：即使在一个 Node 宕机之后 Service 也具有很强容灾能力。|<br>|CalculateAntiAffinityPriorityMap| 这个策略主要是用来实现 Pod 反亲和的|<br>|EqualPriorityMap| 将所有的 Node 设置成相同的权重，默认为 1|</p><p>除了这些策略之外，还有很多其他的策略，同样我们可以查看源码文件：k8s.io/kubernetes/pkg/scheduler/algorithm/priorities/ 了解更多信息。每一个优选函数会返回一个 0-10 的分数，分数越高表示节点越优，同时每一个函数也会对应一个表示权重的值。最终主机的得分用以下公式计算得出：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">finalScoreNode</span> = (weight<span class="hljs-number">1</span> * priorityFunc<span class="hljs-number">1</span>) + (weight<span class="hljs-number">2</span> * priorityFunc<span class="hljs-number">2</span>) + … + (weightn * priorityFuncn)<br></code></pre></td></tr></table></figure><h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h2><p>本文主要讲述了 Kubernetes 调度器的 Predicates 和 Priorities 里默认调度规则的主要工作原理。在实际的执行过程中，调度器里关于集群和 Pod 的信息都已经缓存化，所以这些算法的执行过程还是比较快的。此外，对于比较复杂的调度算法来说，比如 PodAffinityPredicate，它们在计算的时候不只关注待调度 Pod 和待考察 Node，还需要关注整个集群的信息，比如，遍历所有节点，读取它们的 Labels。这时候，Kubernetes 调度器会在为每个待调度 Pod 执行该调度算法之前，先将算法需要的集群信息初步计算一遍，然后缓存起来。这样，在真正执行该算法的时候，调度器只需要读取缓存信息进行计算即可，从而避免了为每个 Node 计算 Predicates 的时候反复获取和计算整个集群的信息。</p><p>此外，除了本篇讲述的这些规则，Kubernetes 调度器里还有一些默认不会开启的策略。你可以通过为 kube-scheduler 指定一个配置文件或者创建一个 ConfigMap ，来配置哪些规则需要开启、哪些规则需要关闭。并且，你可以通过为 Priorities 设置权重，来控制调度器的调度行为。</p>]]></content>
    
    
    <categories>
      
      <category>kubernetes</category>
      
      <category>kube-scheduler</category>
      
      <category>调度器系列</category>
      
      <category>kube-scheduler</category>
      
    </categories>
    
    
    <tags>
      
      <tag>kubernetes</tag>
      
      <tag>scheduler</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes Scheduler 概述</title>
    <link href="/2021/10/20/kubernetes-kube-scheduler-summary-2021-10-20-scheduler-summary/"/>
    <url>/2021/10/20/kubernetes-kube-scheduler-summary-2021-10-20-scheduler-summary/</url>
    
    <content type="html"><![CDATA[<h1 id="Kubernetes-Scheduler-概述"><a href="#Kubernetes-Scheduler-概述" class="headerlink" title="Kubernetes Scheduler 概述"></a>Kubernetes Scheduler 概述</h1><p>调度器的主要职责是负责整个集群资源的调度功能，根据特定的调度算法和策略，将业务调度到最优的工作节点上去，达到更加合理、更加充分利用集群资源的目的。而 Kubernetes 中的默认调度器（default-scheduler）是 kube-scheduler，其主要职责就是为一个新创建出来的 Pod，寻找一个最<br>合适的节点（Node）。其主要思想可简单表述成下面两个阶段：</p><ol><li>根据<strong>过滤算法</strong>从集群的所有节点中挑选出所有可以运行该 Pod 的节点；</li><li>从第一步的结果中，再根据<strong>优选算法</strong>挑选一个最符合条件的节点作为最终结果。<br>所以在具体的调度流程中，默认调度器会首先调用一组叫作 Predicate 的调度算法，来检查每个 Node。然后，再调用一组叫作 Priority 的调度算法，来给上一步得到的结果里的每个 Node 打分。最终的调度结果，就是得分最高的那个 Node。调度器简单逻辑如下所示：</li></ol><p><img src="/2021/10/20/kubernetes-kube-scheduler-summary-2021-10-20-scheduler-summary/scheduler-summary01.png" alt="summary01"></p><h2 id="kube-scheduler-调度器的工作原理"><a href="#kube-scheduler-调度器的工作原理" class="headerlink" title="kube-scheduler 调度器的工作原理"></a>kube-scheduler 调度器的工作原理</h2><p>从上面的简介中可知调度器主要包括<strong>预选</strong>和<strong>优选</strong>两个大阶段，但为了更好的提高调度器的性能 Kubernetes 对默认调度器的设计非常复杂。个人看了张磊老师的文章，觉得大佬写的通俗易懂，在这里我主要做个记录吧，后面可借鉴这个文章去分析源码是如何实现的。在 Kubernetes 中，默认调度器的工作原理，可以用如下所示的一幅示意图来表示。</p><p><img src="/2021/10/20/kubernetes-kube-scheduler-summary-2021-10-20-scheduler-summary/scheduler-summary02.png" alt="summary02"></p><p>可以看到，<strong>Kubernetes 的调度器的核心，实际上就是两个相互独立的控制循环。</strong><br>其中，<strong>第一个控制循环，我们可以称之为 Informer Path。</strong> 它的主要目的，是启动一系列 Informer，用来<code>监听（Watch）Etcd 中 Pod、Node、Service 等与调度相关的 API 对象的变化</code>。比如，当一个待调度 Pod（即：它的 nodeName 字段是空的）被创建出来之后，调度器就会通过 Pod Informer 的 Handler，将这个待调度 Pod 添加进调度队列。在默认情况下，Kubernetes 的调度队列是一个 PriorityQueue（优先级队列），并且当某些集群信息发生变化的时候，调度器还会对调度队列里的内容进行一些特殊操作。这里的设计，主要是出于调度优先级和抢占的考虑，我会在后面的文章中再详细介绍这部分内容。<br>此外，Kubernetes 的默认调度器还要负责<code>对调度器缓存（即：scheduler cache）进行更新</code>。事实上，Kubernetes 调度部分进行性能优化的一个最根本原则，就是尽最大可能将集群信息Cache 化，以便从根本上提高 Predicate 和 Priority 调度算法的执行效率。</p><p>而<strong>第二个控制循环，是调度器负责 Pod 调度的主循环，我们可以称之为 Scheduling Path。</strong> Scheduling Path 的主要逻辑，就是不断地从调度队列里出队一个 Pod。然后，调用Predicates 算法进行“过滤”。这一步“过滤”得到的一组 Node，就是所有可以运行这个Pod 的宿主机列表。当然，Predicates 算法需要的 Node 信息，都是从 Scheduler Cache 里直接拿到的，这是调度器保证算法执行效率的主要手段之一。</p><p>接下来，调度器就会再调用 Priorities 算法为上述列表里的 Node 打分，分数从 0 到 10。得分最高的 Node，就会作为这次调度的结果。</p><p>调度算法执行完成后，调度器就需要将 Pod 对象的 nodeName 字段的值，修改为上述 Node的名字。<strong>这个步骤在 Kubernetes 里面被称作 Bind。</strong> 但是，为了不在关键调度路径里远程访问 APIServer，Kubernetes 的默认调度器在 Bind 阶段，只会更新 Scheduler Cache 里的 Pod 和 Node 的信息。<strong>这种基于“乐观”假设的 API 对象更新方式，在 Kubernetes 里被称作 Assume。</strong> Assume 之后，调度器才会创建一个 Goroutine 来异步地向 APIServer 发起更新 Pod 的请求，来真正完成 Bind 操作。如果这次异步的 Bind 过程失败了，其实也没有太大关系，等Scheduler Cache 同步之后一切就会恢复正常。</p><p><strong>注意：</strong> 调度器调度之前是只监听 spec.nodeName 为空的 pod，并将其添加到调度器的优先级队列中，调度器执行之后是将选中的 node 名字赋值给 pod 的 spec.nodeName 中，即表示调度成功。</p><p>当然，正是由于上述 Kubernetes 调度器的“乐观”绑定的设计，当一个新的 Pod 完成调度需要在某个节点上运行起来之前，该节点上的 <code>kubelet 还会通过一个叫作 Admit 的操作来再次验证该 Pod 是否确实能够运行在该节点上</code>。这一步 Admit 操作，实际上就是把一组叫作GeneralPredicates 的、最基本的调度算法，比如：“资源是否可用”、“端口是否冲突”等再执行一遍，作为 kubelet 端的二次确认。</p><p><strong>除了上述的“Cache 化”和“乐观绑定”，Kubernetes 默认调度器还有一个重要的设计，那就是“无锁化”。</strong> 在 Scheduling Path 上，调度器会启动多个 Goroutine（最多16个）以节点为粒度并发执行 Predicates 算法，从而提高这一阶段的执行效率。而与之类似的，Priorities 算法也会以 MapReduce 的方式并行计算然后再进行汇总。而在这些所有需要并发的路径上，调度器会避免设置任何全局的竞争资源，从而免去了使用锁进行同步带来的巨大的性能损耗。</p><p>所以，在这种思想的指导下，如果你再去查看一下前面的调度器原理图，你就会发现，Kubernetes 调度器只有对调度队列和 Scheduler Cache 进行操作时，才需要加锁。而这两部分操作，都不在 Scheduling Path 的算法执行路径上。</p><p>当然，Kubernetes 调度器的上述设计思想，也是在集群规模不断增长的演进过程中逐步实现的。<strong>尤其是 “Cache 化”，这个变化其实是最近几年 Kubernetes 调度器性能得以提升的一个关键演化。</strong></p><p>不过，随着 Kubernetes 项目发展到今天，它的默认调度器也已经来到了一个关键的十字路口。事实上，Kubernetes 现今发展的主旋律，是整个开源项目的“民主化”。也就是说，Kubernetes 下一步发展的方向，是组件的轻量化、接口化和插件化。所以，我们才有了 CRI、CNI、CSI、CRD、Aggregated APIServer、Initializer、Device Plugin 等各个层级的可扩展能力。可是，默认调度器，却成了 Kubernetes 项目里最后一个没有对外暴露出良好定义过的、可扩展接口的组件。当然随着时间的发展以及用户和市场的需求，Kubernete scheduler 可扩展接口也得到了支持，后面我将详细介绍 Scheduler Framework。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要介绍了 Kubernetes 默认调度器 kube-scheduler 的实现原理，即两个控制循环：Informer path 和 Scheduling path，这两个控制循环实现了对资源依赖的解藕，实现了高性能的调度器。其中 Informer path 与 apiserver 建立通信不断 watch 资源变化情况，并将其本地cache 化；Scheduling path 则不断的从 cache 中读取数据，通过一系列的调度算法后对pod 和资源进行调度和分配，最终将 pod 绑定到合适的节点上。从上面的原理可知，Kubernetes 调度器实现高性能优化的方法主要有以下几个方面：</p><ul><li><strong>优先级队列的实现：</strong> 对于高优先级的 pod 或者特殊的 pod，可以通过指定器优先级来提高被优先调度的可能性。</li><li><strong>资源本地 cache 化：</strong> Informer path 中 pod 添加到优先级队列中和 nodeinfo 添加到本地cache 中。</li><li><strong>Assume 乐观绑定：</strong> bind 阶段不在关键路径调用 apiserver，只更新 scheduler cache 信息。正真的 bind操作通过 goroutine 异步执行。</li><li><strong>并行执行过滤算法和打分算法：</strong> 同一个 pod 在执行不相关的调度算法时会启用 16个 goroutine 来并行计算。</li><li><strong>循环无锁化：</strong> 调度器的主循环中尽量不产生资源竞争，即不添加锁处理；只有在更新优先级队列和 cache 时才会添加锁，避免数据竞争，但这两步都不主循环中。</li><li><strong>提前结束过滤：</strong> 其实为提高调度器的执行性能，在 k8s 1.12 之后会增加一个 percentageOfNodesToScore 字段，即指定需要打分的节点数（百分比），当过滤出来的节点数（满足条件可打分的节点）达到这个指定的值时会停止过滤集群中剩余的pod；只会对过滤出来的指定数量的pod进行打分判断。这种方法在集群规模很小是效果不是很显著，但是在集群规模很大时，调度器的性能提高会非常显著。</li></ul><p>目前 Kubernetes 默认调度器能支持绝大部分场景，对于一些比较特殊复杂的场景中用户可以通过后面介绍的 Scheduler Framework 以及插件机制进行扩展。</p>]]></content>
    
    
    <categories>
      
      <category>kubernetes</category>
      
      <category>kube-scheduler</category>
      
      <category>调度器系列</category>
      
      <category>kube-scheduler</category>
      
    </categories>
    
    
    <tags>
      
      <tag>kubernetes</tag>
      
      <tag>scheduler</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>client-go Informer 机制源码剖析（二）</title>
    <link href="/2021/09/06/kubernetes-client-go-informer-informer-v1-22-02/"/>
    <url>/2021/09/06/kubernetes-client-go-informer-informer-v1-22-02/</url>
    
    <content type="html"><![CDATA[<h1 id="client-go-informer-机制源码剖析（二）"><a href="#client-go-informer-机制源码剖析（二）" class="headerlink" title="client-go informer 机制源码剖析（二）"></a>client-go informer 机制源码剖析（二）</h1><h2 id="之-sharedIndexInformer-具体实现"><a href="#之-sharedIndexInformer-具体实现" class="headerlink" title="之 sharedIndexInformer 具体实现"></a>之 sharedIndexInformer 具体实现</h2><blockquote><p>本文档基于 <code>v1.22</code> 版本</p></blockquote><h2 id="1-Informer-机制回顾"><a href="#1-Informer-机制回顾" class="headerlink" title="1. Informer 机制回顾"></a>1. Informer 机制回顾</h2><p>我们在 <a href="./informer_v1.22_01.md">client-go informer 机制源码剖析（一）</a> 中已经简单介绍了 informer 的使用方式以及其相关结构。如下简单回顾一下 informer 的创建过程：</p><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs livescript">NewSharedInformerFactory<span class="hljs-function"><span class="hljs-params">()</span> --&gt;</span> SharedInformerFactory<span class="hljs-function"><span class="hljs-params">(接口)</span> --&gt;</span> PodInformer<span class="hljs-function"><span class="hljs-params">(具体的 Informer 接口)</span> --&gt;</span> cache.SharedIndexInformer(接口)<br></code></pre></td></tr></table></figure><p>其中 <a href="https://github.com/kubernetes/client-go/blob/master/informers/factory.go#L96">NewSharedInformerFactory()</a> 函数用来构建 informerFactory 工厂函数，会得到包含 k8s 内部所有资源的 <a href="https://github.com/kubernetes/client-go/blob/master/informers/factory.go#L187">SharedInformerFactory</a> 接口，再将具体的的 Informer（如 <a href="https://github.com/kubernetes/client-go/blob/master/informers/core/v1/pod.go#L37">PodInformer</a>）注册到 informerFactory 中完成 informer 的初始化操作，最后再启动 informerFactory，同步 cache 等。而注册具体 Informer 时其实就是初始化/构建 <a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/shared_informer.go#L186">cache.SharedIndexInformer</a>，完成 <code>ListWatch</code>、<code>反射器</code>、<code>deltafifo</code>、<code>indexer</code>、<code>controller</code>、<code>processer(回调函数)</code> 等组件的初始化及启动过程。如下给出一张各个组件之间关系的图（注意：图中的 <code>WorkQueue</code> 和 <code>Control loop</code> 先不讲解，后面会讲解自定义控制器时讲到）：</p><p><img src="/2021/09/06/kubernetes-client-go-informer-informer-v1-22-02/custom_controller.png" alt="custom_controller"></p><h2 id="2-sharedIndexInformer-详解"><a href="#2-sharedIndexInformer-详解" class="headerlink" title="2. sharedIndexInformer 详解"></a>2. sharedIndexInformer 详解</h2><blockquote><p><a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/shared_informer.go">https://github.com/kubernetes/client-go/blob/master/tools/cache/shared_informer.go</a></p></blockquote><h3 id="2-1-sharedIndexInformer-结构体"><a href="#2-1-sharedIndexInformer-结构体" class="headerlink" title="2.1. sharedIndexInformer 结构体"></a>2.1. sharedIndexInformer 结构体</h3><p><code>sharedIndexInformer</code> 实现了 SharedIndexInformer 接口，而 SharedIndexInformer 封装了 SharedInformer 接口，并在其基础上添加了 <code>AddIndexers/GetIndexer</code> 的操作。<a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/shared_informer.go#L287">sharedIndexInformer</a> 包含几个非常重要的组件，如下结构体只显示重要组件：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">//</span> client-go<span class="hljs-regexp">/tools/</span>cache/shared_informer.go<br>type sharedIndexInformer struct &#123;<br>indexer    Indexer   <span class="hljs-regexp">//</span> 本地索引存储，基于 index + threadSafeStore 实现的本地 cache 存储<br>controller Controller <span class="hljs-regexp">//</span> 用于管理 sharedIndexInformer 中的各个组件，并控制各个组件的正常运行<br>processor  *sharedProcessor <span class="hljs-regexp">//</span> 回调处理函数，从 fifo 队列中消费（Pop）消息进行处理<br>listerWatcher ListerWatcher <span class="hljs-regexp">//</span> 反射器通过 listwatch 从 k8s apiserver 的 etcd 中获取数据，并将数据放入 fifo 中<br>    ......<br>&#125;<br></code></pre></td></tr></table></figure><p>上面 4个组件在 sharedIndexInformer 中至关重要，后面会专门讲解，这里只做简单介绍，下图简单描述了下 sharedIndexInformer 结构体内 4个重要组件的关心和流程图：<br><img src="/2021/09/06/kubernetes-client-go-informer-informer-v1-22-02/sharedIndexInformer01.png" alt="sharedindexinformer01"></p><p>下面将根据流程及上图所示各个模块，从上到下逐一讲解其功能：</p><ul><li><code>listerWatcher</code>: 该组件实现 list-watch 机制，其<code>初始化</code>位置在每个具体的 Informer (如 <a href="https://github.com/kubernetes/client-go/blob/master/informers/core/v1/pod.go#L60">PodInformer</a>)中，在 controller 中会将其封装在 <code>反射器</code> 中，使反射器具体有 <code>list-watch</code> 的功能。list-watch 通过 k8s apiservice 从 etcd 中获取数据，并将数据放入 deltafifo 队列中。</li><li><code>controller</code>: 会通过一个 <a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/controller.go#L39">Config</a> 结构体将 <code>deltafifo</code>、<code>listerWatcher</code>、<code>Process</code> 关联起来，并管理各个模块的功能实现对象获取、入队列、从队列消费的整个流程。其中 <code>deltafifo</code> 和 <code>listerWatcher</code> 会用来构造 <code>反射器</code>，并启动一个 goruntine 来运行 <code>反射器</code>。<code>Process</code> 由 <code>controller</code> 的 processLoop() 函数调用，并通过一个死循环来实现从队里中 <code>Pop</code> 数据，并将数据传给回调函数 Process 来进行处理，<strong>注意：该 controller 和我们所说的k8s调谐控制器不一样</strong>。</li><li><code>process</code>: 由 sharedIndexInformer (<a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/shared_informer.go#L527"> HandleDeltas</a>) 来实现，并由 controller 管理调用，controller 通过一个 processLoop（死循环）来不断的从 fifo 队列中 <code>Pop</code> 数据，并将数据回调给 <code>HandleDeltas</code> 函数进行处理。</li><li><code>indexer</code>:  本地索引存储，基于 <code>index + threadSafeStore</code> 实现的本地存储。deltafifo 的底层也是基于 indexer 实现的，HandleDeltas 函数从队列中得到的数据会保存到本地 indexer 中。</li></ul><h3 id="2-2-sharedIndexInformer-重要方法"><a href="#2-2-sharedIndexInformer-重要方法" class="headerlink" title="2.2. sharedIndexInformer 重要方法"></a>2.2. sharedIndexInformer 重要方法</h3><p>sharedIndexInformer 结构实现了 SharedIndexInformer 接口，其中最为重要的是下面几个方法，其中<code>AddEventHandler</code> 和 <code>HasSynced</code> 函数我们在自定义控制器时会经常使用到：</p><ul><li><code>Run</code> 方法：会初始化 informer 相关组件，并启动 controller 来管理各个组件的正常运行，该方法在 <a href="https://github.com/kubernetes/client-go/blob/master/informers/factory.go#L128">informerFactory.Start</a> 中被调用，当启动 informerFactory 时会调用该方法来启动具体的 informer。</li><li><code>HasSynced</code> 方法：用来判断 informer 同步 cache 中数据是否成功，该函数在使用控制器时使用 informer 从cache 中同步数据是会被调用。</li><li><code>AddEventHandler</code> 方法：资源事件处理函数，在初始化 informer 时需要指定 <strong>资源事件回调(处理)函数</strong> <a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/controller.go">ResourceEventHandler</a>，该函数定义了从 deltafifo 中取出的 key 经 <a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/shared_informer.go#L765">processorListener</a> 函数封装后需要以何种方式添加到自定义控制器的 <code>WorkQueue</code> 队列中。</li><li><code>HandleDeltas</code> 方法：是 <code>Process</code> 回调函数的实际执行函数，<code>Process</code> 从 DeltaFifo 队列中 <code>Pop</code> 出的数据最终会回调交给 HandleDeltas 函数进行处理。HandleDeltas 函数主要做两件事情：一是将 pop 出的数据保存到 indexer 中存储；二是将 pop 出的数据传给 processorListener 处理函数进行分发，最中会调用 <code>AddEventHandler</code> 中定义的函数将 <code>key</code> 添加到控制器的 <code>WorkQueue</code> 队列中。</li></ul><p>下面详细讲解几个重要的方法。</p><h4 id="2-2-1-sharedIndexInformer-Run-方法"><a href="#2-2-1-sharedIndexInformer-Run-方法" class="headerlink" title="2.2.1. sharedIndexInformer.Run 方法"></a>2.2.1. sharedIndexInformer.Run 方法</h4><blockquote><p><a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/shared_informer.go#L368">https://github.com/kubernetes/client-go/blob/master/tools/cache/shared_informer.go#L368</a></p></blockquote><p>上面已讲到 <a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/shared_informer.go#L368">sharedIndexInformer.Run</a> 方法的调用在 informerFactory.Start 时会调用该方法来启动具体的 informer 机制。该方法的真正定义是在 <code>client-go/tools/cache/shared_informer.go</code> 文件下定义的。其具体定义实现如下(只展示部分重要代码逻辑)：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// client-go/tools/cache/shared_informer.go</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(s *sharedIndexInformer)</span> <span class="hljs-title">Run</span><span class="hljs-params">(stopCh &lt;-<span class="hljs-keyword">chan</span> <span class="hljs-keyword">struct</span>&#123;&#125;)</span></span> &#123;<br><span class="hljs-keyword">defer</span> utilruntime.HandleCrash()<br><br>    <span class="hljs-comment">// 1. 初始化 Deltafifo，fifo的底层实现是基于 indexer</span><br>fifo := NewDeltaFIFOWithOptions(DeltaFIFOOptions&#123;<br>KnownObjects:          s.indexer,<br>EmitDeltaTypeReplaced: <span class="hljs-literal">true</span>,<br>&#125;)<br><br>    <span class="hljs-comment">// 2. 通过 Config 结构体将 fifo、listerwatcher、Process 三者传给 controller 进行管理</span><br>cfg := &amp;Config&#123;<br>Queue:            fifo,<br>ListerWatcher:    s.listerWatcher,<br>Process:          s.HandleDeltas,<br>        ......<br>&#125;<br><br>    <span class="hljs-comment">// 3. 通过 config 文件 New 了一个 controller</span><br><span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br>        ....<br>s.controller = New(cfg)<br>&#125;()<br><br>    .......<br>    <span class="hljs-comment">// 4. 通过一个协程来运行 processor 函数，该函数会对从 deltafifo 队列中取出来的数据进行分发到控制器的 workqueue 中</span><br>wg.StartWithChannel(processorStopCh, s.processor.run)<br>    ......<br>    <span class="hljs-comment">// 5. 启动 informer 中的 controller，该 controller 和我们所说的k8s调谐控制器不一样</span><br>s.controller.Run(stopCh)<br>&#125;<br></code></pre></td></tr></table></figure><p>该函数主要做 5 件事情：</p><ol><li>初始化 Deltafifo，fifo的底层实现是基于 indexer。</li><li>通过 Config 结构体将 fifo、listerwatcher、Process 三者传给 controller 进行管理。</li><li>通过 config 文件 New 了一个 controller。</li><li>通过一个协程来运行 processor 函数，该函数会对从 deltafifo 队列中取出来的数据进行分发到控制器的 workqueue 中。</li><li>启动 informer 中的 controller，该 controller 和我们所说的k8s调谐控制器不一样。</li></ol><h4 id="2-2-2-sharedIndexInformer-HandleDeltas-方法"><a href="#2-2-2-sharedIndexInformer-HandleDeltas-方法" class="headerlink" title="2.2.2. sharedIndexInformer.HandleDeltas 方法"></a>2.2.2. sharedIndexInformer.HandleDeltas 方法</h4><blockquote><p><a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/shared_informer.go#L527">https://github.com/kubernetes/client-go/blob/master/tools/cache/shared_informer.go#L527</a></p></blockquote><p>上面已提到 <code>HandleDeltas</code> 方法是 <code>Process</code> 回调函数的实际执行函数，<code>Process</code> 从 DeltaFifo 队列中 <code>Pop</code> 出的数据最终会回调交给 HandleDeltas 函数进行处理。HandleDeltas 函数主要做两件事情：一是将 pop 出的数据保存到 indexer 中存储；二是将 pop 出的数据传给 processorListener 处理函数进行分发，最中会调用 <code>AddEventHandler</code> 中定义的函数将 <code>key</code> 添加到控制器的 <code>WorkQueue</code> 队列中。其具体定义如下所示（只显示重要代码）：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// client-go/tools/cache/shared_informer.go</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(s *sharedIndexInformer)</span> <span class="hljs-title">HandleDeltas</span><span class="hljs-params">(obj <span class="hljs-keyword">interface</span>&#123;&#125;)</span> <span class="hljs-title">error</span></span> &#123;<br>s.blockDeltas.Lock()<br><span class="hljs-keyword">defer</span> s.blockDeltas.Unlock()<br><br><span class="hljs-comment">// from oldest to newest</span><br><span class="hljs-keyword">for</span> _, d := <span class="hljs-keyword">range</span> obj.(Deltas) &#123;<br><span class="hljs-keyword">switch</span> d.Type &#123;<br><span class="hljs-keyword">case</span> Sync, Replaced, Added, Updated:<br>s.cacheMutationDetector.AddObject(d.Object)<br><span class="hljs-keyword">if</span> old, exists, err := s.indexer.Get(d.Object); err == <span class="hljs-literal">nil</span> &amp;&amp; exists &#123;<br><span class="hljs-keyword">if</span> err := s.indexer.Update(d.Object); err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> err<br>&#125;<br><br>isSync := <span class="hljs-literal">false</span><br><span class="hljs-keyword">switch</span> &#123;<br><span class="hljs-keyword">case</span> d.Type == Sync:<br><span class="hljs-comment">// Sync events are only propagated to listeners that requested resync</span><br>isSync = <span class="hljs-literal">true</span><br><span class="hljs-keyword">case</span> d.Type == Replaced:<br><span class="hljs-keyword">if</span> accessor, err := meta.Accessor(d.Object); err == <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">if</span> oldAccessor, err := meta.Accessor(old); err == <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-comment">// Replaced events that didn&#x27;t change resourceVersion are treated as resync events</span><br><span class="hljs-comment">// and only propagated to listeners that requested resync</span><br>isSync = accessor.GetResourceVersion() == oldAccessor.GetResourceVersion()<br>&#125;<br>&#125;<br>&#125;<br>s.processor.distribute(updateNotification&#123;oldObj: old, newObj: d.Object&#125;, isSync)<br>&#125; <span class="hljs-keyword">else</span> &#123;<br><span class="hljs-keyword">if</span> err := s.indexer.Add(d.Object); err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> err<br>&#125;<br>s.processor.distribute(addNotification&#123;newObj: d.Object&#125;, <span class="hljs-literal">false</span>)<br>&#125;<br><span class="hljs-keyword">case</span> Deleted:<br><span class="hljs-keyword">if</span> err := s.indexer.Delete(d.Object); err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> err<br>&#125;<br>s.processor.distribute(deleteNotification&#123;oldObj: d.Object&#125;, <span class="hljs-literal">false</span>)<br>&#125;<br>&#125;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>根据上面 HandleDeltas 函数的具体实现可知，HandleDeltas 函数主要进行两个操作:</p><ol><li>根据事件的类型对事件执行不同的 index 操作，如 Add/Update/Delete 操作。  </li><li>根据事件的类型先对事件进行封装，然后再对封装后的事件执行 <code>distribute</code> 分发操作。</li></ol><h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h2><p>本文主要讲述了 sharedIndexInformer 结构体中 4个非常重要的组件并简单介绍了其功能，分别是 <code>listerWatcher</code>、<code>controller</code>、<code>process</code>、<code>indexer</code> 4个主要成员。其中 <code>controller</code> 又通过 Config 组件来关联 <code>fifo</code>、<code>listerwatcher</code>、<code>process</code>，并通过 controller 来管理各组件的功能，使其相互协调完成 informer 机制的整体功能。同时也简单说明了下 <code>反射器</code> 由 <code>listerwatcher</code> 和 <code>deltafifo</code> 两部分组成，这样使得 <code>反射器</code> 具有了 <code>list-watch</code> 的功能，并能不断往 deltafifo 队列中生产数据。随后简单讲述了 sharedIndexInformer 中几个重要的函数 <code>Run</code>、<code>HasSynced</code>、<code>HandleDeltas</code>、<code>AddEventHandler</code>，并对 <code>Run</code> 和 <code>HandleDeltas</code> 函数进行详细了解。后期我将根据 informer 机制的整体工作流程来详细介绍各个组件的组成和具体功能。</p><ul><li>listerwatcher 机制是怎么实现的？</li><li>reflector 反射器是怎么实现的？其主要功能是什么？</li><li>deltafifo 队列是如何实现的，其底层 store 是什么？deltafifo 与 indexer 的关系？</li><li>controller 是如何工作的？</li><li>process 回调函数如何处理从 deltafifo 中 pop 出的数据？processorListener 是什么？它是如何工作的？</li><li>workqueue 是如何工作的？</li></ul><h2 id="4-参考"><a href="#4-参考" class="headerlink" title="4. 参考"></a>4. 参考</h2><ul><li><a href="https://github.com/kubernetes/client-go">https://github.com/kubernetes/client-go</a></li></ul><h2 id="5-推荐文章"><a href="#5-推荐文章" class="headerlink" title="5. 推荐文章"></a>5. 推荐文章</h2><ul><li><a href="https://qingwei8.github.io/2021/09/01/client-go-informer-v1-22-01/#client-go-informer-%E6%9C%BA%E5%88%B6%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89">client-go-informer-机制源码剖析（一）</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>kubernetes</category>
      
      <category>client-go</category>
      
    </categories>
    
    
    <tags>
      
      <tag>informer</tag>
      
      <tag>client-go</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>client-go Informer 机制源码剖析（一）</title>
    <link href="/2021/09/01/kubernetes-client-go-informer-informer-v1-22-01/"/>
    <url>/2021/09/01/kubernetes-client-go-informer-informer-v1-22-01/</url>
    
    <content type="html"><![CDATA[<h1 id="client-go-informer-机制源码剖析（一）"><a href="#client-go-informer-机制源码剖析（一）" class="headerlink" title="client-go informer 机制源码剖析（一）"></a>client-go informer 机制源码剖析（一）</h1><h2 id="之-Informer-机制简介"><a href="#之-Informer-机制简介" class="headerlink" title="之 Informer 机制简介"></a>之 Informer 机制简介</h2><blockquote><p>本文档基于 <code>v1.22</code> 版本</p></blockquote><h2 id="1-什么是-informer"><a href="#1-什么是-informer" class="headerlink" title="1. 什么是 informer"></a>1. 什么是 informer</h2><p>informer 是 client-go 中重要的组件，在 kubernetes 的各个控制器中以及自定义 CRD 的控制器中有着广泛的应用，其主要用途是通过 <code>ListWatch</code> 机制从 K8S APIService 中获取资源信息，并将资源信息实时的同步到本地 cache 中，以减少 k8s 集群对 APIService（及 etcd） 的访问压力。k8s 控制器和 CRD 自定义控制器通过 informer 从本地 cache 中获取资源信息，实现对资源的调谐最终达到用户的期望状态。可以说 informer 机制在 k8s 中以及 crd 控制器中起着举足轻重的地位。</p><h2 id="2-如何使用-informer"><a href="#2-如何使用-informer" class="headerlink" title="2. 如何使用 informer"></a>2. 如何使用 informer</h2><p>在了解了 informer 的基本概念及在 k8s 控制器中的地位后，我们要如何来使用 informer 机制呢？</p><ul><li><p>在 k8s 的控制器中 informer 的使用方式可以参考 <a href="https://github.com/kubernetes/kubernetes/blob/master/cmd/kube-controller-manager/app/controllermanager.go#L221">cmd/kube-controller-manager/app/controllermanager.go</a>。先通过在 <code>CreateControllerContext</code> 中初始化 <code>InformerFactory</code>，再将具体的 informer 注册到 <code>InformerFactory</code> 中 （如 DeploymentInformer），再通过 <code>controllerContext.InformerFactory.Start(controllerContext.Stop)</code> 启动所有已注册的 informer。</p></li><li><p>如在外面编写自定义的 CRD 控制器时，需要获取 k8s 资源（如 pod）或者自定义的资源时，可以通过如下方式启动 informer。<a href="https://github.com/qingwei8/k8s-study-conding/blob/master/controller-sample/mm.go">代码参考例子</a></p>  <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs awk">   ......<br><br>   <span class="hljs-regexp">//</span> 创建 informerFactory<br>   informerFactory := informers.NewSharedInformerFactory(clientset, <span class="hljs-number">30</span>*time.Second)<br>   <br>   <span class="hljs-regexp">//</span> 初始化具体的 informer，将具体的 informer 注册到 informerFactory 中<br>   <span class="hljs-regexp">//</span> factory已经为所有k8s的内置资源对象提供了创建对应informer实例的方法，调用具体informer实例的Lister或Informer方法<br>   <span class="hljs-regexp">//</span> 就完成了将informer注册到factory的过程<br>   podInformer := informerFactory.Core().V1().Pods()<br>   pInformer := podInformer.Informer()  <span class="hljs-regexp">//</span> 会创建 NewSharedIndexInformer<br>pLister := podInformer.Lister()  <span class="hljs-regexp">//</span> 会提供 list 和 get 方法，供外部从 cache 中获取数据<br><br>   <span class="hljs-regexp">//</span> 最终需要使用 informerFactory 来启动所有已经注册的 informer<br>   informerFactory.Start(stopCh)<br><br>   <span class="hljs-regexp">//</span> informerFactory 会调用所有已注册的 informer 的 WaitForCacheSync 方法进行同步<br>   <span class="hljs-regexp">//</span> 先启动，再同步<br>   informerFactory.WaitForCacheSync(stopCh)<br>   ......<br></code></pre></td></tr></table></figure></li></ul><p>综上所述，要使用 informer 机制来实现资源的实时获取，需要通过下面三步来实现：</p><ul><li><ol><li>初始化 informerFactory。</li></ol></li><li><ol start="2"><li>将所需要监听的资源所对应的 Informer 注册到 informerFactory 中去。</li></ol></li><li><ol start="3"><li>通过启动 informerFactory 来启动所有已经注册的 Informer。</li></ol></li></ul><h2 id="3-informer-机制详解"><a href="#3-informer-机制详解" class="headerlink" title="3. informer 机制详解"></a>3. informer 机制详解</h2><h3 id="3-1-SharedInformerFactory-接口"><a href="#3-1-SharedInformerFactory-接口" class="headerlink" title="3.1. SharedInformerFactory 接口"></a>3.1. SharedInformerFactory 接口</h3><blockquote><p><a href="https://github.com/kubernetes/client-go/blob/master/informers/factory.go">https://github.com/kubernetes/client-go/blob/master/informers/factory.go</a></p></blockquote><p>根据上面 informer 的使用方式，可知道 informer 的入口函数为 <code>NewSharedInformerFactory</code>，首先我们来看看这个函数的具体定义。</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-comment">// 代码位于： client-go/informers/factory.go</span><br><span class="hljs-comment">// NewSharedInformerFactory constructs a new instance of sharedInformerFactory for all namespaces.</span><br>func <span class="hljs-constructor">NewSharedInformerFactory(<span class="hljs-params">client</span> <span class="hljs-params">kubernetes</span>.Interface, <span class="hljs-params">defaultResync</span> <span class="hljs-params">time</span>.Duration)</span> SharedInformerFactory &#123;<br>return <span class="hljs-constructor">NewSharedInformerFactoryWithOptions(<span class="hljs-params">client</span>, <span class="hljs-params">defaultResync</span>)</span><br>&#125;<br></code></pre></td></tr></table></figure><p>该函数会返回一个 <a href="https://github.com/kubernetes/client-go/blob/master/informers/factory.go#L187">SharedInformerFactory</a> 的 <code>接口</code>。该接口封装了 k8s 内部所有资源的 informer 接口。实现该接口的具体实例为位于同一文件下 <code>sharedInformerFactory</code> 结构体，其中有两个非常关键的方法：<code>Start</code> 和 <code>WaitForCacheSync</code> 函数。</p><ul><li><code>Start</code> 方法：会通过 goroutine 的方式来启动已注册到 informerFactory 中的 informer。每个 informer 都会启动一个 goroutine 来运行 informer 机制，已启动的 informer 不会再启动。</li><li><code>WaitForCacheSync</code> 方法: 会对已注册到 informerFactory 中的且 start 为正常运行状态的 informer 进行同步数据到本地 Cache 中。并设定 informer 的状态为是否已同步完成。</li></ul><p>接下来，再来看看实现 SharedInformerFactory 接口的<strong>结构体实例</strong> <code>sharedInformerFactory</code>。该结构体的定义如下：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">//</span> 代码位于：client-go<span class="hljs-regexp">/informers/</span>factory.go<br>type sharedInformerFactory struct &#123;<br>    ......<br>informers map[reflect.Type]cache.SharedIndexInformer<br><span class="hljs-regexp">//</span> startedInformers is used <span class="hljs-keyword">for</span> tracking which informers have been started.<br><span class="hljs-regexp">//</span> This allows Start() to be called multiple times safely.<br>startedInformers map[reflect.Type]bool<br>&#125;<br></code></pre></td></tr></table></figure><p>该结构体中最关键的字段为<code>最后两个字段</code>，由定义可知及上述使用时可知，往 informerFactory 中注册 informer 时其实是将具体的 informer 通过 key-value 的方式写入 informers map 字典中。上述 <code>Start</code> 方法即是将 map 中的 informers 逐一启动。而 <code>WaitForCacheSync</code> 方法是将 <code>startedInformers</code> map 字典中值为 true 的 informers 进行 cache 同步，也即只有 start 正常的 informer 才能进行 cache 同步，且可同步多次。</p><h3 id="3-2-注册-informer"><a href="#3-2-注册-informer" class="headerlink" title="3.2. 注册 informer"></a>3.2. 注册 informer</h3><blockquote><p><a href="https://github.com/kubernetes/client-go/tree/master/informers">https://github.com/kubernetes/client-go/tree/master/informers</a></p></blockquote><p>在定义完 informerFactory 后需要往 factory 中注册具体的 informer（如 <code>PodInformer</code>），通过上一节 <code>sharedInformerFactory</code> 结构体可知，往 factory 中注册 informer 其实就是往 informers map 中添加数据，而 <a href="https://github.com/kubernetes/client-go/tree/master/informers">informers</a> 中定义了 k8s 内部所有资源的具体定义，所有资源按照 group + version 的方式组织文件结构。</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-comment">// client-go/informers</span><br>$ tree -L 2<br><span class="hljs-bullet">.</span><br><span class="hljs-bullet"></span><span class="hljs-code">......</span><br><span class="hljs-code">├── apps</span><br><span class="hljs-code">│   ├── interface.go</span><br><span class="hljs-code">│   ├── v1</span><br><span class="hljs-code">│   ├── v1beta1</span><br><span class="hljs-code">│   └── v1beta2</span><br><span class="hljs-code">├── autoscaling</span><br><span class="hljs-code">│   ├── interface.go</span><br><span class="hljs-code">│   ├── v1</span><br><span class="hljs-code">│   ├── v2beta1</span><br><span class="hljs-code">│   └── v2beta2</span><br><span class="hljs-code">├── batch</span><br><span class="hljs-code">│   ├── interface.go</span><br><span class="hljs-code">│   ├── v1</span><br><span class="hljs-code">│   └── v1beta1</span><br><span class="hljs-code">......</span><br></code></pre></td></tr></table></figure><p>如 <a href="https://github.com/kubernetes/client-go/blob/master/informers/core/v1/pod.go#L37">PodInformer</a> 定义在 <code>client-go/informers/core/v1/pod.go</code> 中，其接口定义如下：</p><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs elm">// client-go/informers/core/v1/pod.go<br><span class="hljs-keyword">type</span> <span class="hljs-type">PodInformer</span> interface &#123;<br><span class="hljs-type">Informer</span>() cache.<span class="hljs-type">SharedIndexInformer</span><br><span class="hljs-type">Lister</span>() v1.<span class="hljs-type">PodLister</span><br>&#125;<br></code></pre></td></tr></table></figure><p><code>PodInformer</code> 接口中定义了两个方法，这两个方法在自定义控制器时会经常用到，是 informer 机制的实现者和使用者。</p><ul><li><code>Informer</code> 方法：该方法法类型为 <code>cache.SharedIndexInformer</code>，是实现 informer 机制的主体，informer 所有相关的逻辑实现都在这里实现，后面会详细介绍该接口的具体实现。</li><li><code>Lister</code> 方法：该方法为用户提供从 cache 中访问数据的方法，<code>Informer</code> 通过 <code>ListWath</code> 会将数据同步到 cache 中，<code>Lister</code> 从 cache 中 get/list 数据。</li></ul><h3 id="3-3-SharedInformer-SharedIndexInformer-接口"><a href="#3-3-SharedInformer-SharedIndexInformer-接口" class="headerlink" title="3.3. SharedInformer/SharedIndexInformer 接口"></a>3.3. SharedInformer/SharedIndexInformer 接口</h3><blockquote><p><a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/shared_informer.go">https://github.com/kubernetes/client-go/blob/master/tools/cache/shared_informer.go</a></p></blockquote><p>在定义完 informerFactory 后需要往 factory 中注册具体的 informer（如 <code>PodInformer</code>），通过上一节 <code>sharedInformerFactory</code> 结构体可知，往 factory 中注册 informer 其实就是往 informers map 中添加数据，其类型为 <code>cache.SharedIndexInformer</code> 接口。<a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/shared_informer.go#L186">SharedIndexInformer</a> 接口封装了 <a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/shared_informer.go#L133">SharedInformer</a> 接口，在其基础上添加了 index 索引的相关功能，使查找/增加数据更加快速方便。<br><strong>注意：</strong> <code>SharedInformer</code> 接口中有两个方法非常重要，我们在写 <a href="https://github.com/qingwei8/k8s-study-conding/blob/master/controller-sample/controller/pod_controller.go">控制器</a> 的时候会经常用到。</p><ul><li><code>AddEventHandler</code> 方法：自定义CRD的控制器从 informer 中获取的数据会通过该函数将资源对象的 key 添加到控制器的工作队列 Workqueue 中，后续控制器会从 Workqueue 中获取 key 对对象进行调谐处理。</li><li><code>HasSynced</code> 方法：定义控制器时，每个具体的 informer start 后，需要对数据进行同步 cache 操作，该方法会以参数的形式传入到每个 informer 的 <code>WaitForCacheSync</code> 中，用来跟踪确保每个 informer 是否完成同步操作。</li></ul><p><code>SharedIndexInformer</code> 接口的具体实现由 <code>sharedIndexInformer</code> 结构体来实现。其结构体定义以及方法的实现都相对比较复杂，我将留在下一节进行详细剖析，简单说就是 <code>sharedIndexInformer</code> 结构体中包含了 <code>listWatch</code>、<code>反射器</code>、<code>deltafifo</code>、<code>controller</code>、<code>indexer</code> 等逻辑的处理。</p><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h2><p>本文主要介绍了 informer 机制的简单使用，从外部使用者的角度来介绍涉及到的组件，用户只需要通过如下三步即能完美使用 informer 机制，从 cache 中获取 k8s 中的所有资源信息。</p><ul><li><ol><li>初始化 informerFactory。</li></ol></li><li><ol start="2"><li>将所需要监听的资源所对应的 Informer 注册到 informerFactory 中去。</li></ol></li><li><ol start="3"><li>通过启动 informerFactory 来启动所有已经注册的 Informer。</li></ol></li></ul><h2 id="5-参考"><a href="#5-参考" class="headerlink" title="5. 参考"></a>5. 参考</h2><ul><li><a href="https://github.com/kubernetes/client-go/tree/master/informers">https://github.com/kubernetes/client-go/tree/master/informers</a></li><li><a href="https://github.com/kubernetes/client-go/tree/master/tools/cache">https://github.com/kubernetes/client-go/tree/master/tools/cache</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>kubernetes</category>
      
      <category>client-go</category>
      
    </categories>
    
    
    <tags>
      
      <tag>informer</tag>
      
      <tag>client-go</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>vim multiple gopls</title>
    <link href="/2021/08/12/vim-2021-08-12-vim-multiple-gopls/"/>
    <url>/2021/08/12/vim-2021-08-12-vim-multiple-gopls/</url>
    
    <content type="html"><![CDATA[<h1 id="多个-gopls-daemon-导致内存耗尽"><a href="#多个-gopls-daemon-导致内存耗尽" class="headerlink" title="多个 gopls daemon 导致内存耗尽"></a>多个 gopls daemon 导致内存耗尽</h1><h2 id="问题及现象"><a href="#问题及现象" class="headerlink" title="问题及现象"></a>问题及现象</h2><p>在 vim 中打开大项目（比如 kubernetes）时会导致系统内存被耗尽，最终导致整台电脑死机。其根本的原因是 vim 中配置的各种插件（比如 vim-go、coc.nvim、coc-go、ycm等）都会各自启动一个 gopls serve daemon 进程，如下图所示：</p><p><img src="/2021/08/12/vim-2021-08-12-vim-multiple-gopls/gopls/gopls01.png" alt="g01"></p><p>每个 gopls serve daemon 进程都会消耗 2-4G 的内存，甚至更多（对于更大的项目），如下图：</p><p><img src="/2021/08/12/vim-2021-08-12-vim-multiple-gopls/gopls/gopls02.png" alt="g02"></p><p>对上述现象 go 团队及社区给出的解释可<a href="https://github.com/ericwq/golangIDE/blob/master/shared-gopls-daemon.md">参考</a>。go 团队启动 gopls serve daemon 时会基于lsp 空间，不同 lsp 空间彼此独立以方便启动多个 daemon 来满足特殊场景的测试/开发等，而 lsp 空间最终会根据 TMPDIR 的目录来具体实现，因此可理解为如果两个 gopls 指定 TMPDIR 目录不相同时将会启动两个不同的 gopls serve daemon 进程。而对于 vim-go、coc.nvim/coc-go 等插件其内部获取 TMPDIR 的方式不一样，得到的 TMPDIR 也不一样（可<a href="https://github.com/ericwq/golangIDE/blob/master/shared-gopls-daemon.md">参考</a>），因此会启动多个 gopls serve daemon 进程，最终导致内存耗尽。</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>为了避免 vim 中多个插件开启多个 gopls serve daemon 进程，可通过使多个插件共享同一个 gopls serve daemon 的方式来减少内存消耗。具体有如下方案：</p><ul><li>方案一：向 go 团队提交 issue，说明原因后由 go 团队最终解决。社区已有人向 go 团队<a href="https://groups.google.com/g/golang-tools/c/y3OQNIudLzQ/m/7JYRgEZSAgAJ">反馈</a>。</li><li>方案二：可通过修改 coc.nvim，coc-go 等插件获取 TMPDIR 的结果来指定多个插件共享同一个 gopls，但这种方式需要修改插件的代码源码。可<a href="https://github.com/ericwq/golangIDE/blob/master/shared-gopls-daemon.md">参考</a></li><li>方案三：修改 coc.nvim，coc-go 的配置文件，这种方式比较方便处理。可<a href="https://github.com/golang/go/issues/41998">参考</a></li></ul><p>修改 coc.nvim 的配置:</p><ul><li><p>官方原始配置，通过在 <code>coc-setting.json</code> 指定 <code>language server</code>：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs json">&#123;<br>    <span class="hljs-attr">&quot;languageserver&quot;</span>: &#123;<br>        <span class="hljs-attr">&quot;golang&quot;</span>: &#123;<br>            <span class="hljs-attr">&quot;command&quot;</span>: <span class="hljs-string">&quot;gopls&quot;</span>,<br>            <span class="hljs-attr">&quot;args&quot;</span>: [<span class="hljs-string">&quot;-remote=auto&quot;</span>],<br>            <span class="hljs-attr">&quot;rootPatterns&quot;</span>: [<span class="hljs-string">&quot;go.mod&quot;</span>, <span class="hljs-string">&quot;.vim/&quot;</span>, <span class="hljs-string">&quot;.git/&quot;</span>, <span class="hljs-string">&quot;.hg/&quot;</span>],<br>            <span class="hljs-attr">&quot;filetypes&quot;</span>: [<span class="hljs-string">&quot;go&quot;</span>]<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>这个配置文件会有两个问题：</p><ul><li><p>会导致多个插件有多个 gopls sever daemon 进程，即使加上 <code>-remote=auto</code> 多个插件并不会共享同一个 gopls 进程，因为插件使用的 TMPDIR 目录不一样。</p></li><li><p>在开发过程中，使用代码自动补全时会出现多余的重复选项(如图03)，这个问题我是后来才发现的，当不添加 <code>languageserver</code> 的配置时不会有重复的选项（如图04）。</p><p><img src="/2021/08/12/vim-2021-08-12-vim-multiple-gopls/gopls/gopls03.png" alt="gp03"> <img src="/2021/08/12/vim-2021-08-12-vim-multiple-gopls/gopls/gopls04.png" alt="gp04"></p></li></ul></li><li><p>我的 coc.nvim/coc-go 的最终配置为 <code>cos-setting.json</code>:</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs awk">&#123;<br>    <span class="hljs-string">&quot;go.goplsArgs&quot;</span>: [<span class="hljs-string">&quot;-remote=auto&quot;</span>, <span class="hljs-string">&quot;-logfile&quot;</span>, <span class="hljs-string">&quot;/tmp/gopls-cocvim.log&quot;</span>], <span class="hljs-regexp">//</span> 指定 remote=auto, coc.nvim 的 TMPDIR 与 gopls 的在同一个文件夹下 <br>    <span class="hljs-string">&quot;go.goplsPath&quot;</span>: <span class="hljs-string">&quot;/home/qingwei/go/bin/gopls&quot;</span>  <span class="hljs-regexp">//</span> 指定共享的 gopls 二进制文件，注意：如果只修改 TMPDIR 也不行，必须使用同一个 gopls 二进制文件启动才能共享同一个 serve daemon。<br>    <span class="hljs-string">&quot;go.goplsOptions&quot;</span>: &#123;<br>        <span class="hljs-regexp">//</span> go 多个module 嵌套问题:<br>        <span class="hljs-regexp">//</span> https:<span class="hljs-regexp">//gi</span>thub.com<span class="hljs-regexp">/golang/</span>tools<span class="hljs-regexp">/blob/m</span>aster<span class="hljs-regexp">/gopls/</span>doc/workspace.md<br>        <span class="hljs-regexp">//</span> https:<span class="hljs-regexp">//gi</span>thub.com<span class="hljs-regexp">/josa42/</span>coc-go<br>        <span class="hljs-string">&quot;experimentalWorkspaceModule&quot;</span>: true<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>最终效果如下：<br><img src="/2021/08/12/vim-2021-08-12-vim-multiple-gopls/gopls/gopls05.png" alt="gp05"><br><img src="/2021/08/12/vim-2021-08-12-vim-multiple-gopls/gopls/gopls06.png" alt="gp06"></p></li></ul><h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><ul><li><a href="https://github.com/golang/go/issues/41998">https://github.com/golang/go/issues/41998</a></li><li><a href="https://github.com/ericwq/golangIDE/blob/master/shared-gopls-daemon.md">https://github.com/ericwq/golangIDE/blob/master/shared-gopls-daemon.md</a></li><li><a href="https://github.com/golang/go/issues/41266">https://github.com/golang/go/issues/41266</a></li><li><a href="https://groups.google.com/g/golang-tools/c/y3OQNIudLzQ/m/7JYRgEZSAgAJ">https://groups.google.com/g/golang-tools/c/y3OQNIudLzQ/m/7JYRgEZSAgAJ</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>vim</category>
      
    </categories>
    
    
    <tags>
      
      <tag>vim</tag>
      
      <tag>vim-plugin</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Descheduler</title>
    <link href="/2021/08/09/descheduler-2021-08-09-descheduler/"/>
    <url>/2021/08/09/descheduler-2021-08-09-descheduler/</url>
    
    <content type="html"><![CDATA[<h1 id="Descheduler-学习记录"><a href="#Descheduler-学习记录" class="headerlink" title="Descheduler 学习记录"></a>Descheduler 学习记录</h1><p><a href="https://github.com/kubernetes-sigs/descheduler/tree/master">基于 descheduler v0.21.0</a></p><h2 id="1-资源平衡策略"><a href="#1-资源平衡策略" class="headerlink" title="1. 资源平衡策略"></a>1. 资源平衡策略</h2><p>基于当前版本 <code>v0.21.0</code>，Descheduler 已实现了 9种策略用于 kubernetes 集群的资源再平衡。</p><ul><li>RemoveDuplicates</li><li>LowNodeUtilization</li><li>HighNodeUtilization</li><li>RemovePodsViolatingInterPodAntiAffinity</li><li>RemovePodsViolatingNodeAffinity</li><li>RemovePodsViolatingNodeTaints</li><li>RemovePodsViolatingTopologySpreadConstraint</li><li>RemovePodsHavingTooManyRestarts</li><li>PodLifeTime</li></ul><p>这些策略包含一些共同的配置，分别如下：</p><ul><li><strong>nodeSelector</strong>：节点标签选择器，用来限制 descheduler 可在带有该标签的节点下进行处理，即限制 descheduler 驱逐在哪些节点上的 pod。</li><li><strong>evictLocalStoragePods</strong>：当值为 <code>true</code> 时，表示允许驱逐挂有 local storage 卷的 pods。默认为 <code>false</code>。</li><li><strong>evictSystemCriticalPods</strong>：当值为 <code>true</code> 时，会驱逐 kubernetes 中 kube-system 命名空间下以及任意优先级(priority)的系统关键 pod，这个操作很危险，不建议开启。默认为 <code>false</code>。</li><li><strong>ignorePvcPods</strong>：值为 <code>true</code> 不会（即忽略）驱逐挂载 pvc 卷的 pods。默认值为 <code>false</code>，即默认会驱逐挂载 pvc 的 pods。</li><li><strong>maxNoOfPodsToEvictPerNode</strong>：每个 node 可驱逐 pods 数量的最大值。pods 数量为各个策略统计的 pods 数量的总和。</li></ul><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">apiVersion:</span> <span class="hljs-string">&quot;descheduler/v1alpha1&quot;</span><br><span class="hljs-symbol">kind:</span> <span class="hljs-string">&quot;DeschedulerPolicy&quot;</span><br><span class="hljs-symbol">nodeSelector:</span> prod=dev<br><span class="hljs-symbol">evictLocalStoragePods:</span> true<br><span class="hljs-symbol">evictSystemCriticalPods:</span> true<br><span class="hljs-symbol">maxNoOfPodsToEvictPerNode:</span> <span class="hljs-number">40</span>  <span class="hljs-comment">// 设置可驱逐的 pod 数量的最大值，保证一次不能驱逐太多，导致集群不稳定</span><br><span class="hljs-symbol">ignorePvcPods:</span> false<br><span class="hljs-symbol">strategies:</span><br>  ...<br><br></code></pre></td></tr></table></figure><h3 id="1-1-RemoveDuplicates"><a href="#1-1-RemoveDuplicates" class="headerlink" title="1.1. RemoveDuplicates"></a>1.1. RemoveDuplicates</h3><p>该策略可以确保在同一个 node 节点上只会运行<strong>一个</strong>相关联的 <strong>pod</strong>（这些 pods被同属一个相关的控制器（如 RS、RC、StatefulSet or Job）资源所管理，以下以 RS 为例），即同一个 RS 下的所有 pod 在集群中的所有节点中最多只有一个 pod 会运行在同一个 node 节点上。如果某个 node 节点上出现了同一个 RS 的多个 pod，则会将多余的 pod 驱逐掉，仅保留一个 pod 在该 node 节点上运行。该策略的应用场景为，当集群中的某些 node 因为不可知的原因导致硬件故障等下线后，该 node 节点上的 pod 会被调度到其他节点上（这时其他节点上可能会存在多个同属于一个 RS 的 pod 位于同一个 node 节点上），等待 node 重新上线后，该策略会驱逐其他节点中多余的 pod 并将驱逐的 pod 重新调度到 node 节点上。</p><p>参数：</p><ul><li><p>可选参数：<code>excludeOwerKinds</code> 列表类型，被参数中的所有 OwerRef 相关的 Kinds 所管理的 pods 不会被驱逐。但需<strong>注意：该策略会驱逐被 Deployment 所创建的 pod。如果需要 Deployment 所创建的 pod 不被驱逐，需要在参数中指定 ReplicaSet，而不是 Deployment。</strong></p><table><thead><tr><th align="center">Name</th><th align="center">Type</th></tr></thead><tbody><tr><td align="center">excludeOwerKinds</td><td align="center">list(string)</td></tr></tbody></table><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">apiVersion:</span> <span class="hljs-string">&quot;descheduler/v1alpha1&quot;</span><br><span class="hljs-symbol">kind:</span> <span class="hljs-string">&quot;DeschedulerPolicy&quot;</span><br><span class="hljs-symbol">strategies:</span><br>  <span class="hljs-string">&quot;RemoveDuplicates&quot;</span>:<br><span class="hljs-symbol">    enabled:</span> true<br><span class="hljs-symbol">    params:</span><br><span class="hljs-symbol">      removeDuplicates:</span><br><span class="hljs-symbol">        excludeOwnerKinds:</span><br>        - <span class="hljs-string">&quot;ReplicaSet&quot;</span><br></code></pre></td></tr></table></figure></li><li><p>过滤型参数</p><table><thead><tr><th align="center">Name</th><th align="center">Type</th><th>说明</th></tr></thead><tbody><tr><td align="center">namespaces</td><td align="center">list</td><td>支持 include 和 exclude 两种过滤策略</td></tr><tr><td align="center">thresholdPriority</td><td align="center">int</td><td>直接指定驱逐优先级</td></tr><tr><td align="center">thresholdPriorityClassName</td><td align="center">string</td><td>通过 k8s 的 priorityClass 来指点，如果 k8s 没有创建 priorityClass 会报错</td></tr><tr><td align="center">nodeFit</td><td align="center">bool</td><td>true 为开启，开启时会优化驱逐调度，即会考虑 pod 是否满足驱逐条件，驱逐后是否有 node 可适合运行，如果没有则不会驱逐</td></tr></tbody></table></li></ul><h3 id="1-2-LowNodeUtilization"><a href="#1-2-LowNodeUtilization" class="headerlink" title="1.2. LowNodeUtilization"></a>1.2. LowNodeUtilization</h3><p>该策略会查找整个集群中 <code>未充分利用的节点</code>，并将其他 <code>高利用的节点</code> 中的一些 pod 驱逐并最终在这些 <code>未充分利用的节点</code> 上重建 pod。该策略的参数在 <code>nodeResourceUtilizationThresholds</code> 下配置。其中，还有另外两个非常关键的参数 <code>thresholds</code> 和 <code>targetThresholds</code>。</p><ul><li><code>thresholds</code>: 指定资源阀值（cpu/mem/pod 数量/gpu/其他可计算的资源），用来确定哪些节点是 <strong>未充分利用的节点</strong>。如果节点中的<strong>所有</strong>相关资源利用率<strong>都低于</strong> <code>thresholds</code> 中所指定资源的阀值，则可认为该节点为 <code>未充分利用的节点</code>。其中 pod 中 cpu/mem 等资源的值通过 k8s 中的 request 字段的值来进行计算。</li><li><code>targetThresholds</code>：用来确定哪些节点是<strong>高利用率的节点</strong>，是可被驱逐的。当节点中的<strong>任一</strong>相关资源利用率<strong>高于</strong> <code>targetThresholds</code> 中指定的阀值时，则可认为该节点是 <code>高利用率的节点</code>，节点上的 pod 可以被驱逐。</li></ul><p>当节点的所有资源利用率位于 <code>thresholds</code> 和 <code>targetThresholds</code> 时，则可认为该节点的利用率是合理的，节点上的 pod 将不会被驱逐。该策略的驱逐方向是从 <code>高利用率的的节点</code> 中驱逐 pod，并最终在 <code>未充分利用的节点</code> 上重建出 pod；且当 <code>高利用率的节点</code> 或者 <code>未充分利用的节点</code> 的节点数中有一个为 0 时将不再执行驱逐策略驱逐 pod。</p><p>参数：</p><ul><li>普通参数：<table><thead><tr><th align="center">Name</th><th align="center">Type</th><th align="left">说明</th></tr></thead><tbody><tr><td align="center">thresholds</td><td align="center">map[string]int</td><td align="left">设定阀值，确定 <code>未充分利用的节点</code> 的界限</td></tr><tr><td align="center">targetThresholds</td><td align="center">map[srting]int</td><td align="left">设定阀值，确定 <code>高利用率的节点</code> 的界限</td></tr><tr><td align="center">numberOfNodes</td><td align="center">int</td><td align="left">用来在大集群中来决定是否开启 <code>LowNodeUtilization</code> 策略的阀值，如果 numberOfNodes 不为 0 时，则当集群中 <code>未充分利用的节点</code> 大于 numberOfNodes 的值时会开启该策略；反之，不开启该策略。默认 numberOfNodes 为 0</td></tr></tbody></table></li><li>过滤型参数：<table><thead><tr><th align="center">Name</th><th align="center">Type</th><th align="left">说明</th></tr></thead><tbody><tr><td align="center">thresholdPriority</td><td align="center">int</td><td align="left">同上</td></tr><tr><td align="center">thresholdPriorityClassName</td><td align="center">string</td><td align="left">同上</td></tr><tr><td align="center">nodeFit</td><td align="center">bool</td><td align="left">同上</td></tr></tbody></table></li></ul><p>说明：</p><ul><li><code>Descheduler</code> 内部原生支持三种资源类型：cpu、memory 以及 pods 数量。如果其中有资源类型的值没有指定，则默认设置为 100%，目的是避免将 <code>未充分利用的节点</code> 误统计为 <code>高利用的节点</code>。</li><li><code>Descheduler</code> 也支持可选择的扩展资源，比如支持 GPU 数量类型的资源 <code>nvidia.com/gpu</code>。如果指定了扩展资源，则 node 的节点的总资源利用率会将扩展资源算进去，如果没有指定扩展资源，则不会将该资源统计进去。</li><li>阀值选项 <code>thresholds</code> 和 <code>targetThresholds</code> 中的值不能为空，且对同一种资源指定要么全指定值要么全不指定值。</li><li>阀值选项 <code>thresholds</code> 和 <code>targetThresholds</code> 中对同一种资源中的值中，<code>thresholds</code> 的值必须小于或者等于（不能大于）<code>targetThresholds</code> 的对应类型资源的值。</li><li>阀值选项 <code>thresholds</code> 和 <code>targetThresholds</code> 中的资源类型的值必须为百分比值，且值必须位于 [0,100] 中。</li><li>numberOfNodes：参数用来触发是否开启该策略，如果 <code>未充分利用的节点数</code> 大于 numberOfNodes 时，将激活该策略，启动驱逐功能。否则，不启动该策略，numberOfNodes 的默认值为 0。</li><li>该策略是将 pod 从 <code>高利用率的节点</code> 上往 <code>低利用率的节点</code> 调，会让整个集群的整体资源平衡利用。</li></ul><p>例子：</p>  <figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">apiVersion:</span> <span class="hljs-string">&quot;descheduler/v1alpha1&quot;</span><br><span class="hljs-symbol">kind:</span> <span class="hljs-string">&quot;DeschedulerPolicy&quot;</span><br><span class="hljs-symbol">strategies:</span><br>  <span class="hljs-string">&quot;LowNodeUtilization&quot;</span>:<br><span class="hljs-symbol">    enabled:</span> true<br><span class="hljs-symbol">    params:</span><br><span class="hljs-symbol">      nodeResourceUtilizationThresholds:</span><br><span class="hljs-symbol">        thresholds:</span><br>          <span class="hljs-string">&quot;cpu&quot;</span> : <span class="hljs-number">20</span><br>          <span class="hljs-string">&quot;memory&quot;</span>: <span class="hljs-number">20</span><br>          <span class="hljs-string">&quot;pods&quot;</span>: <span class="hljs-number">20</span><br><span class="hljs-symbol">        targetThresholds:</span><br>          <span class="hljs-string">&quot;cpu&quot;</span> : <span class="hljs-number">50</span><br>          <span class="hljs-string">&quot;memory&quot;</span>: <span class="hljs-number">50</span><br>          <span class="hljs-string">&quot;pods&quot;</span>: <span class="hljs-number">50</span><br><br></code></pre></td></tr></table></figure><h3 id="1-3-HighNodeUtilization"><a href="#1-3-HighNodeUtilization" class="headerlink" title="1.3. HighNodeUtilization"></a>1.3. HighNodeUtilization</h3><p>该策略会将 <code>低利用率节点</code> 上的 pod 驱逐到 <code>高利用率节点</code> 上。该策略必须和 k8s 默认调度器策略中的优选策略 <code>MostRequestedPriority</code> 配合一起使用，在进行驱逐重调度时会给<code>高利用率的节点打上高分</code>。该策略的参数都配置在 <code>nodeResourceUtilizationThresholds</code> 下。</p><p>该策略也通过参数 <code>thresholds</code> 的阀值来确定哪些节点是 <code>低利用率的节点</code>，它所支持的资源类型包括 cpu、memory、pods 数量以及扩展资源类型等，当节点相关资源的实际申请使用率（以 k8s 中的 request 为准）<strong>都低于</strong> 参数阀值 <code>thresholds</code> 中 <strong>所有</strong> 相关资源所设定的值时，该节点会给认为是 <code>低利用率的节点</code>（未充分利用的节点）。节点中只要<strong>任一一个</strong>资源的使用率大于阀值 <code>thresholds</code> 中的同类型资源的值时，该节点被认为合理利用的节点，该节点上的 pod 将不会被驱逐。</p><p>注意，该策略是将 pod 的从 <code>低利用率的节点</code> 上驱逐并在合适的 <code>高利用率的节点</code> 上重建。当 <code>高利用率的节点</code> 或者 <code>未充分利用的节点</code> 的节点数中有一个为 0 时将不再执行驱逐策略驱逐 pod。</p><p>参数：</p><ul><li><p>普通参数：</p><table><thead><tr><th align="center">Name</th><th align="center">Type</th><th align="left">说明</th></tr></thead><tbody><tr><td align="center">thresholds</td><td align="center">map[string]int</td><td align="left">同上，设定阀值，确定 <code>未充分利用节点</code> 的界限</td></tr><tr><td align="center">numberOfNodes</td><td align="center">int</td><td align="left">同上，在大集群中确定开启该策略的阀值</td></tr></tbody></table></li><li><p>过滤型参数：</p><table><thead><tr><th align="center">Name</th><th align="center">Type</th><th align="left">说明</th></tr></thead><tbody><tr><td align="center">thresholdPriority</td><td align="center">int</td><td align="left">同上</td></tr><tr><td align="center">thresholdPriorityClassName</td><td align="center">string</td><td align="left">同上</td></tr><tr><td align="center">nodeFit</td><td align="center">bool</td><td align="left">同上</td></tr></tbody></table></li></ul><p>说明：</p><ul><li>与 <code>LowNodeUtilization</code> 策略一样，该策略原生也支持 cpu、memory、pods 数量三种原生资源类型。如果资源类型未指定默认设置为 100%。</li><li>该策略也同样支持扩展资源（如 gpu 资源 <code>nvidia.com/gpu</code>），如果用户未指定扩展资源，该资源同样不参数 node 节点资源使用量的计算。</li><li>阀值 <code>thresholds</code> 同样不能为空。其资源的有效值为[0,100]。</li><li>numberOfNodes：参数用来触发是否开启该策略，如果 <code>未充分利用的节点数</code> 大于 numberOfNodes 时，将激活该策略，启动驱逐功能。否则，不启动该策略，numberOfNodes 的默认值为 0。</li><li>该策略是将 pod 从 <code>低利用率的节点</code> 上往 <code>高利用率的节点</code> 调，会让高利用率的节点利用率更高，低利用率的节点更低。该策略正好与 <code>LowNodeUtilization</code> 相反。</li></ul><p>例子</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">apiVersion:</span> <span class="hljs-string">&quot;descheduler/v1alpha1&quot;</span><br><span class="hljs-symbol">kind:</span> <span class="hljs-string">&quot;DeschedulerPolicy&quot;</span><br><span class="hljs-symbol">strategies:</span><br>  <span class="hljs-string">&quot;HighNodeUtilization&quot;</span>:<br><span class="hljs-symbol">     enabled:</span> true<br><span class="hljs-symbol">     params:</span><br><span class="hljs-symbol">       nodeResourceUtilizationThresholds:</span><br><span class="hljs-symbol">         thresholds:</span><br>           <span class="hljs-string">&quot;cpu&quot;</span> : <span class="hljs-number">20</span><br>           <span class="hljs-string">&quot;memory&quot;</span>: <span class="hljs-number">20</span><br>           <span class="hljs-string">&quot;pods&quot;</span>: <span class="hljs-number">20</span><br><br></code></pre></td></tr></table></figure><h3 id="1-4-RemovePodsViolatingInterPodAntiAffinity"><a href="#1-4-RemovePodsViolatingInterPodAntiAffinity" class="headerlink" title="1.4. RemovePodsViolatingInterPodAntiAffinity"></a>1.4. RemovePodsViolatingInterPodAntiAffinity</h3><p>该策略会确保同一个节点上的 pod 之间不违背 pod 的亲和性，如果同一个节点上 pod 之间存在反亲和性，则会将相关 pod 驱逐掉。比如，如果 node 上同时运行 podA、podB、podC 三个pod，podA 与 podB 和 podC 之间存在反亲和性，则该策略会将 podA 驱逐掉，以保证 podB 和 podC 能在 node 上正常运行。</p><p>参数</p><ul><li>过滤型参数<table><thead><tr><th align="center">Name</th><th align="center">Type</th><th align="left">说明</th></tr></thead><tbody><tr><td align="center">thresholdPriority</td><td align="center">int</td><td align="left">同上</td></tr><tr><td align="center">thresholdPriorityClassName</td><td align="center">string</td><td align="left">同上</td></tr><tr><td align="center">namespaces</td><td align="center">list</td><td align="left">指定操作 namespace</td></tr><tr><td align="center">labelSelector</td><td align="center"></td><td align="left"></td></tr><tr><td align="center">nodeFit</td><td align="center">bool</td><td align="left">同上</td></tr></tbody></table></li></ul><p>例子</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">apiVersion:</span> <span class="hljs-string">&quot;descheduler/v1alpha1&quot;</span><br><span class="hljs-symbol">kind:</span> <span class="hljs-string">&quot;DeschedulerPolicy&quot;</span><br><span class="hljs-symbol">strategies:</span><br>  <span class="hljs-string">&quot;RemovePodsViolatingInterPodAntiAffinity&quot;</span>:<br><span class="hljs-symbol">     enabled:</span> true<br></code></pre></td></tr></table></figure><h3 id="1-5-RemovePodsViolatingNodeAffinity"><a href="#1-5-RemovePodsViolatingNodeAffinity" class="headerlink" title="1.5. RemovePodsViolatingNodeAffinity"></a>1.5. RemovePodsViolatingNodeAffinity</h3><p>该策略可以确保将所有违背 <code>节点亲和性</code> 的 pods 都从该 node 节点驱逐掉。在 k8s 中 <code>node 亲和性</code> 可通过参数<code>requiredDuringSchedulingIgnoredDuringExecution</code> 来指定，表示 <code>调度时调度器必须满足条件，执行时 kubelet 可忽略（即kubelet 不执行驱逐）</code> 策略。在开始是某个 podA 满足调度策略，k8s 调度器能将该 podA 调度到该 node 节点上，但随着一段时间后，podA 不在满足 node 的亲和性，此时传统的 k8s 机制 kubelet 是不能驱逐掉 podA 的；但当开启该策略时，该策略会驱使 kubelet 执行驱逐动作，使 node 上的有 <code>requiredDuringSchedulingIgnoredDuringExecution</code> 标签且有违反该 node 亲和性的 pod 将会被驱逐。</p><p>参数：</p><ul><li><p>普通参数：</p><table><thead><tr><th align="center">Name</th><th align="center">Type</th><th align="left">说明</th></tr></thead><tbody><tr><td align="center">nodeAffinityType</td><td align="center">list(string)</td><td align="left">指明 pod 违背 node 的哪些亲和性类型，最终使 kubelet 执行驱逐动作</td></tr></tbody></table></li><li><p>过滤型参数</p><table><thead><tr><th align="center">Name</th><th align="center">Type</th><th align="left">说明</th></tr></thead><tbody><tr><td align="center">thresholdPriority</td><td align="center">int</td><td align="left">同上</td></tr><tr><td align="center">thresholdPriorityClassName</td><td align="center">string</td><td align="left">同上</td></tr><tr><td align="center">namespaces</td><td align="center">list</td><td align="left">include/exclude两个方案</td></tr><tr><td align="center">labelSelector</td><td align="center">list</td><td align="left">同k8s</td></tr><tr><td align="center">nodeFit</td><td align="center">bool</td><td align="left">同上</td></tr></tbody></table></li></ul><p>例子</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">apiVersion:</span> <span class="hljs-string">&quot;descheduler/v1alpha1&quot;</span><br><span class="hljs-symbol">kind:</span> <span class="hljs-string">&quot;DeschedulerPolicy&quot;</span><br><span class="hljs-symbol">strategies:</span><br>  <span class="hljs-string">&quot;RemovePodsViolatingNodeAffinity&quot;</span>:<br><span class="hljs-symbol">    enabled:</span> true<br><span class="hljs-symbol">    params:</span><br><span class="hljs-symbol">      nodeAffinityType:</span><br>      - <span class="hljs-string">&quot;requiredDuringSchedulingIgnoredDuringExecution&quot;</span><br><br></code></pre></td></tr></table></figure><h3 id="1-6-RemovePodsViolatingNodeTaints"><a href="#1-6-RemovePodsViolatingNodeTaints" class="headerlink" title="1.6. RemovePodsViolatingNodeTaints"></a>1.6. RemovePodsViolatingNodeTaints</h3><p>该策略会对违反 <code>node taints 污点</code>的 pod 进行驱逐。比如，在 pod 调度开始时，node 上含有污点 <code>NoSchedule</code>，且 podA 中也含有容忍(toleration) node 污点的标签 <code>key=value:NoSchedule</code>，此时该 podA 能被调度到 node 上运行，kubelet 也不会驱逐该 podA。随着一段时间后，将 node 上的污点更新了或者移除了等情况下，在原有 k8s 上 kubelet 不能将 pod 驱逐掉。如果开启该策略将会使 kubelet 去驱逐 podA。</p><p>参数：</p><ul><li>过滤型参数：<table><thead><tr><th align="center">Name</th><th align="center">Type</th><th align="left">说明</th></tr></thead><tbody><tr><td align="center">thresholdPriority</td><td align="center">int</td><td align="left">同上</td></tr><tr><td align="center">thresholdPriorityClassName</td><td align="center">string</td><td align="left">同上</td></tr><tr><td align="center">namespaces</td><td align="center"></td><td align="left"></td></tr><tr><td align="center">labelSelector</td><td align="center"></td><td align="left"></td></tr><tr><td align="center">nodeFit</td><td align="center">bool</td><td align="left">同上</td></tr></tbody></table></li></ul><p>例子</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">apiVersion:</span> <span class="hljs-string">&quot;descheduler/v1alpha1&quot;</span><br><span class="hljs-symbol">kind:</span> <span class="hljs-string">&quot;DeschedulerPolicy&quot;</span><br><span class="hljs-symbol">strategies:</span><br>  <span class="hljs-string">&quot;RemovePodsViolatingNodeTaints&quot;</span>:<br><span class="hljs-symbol">    enabled:</span> true<br><br></code></pre></td></tr></table></figure><h3 id="1-7-RemovePodsViolatingTopologySpreadConstraint"><a href="#1-7-RemovePodsViolatingTopologySpreadConstraint" class="headerlink" title="1.7. RemovePodsViolatingTopologySpreadConstraint"></a>1.7. RemovePodsViolatingTopologySpreadConstraint</h3><p>该策略会对违反拓扑约束关系的 pod 从 node 上驱逐，以达到在k8s 中多个域之间平台 pod 数量的要求，以便实现 pods 之间更细粒度的调度，方便实现容灾和高可用。 topologySpreadConstraints 表示拓扑分布约束，可以控制 Pod 在某些节点的分布，可以在多个域之间平衡 Pod 数量，topologySpreadConstraints 策略是在 k8s v1.16第一次提出，在 v1.18 进入beta版默认开启，具体详情可参考 k8s v1.18 官方说明。因此该策略只在 k8s <code>v1.18</code> 以上版本中可用。</p><p>说明：</p><ul><li>该策略默认只处理<code>硬约束（hard constraints）</code>的条件，如果要处理 <code>软约束（soft constraints）</code>的条件，需要将参数 <code>includeSoftConstraints</code> 设置为 <code>true</code>。</li><li>该策略中的参数 <code>labelSelector</code> 在 <code>处理拓扑平衡域</code> 时不会生效，它只会在 驱逐 pod 阶段或者决定哪个 pod 可被驱逐时才生效。</li></ul><p>参数：</p><ul><li><p>普通参数：</p><table><thead><tr><th align="left">Name</th><th align="center">Type</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">includeSoftConstraints</td><td align="center">bool</td><td align="left">默认不开启，如要开启需要设为 <code>true</code></td></tr></tbody></table></li><li><p>过滤型参数：</p><table><thead><tr><th align="left">Name</th><th align="center">Type</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">thresholdPriority</td><td align="center">int</td><td align="left">同上</td></tr><tr><td align="left">thresholdPriorityClassName</td><td align="center">string</td><td align="left">同上</td></tr><tr><td align="left">namespaces</td><td align="center"></td><td align="left"></td></tr><tr><td align="left">labelSelector</td><td align="center"></td><td align="left"></td></tr><tr><td align="left">nodeFit</td><td align="center">bool</td><td align="left">同上</td></tr></tbody></table></li></ul><p>例子</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">apiVersion:</span> <span class="hljs-string">&quot;descheduler/v1alpha1&quot;</span><br><span class="hljs-symbol">kind:</span> <span class="hljs-string">&quot;DeschedulerPolicy&quot;</span><br><span class="hljs-symbol">strategies:</span><br>  <span class="hljs-string">&quot;RemovePodsViolatingTopologySpreadConstraint&quot;</span>:<br><span class="hljs-symbol">     enabled:</span> true<br><span class="hljs-symbol">     params:</span><br><span class="hljs-symbol">       includeSoftConstraints:</span> false<br><br></code></pre></td></tr></table></figure><h3 id="1-8-RemovePodsHavingTooManyRestarts"><a href="#1-8-RemovePodsHavingTooManyRestarts" class="headerlink" title="1.8. RemovePodsHavingTooManyRestarts"></a>1.8. RemovePodsHavingTooManyRestarts</h3><p>该策略会对 pod 重启次数太多的时候将 pod 驱逐，pod 重启原因有很多种，有因为 pod 的健康检查而一直重启，也有因为挂载卷不成功或者其他因素导致的 pod 重启，也有可能是 pod 所在的 node 因素导致的等，这种情况下该策略可将 pod 驱逐重建。</p><p>参数</p><ul><li><p>普通参数：</p><table><thead><tr><th align="center">Name</th><th align="center">Type</th><th align="left">说明</th></tr></thead><tbody><tr><td align="center">podRestartThreshold</td><td align="center">int</td><td align="left">指定 pod 重启次数的阀值，当 pod 的重启次数大于该值时，将执行驱逐策略</td></tr><tr><td align="center">includingInitContainers</td><td align="center">bool</td><td align="left">计算 pod 的重启次数时，是否将 init container 的重启次数计算进去</td></tr></tbody></table></li><li><p>过滤型参数：</p><table><thead><tr><th align="center">Name</th><th align="center">Type</th><th align="left">说明</th></tr></thead><tbody><tr><td align="center">thresholdPriority</td><td align="center">int</td><td align="left">同上</td></tr><tr><td align="center">thresholdPriorityClassName</td><td align="center">string</td><td align="left">同上</td></tr><tr><td align="center">namespaces</td><td align="center"></td><td align="left"></td></tr><tr><td align="center">labelSelector</td><td align="center"></td><td align="left"></td></tr><tr><td align="center">nodeFit</td><td align="center">bool</td><td align="left">同上</td></tr></tbody></table></li></ul><p>例子</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">apiVersion:</span> <span class="hljs-string">&quot;descheduler/v1alpha1&quot;</span><br><span class="hljs-symbol">kind:</span> <span class="hljs-string">&quot;DeschedulerPolicy&quot;</span><br><span class="hljs-symbol">strategies:</span><br>  <span class="hljs-string">&quot;RemovePodsHavingTooManyRestarts&quot;</span>:<br><span class="hljs-symbol">     enabled:</span> true<br><span class="hljs-symbol">     params:</span><br><span class="hljs-symbol">       podsHavingTooManyRestarts:</span><br><span class="hljs-symbol">         podRestartThreshold:</span> <span class="hljs-number">100</span><br><span class="hljs-symbol">         includingInitContainers:</span> true<br><br></code></pre></td></tr></table></figure><h3 id="1-9-PodLifeTime"><a href="#1-9-PodLifeTime" class="headerlink" title="1.9. PodLifeTime"></a>1.9. PodLifeTime</h3><p>该策略会驱逐长时间运行中的 pod，即根据设定的阀值以及 pod 的生命时长来决定是否驱逐 pod。目前仅支持对两种状态的 pod 进行驱逐，即 running 或 pending 状态的 pod。</p><p>参数</p><ul><li><p>普通参数：</p><table><thead><tr><th align="center">Name</th><th align="center">Type</th><th align="left">说明</th></tr></thead><tbody><tr><td align="center">maxPodLifeTimeSeconds</td><td align="center">int</td><td align="left">设定 pod 生命时长的驱逐阀值，该策略会驱逐大于该阀值的 pod</td></tr><tr><td align="center">podStatusPhases</td><td align="center">list</td><td align="left">指定可驱逐 pod 的状态，目前只支持驱逐状态为 running 和 pending 的 pod</td></tr></tbody></table></li><li><p>过滤型参数</p><table><thead><tr><th align="center">Name</th><th align="center">Type</th><th align="left">说明</th></tr></thead><tbody><tr><td align="center">thresholdPriority</td><td align="center">int</td><td align="left">同上</td></tr><tr><td align="center">thresholdPriorityClassName</td><td align="center">string</td><td align="left">同上</td></tr><tr><td align="center">namespaces</td><td align="center"></td><td align="left"></td></tr><tr><td align="center">labelSelector</td><td align="center"></td><td align="left"></td></tr></tbody></table></li></ul><p>例子</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">apiVersion:</span> <span class="hljs-string">&quot;descheduler/v1alpha1&quot;</span><br><span class="hljs-symbol">kind:</span> <span class="hljs-string">&quot;DeschedulerPolicy&quot;</span><br><span class="hljs-symbol">strategies:</span><br>  <span class="hljs-string">&quot;PodLifeTime&quot;</span>:<br><span class="hljs-symbol">     enabled:</span> true<br><span class="hljs-symbol">     params:</span><br><span class="hljs-symbol">       podLifeTime:</span><br><span class="hljs-symbol">         maxPodLifeTimeSeconds:</span> <span class="hljs-number">86400</span><br><span class="hljs-symbol">         podStatusPhases:</span><br>         - <span class="hljs-string">&quot;Pending&quot;</span><br></code></pre></td></tr></table></figure><h2 id="2-Pods-过滤参数详解"><a href="#2-Pods-过滤参数详解" class="headerlink" title="2. Pods 过滤参数详解"></a>2. Pods 过滤参数详解</h2><h3 id="2-1-Namespace-过滤"><a href="#2-1-Namespace-过滤" class="headerlink" title="2.1. Namespace 过滤"></a>2.1. Namespace 过滤</h3><p>Descheduler 中的策略可以通过 namespaces 参数来过滤以决定这些策略可以在哪些 namespaces 中生效或者。基于 namespaces 过滤方法共有两种，即 include 和 exclude 方法，include 表示策略可以在指定的 namespaces 中生效，而 exclude 则表示 <strong>排除</strong> 指定的 namespaces 后，策略可在其他 namespaces 中生效。如下策略支持 namespaces 过滤：</p><ul><li>PodLifeTime</li><li>RemovePodsHavingTooManyRestarts</li><li>RemovePodsViolatingNodeTaints</li><li>RemovePodsViolatingNodeAffinity</li><li>RemovePodsViolatingInterPodAntiAffinity</li><li>RemoveDuplicates</li><li>RemovePodsViolatingTopologySpreadConstraint</li></ul><p>如：</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">apiVersion:</span> <span class="hljs-string">&quot;descheduler/v1alpha1&quot;</span><br><span class="hljs-symbol">kind:</span> <span class="hljs-string">&quot;DeschedulerPolicy&quot;</span><br><span class="hljs-symbol">strategies:</span><br>  <span class="hljs-string">&quot;PodLifeTime&quot;</span>:<br><span class="hljs-symbol">     enabled:</span> true<br><span class="hljs-symbol">     params:</span><br><span class="hljs-symbol">        podLifeTime:</span><br><span class="hljs-symbol">          maxPodLifeTimeSeconds:</span> <span class="hljs-number">86400</span><br><span class="hljs-symbol">        namespaces:</span><br><span class="hljs-symbol">          include:</span><br>          - <span class="hljs-string">&quot;namespace1&quot;</span><br>          - <span class="hljs-string">&quot;namespace2&quot;</span><br></code></pre></td></tr></table></figure><p>表示 <code>PodLifeTime</code> 策略仅在 <code>namespace1</code> 和 <code>namespace2</code> 中生效。</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">apiVersion:</span> <span class="hljs-string">&quot;descheduler/v1alpha1&quot;</span><br><span class="hljs-symbol">kind:</span> <span class="hljs-string">&quot;DeschedulerPolicy&quot;</span><br><span class="hljs-symbol">strategies:</span><br>  <span class="hljs-string">&quot;PodLifeTime&quot;</span>:<br><span class="hljs-symbol">     enabled:</span> true<br><span class="hljs-symbol">     params:</span><br><span class="hljs-symbol">        podLifeTime:</span><br><span class="hljs-symbol">          maxPodLifeTimeSeconds:</span> <span class="hljs-number">86400</span><br><span class="hljs-symbol">        namespaces:</span><br><span class="hljs-symbol">          exclude:</span><br>          - <span class="hljs-string">&quot;namespace1&quot;</span><br>          - <span class="hljs-string">&quot;namespace2&quot;</span><br></code></pre></td></tr></table></figure><p>表示 <code>PodLifeTime</code> 策略可在 <strong>除了</strong> <code>namespace1</code> 和 <code>namespace2</code> 的其他所有 namespaces 中生效。</p><h3 id="2-2-Priority-优先级过滤"><a href="#2-2-Priority-优先级过滤" class="headerlink" title="2.2. Priority 优先级过滤"></a>2.2. Priority 优先级过滤</h3><p>Descheduler 中的 <code>所有策略</code> 都可以指定 <code>优先级过滤</code>，优先级策略通过一个优先级阀值来指定，只有当 pod 的优先级值<strong>小于</strong>该阀值时才能被 Descheduler 驱逐。有两种方式来指定优先级策略（默认情况下通过 <code>system-cluster-critical</code> priorityClass 来指定优先级的阀值。）：</p><ul><li><code>thresholdPriority</code>：直接通过一个 int 数值来指定。</li><li><code>thresholdPriorityClassName</code>：通过 k8s 的 <code> priority class</code> 来进行关联，<code> priority class</code> 中具体会指定优先级的数值。如果策略中指定的 <code>priority class</code> 在 k8s 集群中没有创建（不存在），则会直接报错。</li><li><strong>注意</strong>：上述两个优先级只能使用其中一种。在一个策略中不能同时指定上述两种优先级。</li><li><strong>注意</strong>：如果 <code>evictSystemCriticalPods</code> 设置为 <code>true</code>，将会驱逐系统关键 pod，且设置的所有 <code>优先级策略</code> 都将 <strong>无效</strong>。</li></ul><p>例子</p><ul><li><p><code>thresholdPriority</code> 方式：</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">apiVersion:</span> <span class="hljs-string">&quot;descheduler/v1alpha1&quot;</span><br><span class="hljs-symbol">kind:</span> <span class="hljs-string">&quot;DeschedulerPolicy&quot;</span><br><span class="hljs-symbol">strategies:</span><br>  <span class="hljs-string">&quot;PodLifeTime&quot;</span>:<br><span class="hljs-symbol">    enabled:</span> true<br><span class="hljs-symbol">    params:</span><br><span class="hljs-symbol">        podLifeTime:</span><br><span class="hljs-symbol">          maxPodLifeTimeSeconds:</span> <span class="hljs-number">86400</span><br><span class="hljs-symbol">        thresholdPriority:</span> <span class="hljs-number">10000</span><br></code></pre></td></tr></table></figure></li><li><p><code>thresholdPriorityClassName</code> 方式：</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">apiVersion:</span> <span class="hljs-string">&quot;descheduler/v1alpha1&quot;</span><br><span class="hljs-symbol">kind:</span> <span class="hljs-string">&quot;DeschedulerPolicy&quot;</span><br><span class="hljs-symbol">strategies:</span><br>  <span class="hljs-string">&quot;PodLifeTime&quot;</span>:<br><span class="hljs-symbol">    enabled:</span> true<br><span class="hljs-symbol">    params:</span><br><span class="hljs-symbol">        podLifeTime:</span><br><span class="hljs-symbol">          maxPodLifeTimeSeconds:</span> <span class="hljs-number">86400</span><br><span class="hljs-symbol">        thresholdPriorityClassName:</span> <span class="hljs-string">&quot;priorityclass1&quot;</span><br></code></pre></td></tr></table></figure></li></ul><h3 id="2-3-Label-标签过滤"><a href="#2-3-Label-标签过滤" class="headerlink" title="2.3. Label 标签过滤"></a>2.3. Label 标签过滤</h3><p>如下策略支持通过 k8s 的标准标签过滤器（<code>labelSelector</code>）来过滤需要驱逐指定的 pod。</p><ul><li>PodLifeTime</li><li>RemovePodsHavingTooManyRestarts</li><li>RemovePodsViolatingNodeTaints</li><li>RemovePodsViolatingNodeAffinity</li><li>RemovePodsViolatingInterPodAntiAffinity</li><li>RemovePodsViolatingTopologySpreadConstraint</li></ul><p>例子</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">&quot;descheduler/v1alpha1&quot;</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">&quot;DeschedulerPolicy&quot;</span><br><span class="hljs-attr">strategies:</span><br>  <span class="hljs-attr">&quot;PodLifeTime&quot;:</span><br>    <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span><br>    <span class="hljs-attr">params:</span><br>      <span class="hljs-attr">podLifeTime:</span><br>        <span class="hljs-attr">maxPodLifeTimeSeconds:</span> <span class="hljs-number">86400</span><br>      <span class="hljs-attr">labelSelector:</span><br>        <span class="hljs-attr">matchLabels:</span><br>          <span class="hljs-attr">component:</span> <span class="hljs-string">redis</span><br>        <span class="hljs-attr">matchExpressions:</span><br>          <span class="hljs-bullet">-</span> &#123;<span class="hljs-attr">key:</span> <span class="hljs-string">tier</span>, <span class="hljs-attr">operator:</span> <span class="hljs-string">In</span>, <span class="hljs-attr">values:</span> [<span class="hljs-string">cache</span>]&#125;<br>          <span class="hljs-bullet">-</span> &#123;<span class="hljs-attr">key:</span> <span class="hljs-string">environment</span>, <span class="hljs-attr">operator:</span> <span class="hljs-string">NotIn</span>, <span class="hljs-attr">values:</span> [<span class="hljs-string">dev</span>]&#125;<br></code></pre></td></tr></table></figure><h3 id="2-4-Node-fit-过滤"><a href="#2-4-Node-fit-过滤" class="headerlink" title="2.4. Node fit 过滤"></a>2.4. Node fit 过滤</h3><p>Descheduler 中以下策略支持 nodeFit 过滤来优化驱逐调度的选择。</p><ul><li>RemoveDuplicates</li><li>LowNodeUtilization</li><li>HighNodeUtilization</li><li>RemovePodsViolatingInterPodAntiAffinity</li><li>RemovePodsViolatingNodeAffinity</li><li>RemovePodsViolatingNodeTaints</li><li>RemovePodsViolatingTopologySpreadConstraint</li><li>RemovePodsHavingTooManyRestarts</li></ul><p>如果 nodeFit 为 <code>true</code>，则 descheduler 在驱逐 pod 时会考虑驱逐的 pod 是否满足可驱逐的标准，并且可驱逐的 pod 是否可以重新调度到其他 node 节点上，如果 pod 不能重新调度到其他 node 节点上，则不会驱逐该 pod。当前 pod 是否可驱逐的标准如下：</p><ul><li>pod 中是否有 nodeSelector 标签选择器</li><li>pod 中有可容忍 node 的污点，即 pod 中有 <code>Tolerations</code> 标签， node 有对应的 <code>Taints</code> 标签</li><li>pod 中有 <code>nodeAffinity</code></li><li>是否有其他 node 节点被标记为 <code>unschedulable</code></li></ul><p><strong>注意</strong>：<code>nodeFit</code> 过滤是基于 pod 的 spec 进行过滤的，并不会关联 pod 的 <code>owner</code>；如果 pod 的 owner（比如 RC 等）被修改了但 pod 的 spec 未被修改，这时 nodeFit 的过滤将会引用 pod 旧的 spec 信息。 这种行为在 Descheduler 中时可允许的或者说不严重的，因为 Descheduler 的机制是 <code>尽最大努力</code> 来进行资源的再度平衡。当然如果想实时得到 pod 的最新信息，可以使用 <code>Deployment</code> 来代替 RC，Deployment 会实现自动同步更新 pod 的 spec 的功能，以保证集群中 pod 的最新信息。</p><h2 id="3-Pod-驱逐说明"><a href="#3-Pod-驱逐说明" class="headerlink" title="3. Pod 驱逐说明"></a>3. Pod 驱逐说明</h2><p>Descheduler 中从 node 中驱逐一个 pod 必须遵守以下机制：</p><ul><li><code>Critical pods</code>：通过 <code>PriorityClassName</code> 设置的 <code>system-cluster-critical</code> 或者 <code>system-node-critical</code> 关键性 pod 永远不会被驱逐，除非将 <code>evictSystemCriticalPods</code> 设置为 <code>true</code>。</li><li><code>静态 pod</code>、<code>kubelet 直接用镜像启动的pod</code>、<code>孤儿 pod</code> 等不被 ReplicationController, ReplicaSet(Deployment), StatefulSet, or Job 等控制器管理的 pod 永远不会被驱逐。因为一旦驱逐了，这些 pod 将永远不会被拉起来。</li><li>被 DaemonSets 管理的 pod 永远不会被驱逐。</li><li>pod 中使用本地存储卷的（local storage）不会被驱逐，除非将 <code>evictLocalStoragePods</code> 设为 <code>true</code>。</li><li>pod 中使用 PVCs 的默认是可以被驱逐的，可通过设置 <code>ignorePvcPods</code> 为 <code>true</code> 来保证 <strong>不被</strong> 驱逐。</li><li>在 <code>LowNodeUtilization</code> 和 <code>RemovePodsViolatingInterPodAntiAffinity</code> 中，pod 的驱逐优先级是从低到高，当 pod 之间的优先级相同时，会根据 k8s 的 Qos 等级来进行驱逐，即 <code>best effort</code> 先驱逐，其次 <code> burstable</code>，最后 <code>guaranteed</code>。</li><li>所有 pod 中如果有<code>注释:descheduler.alpha.kubernetes.io/evict</code>，则表明该 pod 可以被驱逐。这种情况下，用户可根据需求以及 pod 的信息来特定指定可驱逐的 pod。</li></ul><h2 id="4-其他："><a href="#4-其他：" class="headerlink" title="4. 其他："></a>4. 其他：</h2><ul><li>设置 –v=4 或者更大的值，Descheduler 的日志中会打印出任一 pod 不能被驱逐的原因。</li><li>Descheduler 的驱逐信息可通过 <code> https://localhost:10258/metrics</code> 地址进行查看，地址可以通过参数 <code>--binding-address</code> 更改，基于 https的安全端口号可以通过参数 <code>--secure-port</code> 更改。<table><thead><tr><th align="center">name</th><th align="center">type</th><th align="left">说明</th></tr></thead><tbody><tr><td align="center">build_info</td><td align="center">gauge</td><td align="left">constant 1</td></tr><tr><td align="center">pods_evicted</td><td align="center">CounterVec</td><td align="left">total number of pods evicted</td></tr></tbody></table></li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://github.com/kubernetes-sigs/descheduler">https://github.com/kubernetes-sigs/descheduler</a></li></ul><h2 id="猜你喜欢"><a href="#猜你喜欢" class="headerlink" title="猜你喜欢"></a>猜你喜欢</h2><ul><li><a href="./hello-world.md">hello world</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>调度器系列</category>
      
      <category>descheduler</category>
      
    </categories>
    
    
    <tags>
      
      <tag>kubernetes</tag>
      
      <tag>descheduler</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
